{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Libraries for importing data from binary file\n",
    "import os.path #for accessing the file path\n",
    "import struct  #for unpacking the binary data\n",
    "\n",
    "import time    #for calculating time\n",
    "\n",
    "#core packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#custom module\n",
    "from dataPrep import retrive_data, sample_origDataset , dev_test_split, prep_dataset, visualize_orig\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Datatype\t\t Shape\n",
      "=================================================================\n",
      "Training Set Images:\t<class 'numpy.ndarray'>\t (60000, 28, 28)\n",
      "Training Set Labels:\t<class 'numpy.ndarray'>\t (60000, 1)\n",
      "Test Set Images:\t<class 'numpy.ndarray'>\t (10000, 28, 28)\n",
      "Test Set Labels:\t<class 'numpy.ndarray'>\t (10000, 1)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "#retriving the data\n",
    "train_x_orig, train_y_orig = retrive_data(dataset=\"training-set\")\n",
    "test_x_temp, test_y_temp = retrive_data(dataset=\"test-set\")\n",
    "\n",
    "#displaying the retrival info\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_temp))+\"\\t\",str(test_x_temp.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_temp))+\"\\t\",str(test_y_temp.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Complete Data Shape\t Sample Data Shape\t Sample Size\n",
      "=====================================================================================\n",
      "Training Set Images:\t(60000, 28, 28)\t\t(15000, 28, 28)\t\t25%\n",
      "Training Set Labels:\t(60000, 1)\t\t(15000, 1)\n",
      "Test Set Images:\t(10000, 28, 28)\t\t(2500, 28, 28)\t\t25%\n",
      "Test Set Labels:\t(10000, 1)\t\t(2500, 1)\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sampling the Dataset for Model Experiment\n",
    "train_Vol,train_x_sample, train_y_sample = sample_origDataset(train_x_orig,train_y_orig,dataVol= 25)\n",
    "test_Vol,test_x_sample,test_y_sample = sample_origDataset(test_x_temp,test_y_temp,dataVol= 25)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Complete Data Shape\\t\",\"Sample Data Shape\\t\",\"Sample Size\")\n",
    "print(\"=====================================================================================\")\n",
    "print(\"Training Set Images:\\t\"+ str(train_x_orig.shape)+\"\\t\\t\"+ str(train_x_sample.shape)+\"\\t\\t\"+str(train_Vol)+\"%\")\n",
    "print(\"Training Set Labels:\\t\"+ str(train_y_orig.shape)+\"\\t\\t\"+ str(train_y_sample.shape))\n",
    "print(\"Test Set Images:\\t\"+str(test_x_temp.shape)+\"\\t\\t\"+ str(test_x_sample.shape)+\"\\t\\t\"+str(test_Vol)+\"%\")\n",
    "print(\"Test Set Labels:\\t\"+str(test_y_temp.shape)+\"\\t\\t\"+ str(test_y_sample.shape))\n",
    "print(\"=====================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Shape\n",
      "=================================================================\n",
      "Dev Set Images:\t\t (1250, 28, 28)\n",
      "Dev Set Labels:\t\t (1250, 1)\n",
      "Test Set Images:\t (1250, 28, 28)\n",
      "Test Set Labels:\t (1250, 1)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Splitting Dev-Test Set\n",
    "dev_x_orig,dev_y_orig,test_x_orig,test_y_orig = dev_test_split(test_x_sample,test_y_sample)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Dev Set Images:\\t\\t\" ,str(dev_x_orig.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" ,str(dev_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(60000, 28, 28)\t\t(784, 15000)\n",
      "Training Set Labels:\t(60000, 1)\t\t(11, 15000)\n",
      "Dev Set Images:\t\t(1250, 28, 28)\t\t(784, 1250)\n",
      "Dev Set Labels:\t\t(1250, 1)\t\t(11, 1250)\n",
      "Test Set Images:\t(1250, 28, 28)\t\t(784, 1250)\n",
      "Test Set Labels:\t(1250, 1)\t\t(11, 1250)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Dataset (Flattening and Normalizing)\n",
    "train_x_norm,train_y_encoded, dev_x_norm,dev_y_encoded, test_x_norm, test_y_encoded = prep_dataset(train_x_sample, train_y_sample, dev_x_orig, dev_y_orig, test_x_orig, test_y_orig)\n",
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_x_orig.shape)+\"\\t\\t\"+ str(train_x_norm.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_y_orig.shape)+\"\\t\\t\"+ str(train_y_encoded.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(dev_x_orig.shape)+\"\\t\\t\"+ str(dev_x_norm.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(dev_y_orig.shape)+\"\\t\\t\"+ str(dev_y_encoded.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_norm.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_encoded.shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0.0,Z)\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([1,2,3,4])\n",
    "A,cache = relu(Z)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "#     dZ[Z <= 0] = 0\n",
    "    dZ[Z < 0] = 0\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    shift = Z - np.max(Z) #Avoiding underflow or overflow errors due to floating point instability in softmax\n",
    "    t = np.exp(shift)\n",
    "#     t = np.exp(Z)\n",
    "    A = np.divide(t,np.sum(t,axis = 0))\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = np.array([5,2,-1,3]).reshape(4,1)\n",
    "Z= np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]).reshape(7,1)\n",
    "print(Z.shape)\n",
    "A,cache = softmax(Z)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(dA,cache):\n",
    "    Z = cache\n",
    "    s = dA.reshape(-1,1)\n",
    "#     dZ = np.diagflat(s) - np.dot(s,s.T)\n",
    "    dZ = np.multiply(dA,(1-dA))\n",
    "#     assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[5],[2],[-1],[3]]).reshape(4,1) \n",
    "Y = np.array([1,0,0,0]).reshape(4,1)\n",
    "A,cache = softmax(Z)\n",
    "print(A)\n",
    "dA = A - Y\n",
    "print(dA)\n",
    "dZ = softmax_grad(dA,cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers():\n",
    "    layers_dim = [784,16,16,11]\n",
    "    return layers_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dim = init_layers()\n",
    "print(\"Layer\\t\\tNodes\")\n",
    "print(\"======================\")\n",
    "for layer,node in enumerate(layers_dim):\n",
    "    print(str(layer) + \"\\t\\t\" + str(node))\n",
    "\n",
    "print(\"======================\")\n",
    "\n",
    "print(\"No. of Hidden Layers: \" + str(len(layers_dim)-2))\n",
    "print(\"Total No. of Layers: \" + str(len(layers_dim)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers_dim):\n",
    "    \n",
    "    L = len(layers_dim)\n",
    "    params = {}\n",
    "        \n",
    "    for l in range(1,L):\n",
    "        params['W' + str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) *0.01\n",
    "        params['b' + str(l)] = np.zeros((layers_dim[l],1))\n",
    "     \n",
    "        assert(params['W' + str(l)].shape == (layers_dim[l],layers_dim[l-1]))\n",
    "        assert(params['b' + str(l)].shape == (layers_dim[l],1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = init_params(layers_dim)\n",
    "print(\"Layer\\tWeight\\t\\tBias\")\n",
    "print(\"================================\")\n",
    "for l in range(1,len(layers_dim)):\n",
    "    print(str(l) +\"\\t\" + str(parameters['W'+str(l)].shape) +\"\\t\"+ str(parameters['b'+str(l)].shape))\n",
    "    print()    \n",
    "print(\"Total Connections: \"+ str(784*16*16*10))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hyperParams(alpha = 0.01,ite = 5000):\n",
    "    hyperParams = {}\n",
    "    hyperParams['learning_rate'] = alpha\n",
    "    hyperParams['num_iterations'] = ite\n",
    "    \n",
    "    \n",
    "    return hyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = init_hyperParams(alpha = 0.01,ite = 1000)\n",
    "print(\"Learning Rate:\\t\"+ str(hyperParams['learning_rate']))\n",
    "print(\"Epoch:\\t\"+ str(hyperParams['num_iterations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sum(A,W,b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    assert(Z.shape == (W.shape[0],Z.shape[1]))\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A,W,b,activation):\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, sum_cache = forward_sum(A,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    if activation == 'softmax':\n",
    "        Z, sum_cache = forward_sum(A,W,b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    cache = (sum_cache,activation_cache)\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "A, linear_activation_cache = forward_activation(X, W, b, activation = \"softmax\")\n",
    "print(\"With softmax: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = forward_activation(X, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = forward_activation(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation='relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = forward_activation(A,parameters['W' + str(L)],parameters['b' + str(L)],activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (11,X.shape[1]))\n",
    "    \n",
    "    return AL,caches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = forward_prop(train_x_norm,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "#     cost = (1./m) * np.sum(-np.dot(Y,np.log(AL).T))\n",
    "    cost = -(1./m) * np.sum(np.sum(np.multiply(Y,np.log(AL)), axis = 0,keepdims=True))\n",
    "    \n",
    "    \n",
    "    cost = np.squeeze(cost)      # Making sure your cost's shape is not returned as ndarray\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = compute_cost(AL,train_y_encoded)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[0,1,0,0],[1,0,0,0]]).reshape(4,2)\n",
    "AL = np.array([[0.1,0.7,0.1,0.1],[0.6,0.2,0.1,0.1]]).reshape(4,2)\n",
    "cost = compute_cost(AL,Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_grad(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims=True )\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA,cache,activation):\n",
    "    sum_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_grad(dA,activation_cache)\n",
    "        dA_prev, dW, db = backward_grad(dZ, sum_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = backward_grad(dA, sum_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dA = np.subtract(AL,Y)\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_activation(dA, current_cache, activation = 'softmax')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_activation(grads[\"dA\" + str(l + 1)], current_cache, activation = 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,y,parameters):\n",
    "    m = y.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = forward_prop(X, parameters)\n",
    "   \n",
    "    assert(probas.shape == y.shape)\n",
    "        \n",
    "    predicted_labels = np.argmax(probas,axis=0).reshape(1,probas.shape[1])\n",
    "    predicted_prob = np.max(probas,axis = 0).reshape(1,m)\n",
    "    \n",
    "    Y = np.argmax(y,axis=0).reshape(1,y.shape[1])\n",
    "    \n",
    "\n",
    "    #print results\n",
    "    true_prediction = np.equal(predicted_labels,Y)\n",
    "#     print(true_prediction.shape)\n",
    "    \n",
    "    num_correct_labels = np.sum(true_prediction)\n",
    "    num_incorrect_labels = m - num_correct_labels\n",
    "    accuracy = num_correct_labels/m\n",
    "#     print(\"No. of Correct Prediction:\\t\"+str(num_correct_labels))\n",
    "#     print(\"No. of Incorrect Prediction:\\t\"+str(num_incorrect_labels))\n",
    "#     print(\"\\nAccuracy: \"  + str(accuracy*100)+\"%\")\n",
    "#     print(\"\\nError:\\t\"+str((1-accuracy)*100)+\"%\")\n",
    "        \n",
    "    return predicted_labels, predicted_prob, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(attr, attr_type):\n",
    "    \n",
    "    plt.plot(np.squeeze(attr))\n",
    "    if attr_type == 'costs':\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.title(\"Cost\")\n",
    "        \n",
    "    elif attr_type == 'train_accs':\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        \n",
    "    elif attr_type == 'val_accs':\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.title(\"Validation Accuracy\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "        \n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, X_dev, Y_dev, layers_dim, hyperParams):\n",
    "\n",
    "    learning_rate = hyperParams['learning_rate']\n",
    "    num_iterations = hyperParams['num_iterations']\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []              # keep track of cost\n",
    "    train_accs = []  # keep track of training accuracy\n",
    "    val_accs = []     # keep track of Validation accuracy\n",
    "    \n",
    "    parameters = init_params(layers_dim)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = forward_prop(X_train, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y_train)\n",
    "    \n",
    "        grads = backward_prop(AL, Y_train, caches)\n",
    " \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        _,_,train_acc = predict(X_train, Y_train,parameters)\n",
    "        _,_,val_acc= predict(X_dev, Y_dev,parameters)        \n",
    "        \n",
    "        if i % 200 == 0 or i == num_iterations:\n",
    "            print (\"Iteration: %d == Cost: %f || Training acc: %f || Val acc: %f\"%(i,cost,train_acc,val_acc))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            \n",
    "    visualize_results(costs, attr_type='costs')  \n",
    "    visualize_results(train_accs, attr_type='train_accs')       \n",
    "    visualize_results(val_accs, attr_type='val_accs')       \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = init_hyperParams(alpha = 0.05,ite = 5000)\n",
    "layers_dim = init_layers()\n",
    "parameters = train(train_x_norm, train_y_encoded,dev_x_norm, dev_y_encoded,layers_dim, hyperParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_train, prediction_prob_train,train_acc = predict(train_x_norm, train_y_encoded,parameters)\n",
    "print(\"\\nAccuracy: \"  + str(train_acc*100)+\"%\")\n",
    "print(\"\\nError:\\t\"+str((1-train_acc)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(x_orig, y_orig, predicted_labels, prediction_prob, dataset):\n",
    "#     print(x_orig.shape,y_orig.shape, predicted_labels.shape)\n",
    "    if(dataset == \"training\"):\n",
    "        visual_title = \"Sample Training Data Set\"\n",
    "        rng = range(30,40)\n",
    "    elif(dataset == \"dev\"):\n",
    "        visual_title = \"Sample Dev Data Set\"\n",
    "        rng = range(110,120)\n",
    "    elif(dataset == \"test\"):\n",
    "        visual_title = \"Sample Test Data Set\"\n",
    "        rng = range(110,120)        \n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    fig.suptitle(visual_title)\n",
    "\n",
    "    for ax,i in zip(axes.flatten(),rng):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest', cmap='Greys')\n",
    "        ax.set(title = \"True: \"+ str(y_orig[0,i])+\" | Predicted: \"+str(predicted_labels[0,i]))\n",
    "        ax.set(xlabel= \"Prediction Prob: %f\"%(prediction_prob[0,i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(train_x_sample, train_y_sample.T,predicted_labels_train, prediction_prob_train,dataset = \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mislabelled_images(x_orig,y_orig,predicted_labels,prediction_prob,dataset):\n",
    "    true_prediction = np.equal(predicted_labels,y_orig)\n",
    "    mislabelled_indices = np.asarray(np.where(true_prediction == False))\n",
    "#     print(mislabelled_indices)\n",
    "    print(\"Total Mislabelled Images: \"+str(len(mislabelled_indices[0])))\n",
    "    \n",
    "    if(dataset == \"training\"):\n",
    "        visual_title = \"Sample Mislabelled Training Images\"\n",
    "    elif(dataset == \"dev\"):\n",
    "        visual_title = \"Sample Mislabelled Dev Images\"\n",
    "    elif(dataset == \"test\"):\n",
    "        visual_title = \"Sample Mislabelled Test Images\"\n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    fig.suptitle(visual_title)\n",
    "\n",
    "    for ax,i in zip(axes.flatten(),mislabelled_indices[1]):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest')\n",
    "        ax.set(title = \"True: \"+ str(y_orig[0,i])+\" | Predicted: \"+str(predicted_labels[0,i]))\n",
    "        ax.set(xlabel= \"Prediction Prob: %f\"%(prediction_prob[0,i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mislabelled_images(train_x_sample, train_y_sample.T,predicted_labels_train, prediction_prob_train,dataset = \"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Real Time images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dataPrep import one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"8_1.jpg\" \n",
    "label = np.array([8]).reshape(1,1)\n",
    "\n",
    "fname = \"dataset/\" + image_name\n",
    "\n",
    "# image_data = np.asarray(plt.imread(fname))\n",
    "image_data =255 - np.asarray(Image.open(fname).convert('L').resize((28,28)))\n",
    "# print(image_data)\n",
    "image_flattened = image_data.reshape(image_data.shape[0]*image_data.shape[1],-1)\n",
    "# print(image_flattened.shape)\n",
    "image_norm =(image_flattened/255.)\n",
    "\n",
    "label_encoded = one_hot_encoding(label)\n",
    "# print(label_encoded)\n",
    "\n",
    "pridected_label,pred_prob,acc = predict(image_norm, label_encoded, parameters)\n",
    "\n",
    "plt.title(\"True Label: \"+ str(label.squeeze()))\n",
    "plt.xlabel(\"Predicted: %d | With Prob: %.4f | Acc: %.1f\"%(pridected_label.squeeze(), pred_prob.squeeze(),acc))\n",
    "plt.imshow(image_data, interpolation ='nearest',cmap='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
