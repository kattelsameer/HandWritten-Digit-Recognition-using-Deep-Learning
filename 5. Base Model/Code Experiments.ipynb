{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Libraries for importing data from binary file\n",
    "import os.path #for accessing the file path\n",
    "import struct  #for unpacking the binary data\n",
    "\n",
    "import time    #for calculating time\n",
    "\n",
    "#core packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#custom module\n",
    "from dataPrep import load_dataset, load_sample_dataset\n",
    "from dataPrep import visual_charts, visualize_dataset\n",
    "from dataPrep import prep_dataset\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading complete dataset\n",
    "train_x_orig, train_y_orig, dev_x_orig,dev_y_orig,test_x_orig,test_y_orig = load_dataset()\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(type(dev_x_orig))+\"\\t\",str(dev_x_orig.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(type(dev_y_orig))+\"\\t\",str(dev_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_orig))+\"\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_orig))+\"\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Sample dataset\n",
    "sample_size = 100\n",
    "train_x_sample, train_y_sample, dev_x_sample, dev_y_sample, test_x_sample, test_y_sample = load_sample_dataset(sample_size)\n",
    "\n",
    "print(\"Sample Size : %d%%\\n\"%(sample_size))\n",
    "print(\"Data\\t\\t\\t\",\"Complete Dataset\\t\",\"Sample Dataset\\t\")\n",
    "print(\"================================================================\")\n",
    "print(\"Training Set Images:\\t\"+ str(train_x_orig.shape)+\"\\t\\t\"+ str(train_x_sample.shape))\n",
    "print(\"Training Set Labels:\\t\"+ str(train_y_orig.shape)+\"\\t\\t\"+ str(train_y_sample.shape))\n",
    "print(\"Training Set Images:\\t\"+ str(dev_x_orig.shape)+\"\\t\\t\"+ str(dev_x_sample.shape))\n",
    "print(\"Training Set Labels:\\t\"+ str(dev_y_orig.shape)+\"\\t\\t\"+ str(dev_y_sample.shape))\n",
    "print(\"Test Set Images:\\t\"+str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_sample.shape))\n",
    "print(\"Test Set Labels:\\t\"+str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_sample.shape))\n",
    "print(\"================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Dataset (Flattening and Normalizing)\n",
    "train_x_norm,train_y_encoded, dev_x_norm,dev_y_encoded, test_x_norm, test_y_encoded = prep_dataset(train_x_sample, train_y_sample, dev_x_sample, dev_y_sample, test_x_sample, test_y_sample)\n",
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_x_orig.shape)+\"\\t\\t\"+ str(train_x_norm.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_y_orig.shape)+\"\\t\\t\"+ str(train_y_encoded.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(dev_x_orig.shape)+\"\\t\\t\"+ str(dev_x_norm.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(dev_y_orig.shape)+\"\\t\\t\"+ str(dev_y_encoded.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_norm.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_encoded.shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0.0,Z)\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([1,2,3,4])\n",
    "A,cache = relu(Z)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "#     dZ[Z <= 0] = 0\n",
    "    dZ[Z < 0] = 0\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    shift = Z - np.max(Z) #Avoiding underflow or overflow errors due to floating point instability in softmax\n",
    "    t = np.exp(shift)\n",
    "#     t = np.exp(Z)\n",
    "    A = np.divide(t,np.sum(t,axis = 0))\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = np.array([5,2,-1,3]).reshape(4,1)\n",
    "Z= np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]).reshape(7,1)\n",
    "print(Z.shape)\n",
    "A,cache = softmax(Z)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(dA,cache):\n",
    "    Z = cache\n",
    "    s = dA.reshape(-1,1)\n",
    "#     dZ = np.diagflat(s) - np.dot(s,s.T)\n",
    "    dZ = np.multiply(dA,(1-dA))\n",
    "#     assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[5],[2],[-1],[3]]).reshape(4,1) \n",
    "Y = np.array([1,0,0,0]).reshape(4,1)\n",
    "A,cache = softmax(Z)\n",
    "print(A)\n",
    "dA = A - Y\n",
    "print(dA)\n",
    "dZ = softmax_grad(dA,cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers():\n",
    "    layers_dim = [784,32,16,10]\n",
    "    return layers_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dim = init_layers()\n",
    "print(\"Layer\\t\\tNodes\")\n",
    "print(\"======================\")\n",
    "for layer,node in enumerate(layers_dim):\n",
    "    print(str(layer) + \"\\t\\t\" + str(node))\n",
    "\n",
    "print(\"======================\")\n",
    "\n",
    "print(\"No. of Hidden Layers: \" + str(len(layers_dim)-2))\n",
    "print(\"Total No. of Layers: \" + str(len(layers_dim)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers_dim):\n",
    "    \n",
    "    L = len(layers_dim)\n",
    "    params = {}\n",
    "        \n",
    "    for l in range(1,L):\n",
    "        params['W' + str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) *0.01\n",
    "        params['b' + str(l)] = np.zeros((layers_dim[l],1))\n",
    "     \n",
    "        assert(params['W' + str(l)].shape == (layers_dim[l],layers_dim[l-1]))\n",
    "        assert(params['b' + str(l)].shape == (layers_dim[l],1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = init_params(layers_dim)\n",
    "print(\"Layer\\tWeight\\t\\tBias\")\n",
    "print(\"================================\")\n",
    "for l in range(1,len(layers_dim)):\n",
    "    print(str(l) +\"\\t\" + str(parameters['W'+str(l)].shape) +\"\\t\"+ str(parameters['b'+str(l)].shape))\n",
    "    print()    \n",
    "print(\"Total Connections: \"+ str(784*16*16*10))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hyperParams(alpha = 0.01,ite = 5000):\n",
    "    hyperParams = {}\n",
    "    hyperParams['learning_rate'] = alpha\n",
    "    hyperParams['num_iterations'] = ite\n",
    "    \n",
    "    \n",
    "    return hyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = init_hyperParams(alpha = 0.01,ite = 1000)\n",
    "print(\"Learning Rate:\\t\"+ str(hyperParams['learning_rate']))\n",
    "print(\"Epoch:\\t\"+ str(hyperParams['num_iterations']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sum(A,W,b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    assert(Z.shape == (W.shape[0],Z.shape[1]))\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A,W,b,activation):\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, sum_cache = forward_sum(A,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    if activation == 'softmax':\n",
    "        Z, sum_cache = forward_sum(A,W,b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    cache = (sum_cache,activation_cache)\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "A, linear_activation_cache = forward_activation(X, W, b, activation = \"softmax\")\n",
    "print(\"With softmax: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = forward_activation(X, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = forward_activation(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation='relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = forward_activation(A,parameters['W' + str(L)],parameters['b' + str(L)],activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (10,X.shape[1]))\n",
    "    \n",
    "    return AL,caches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = forward_prop(train_x_norm,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "#     cost = (1./m) * np.sum(-np.dot(Y,np.log(AL).T))\n",
    "    cost = -(1./m) * np.sum(np.sum(np.multiply(Y,np.log(AL)), axis = 0,keepdims=True))\n",
    "    \n",
    "    \n",
    "    cost = np.squeeze(cost)      # Making sure your cost's shape is not returned as ndarray\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = compute_cost(AL,train_y_encoded)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[0,1,0,0],[1,0,0,0]]).reshape(4,2)\n",
    "AL = np.array([[0.1,0.7,0.1,0.1],[0.6,0.2,0.1,0.1]]).reshape(4,2)\n",
    "cost = compute_cost(AL,Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_grad(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims=True )\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA,cache,activation):\n",
    "    sum_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_grad(dA,activation_cache)\n",
    "        dA_prev, dW, db = backward_grad(dZ, sum_cache)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = backward_grad(dA, sum_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dA = np.subtract(AL,Y)\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_activation(dA, current_cache, activation = 'softmax')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_activation(grads[\"dA\" + str(l + 1)], current_cache, activation = 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,y,parameters):\n",
    "    m = y.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = forward_prop(X, parameters)\n",
    "   \n",
    "    assert(probas.shape == y.shape)\n",
    "        \n",
    "    predicted_labels = np.argmax(probas,axis=0).reshape(1,probas.shape[1])\n",
    "    predicted_prob = np.max(probas,axis = 0).reshape(1,m)\n",
    "    \n",
    "    Y = np.argmax(y,axis=0).reshape(1,y.shape[1])\n",
    "    \n",
    "\n",
    "    #print results\n",
    "    true_prediction = np.equal(predicted_labels,Y)\n",
    "#     print(true_prediction.shape)\n",
    "    \n",
    "    num_correct_labels = np.sum(true_prediction)\n",
    "    num_incorrect_labels = m - num_correct_labels\n",
    "    accuracy = num_correct_labels/m\n",
    "#     print(\"No. of Correct Prediction:\\t\"+str(num_correct_labels))\n",
    "#     print(\"No. of Incorrect Prediction:\\t\"+str(num_incorrect_labels))\n",
    "#     print(\"\\nAccuracy: \"  + str(accuracy*100)+\"%\")\n",
    "#     print(\"\\nError:\\t\"+str((1-accuracy)*100)+\"%\")\n",
    "        \n",
    "    return predicted_labels, predicted_prob, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(attr, attr_type):\n",
    "    \n",
    "    plt.plot(np.squeeze(attr))\n",
    "    if attr_type == 'costs':\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.title(\"Cost\")\n",
    "        \n",
    "    elif attr_type == 'train_accs':\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.title(\"Training Accuracy\")\n",
    "        \n",
    "    elif attr_type == 'val_accs':\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.title(\"Validation Accuracy\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "        \n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, X_dev, Y_dev, layers_dim, hyperParams):\n",
    "\n",
    "    learning_rate = hyperParams['learning_rate']\n",
    "    num_iterations = hyperParams['num_iterations']\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []              # keep track of cost\n",
    "    train_accs = []  # keep track of training accuracy\n",
    "    val_accs = []     # keep track of Validation accuracy\n",
    "    \n",
    "    parameters = init_params(layers_dim)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = forward_prop(X_train, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y_train)\n",
    "    \n",
    "        grads = backward_prop(AL, Y_train, caches)\n",
    " \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        _,_,train_acc = predict(X_train, Y_train,parameters)\n",
    "        _,_,val_acc= predict(X_dev, Y_dev,parameters)        \n",
    "        \n",
    "        if i % 200 == 0 or i == num_iterations:\n",
    "            print (\"Iteration: %d == Cost: %f || Training acc: %f || Val acc: %f\"%(i,cost,train_acc,val_acc))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            \n",
    "    visualize_results(costs, attr_type='costs')  \n",
    "    visualize_results(train_accs, attr_type='train_accs')       \n",
    "    visualize_results(val_accs, attr_type='val_accs')       \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = init_hyperParams(alpha = 0.05,ite = 5000)\n",
    "layers_dim = init_layers()\n",
    "parameters = train(train_x_norm, train_y_encoded,dev_x_norm, dev_y_encoded,layers_dim, hyperParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_train, prediction_prob_train,train_acc = predict(train_x_norm, train_y_encoded,parameters)\n",
    "print(\"\\nAccuracy: \"  + str(train_acc*100)+\"%\")\n",
    "print(\"\\nError:\\t\"+str((1-train_acc)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(y_orig,y_predicted):\n",
    "    m = y_orig.shape[0]\n",
    "    k = len(np.unique(y_orig)) # or simply take k =10\n",
    "    \n",
    "    cm = np.zeros((k,k))\n",
    "\n",
    "    for i in range(m):\n",
    "        cm[y_orig[i],y_predicted[i]] += 1\n",
    "   \n",
    "    return cm.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(train_y_sample,predicted_labels_train.T)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = plt.imshow(cm,cmap=\"GnBu\") #RdYlGn, PiYG, Accent,Blues,viridis, YlGnBu\n",
    "\n",
    "\n",
    "    fig.colorbar(im,ax=ax,fraction=0.045)\n",
    "    # ax.set_aspect('auto')\n",
    "\n",
    "#     ax.set_title(\"Confusion Matrix\",fontsize=24)\n",
    "    ax.set_xticks(range(0,10))\n",
    "    ax.set_yticks(range(0,10))\n",
    "    ax.set_xlabel(\"Predicted\", fontsize = 20)\n",
    "    ax.set_ylabel(\"Expexted\", fontsize = 20)\n",
    "\n",
    "    ax.set_xticklabels([0,1,2,3,4,5,6,7,8,9],fontsize=16)\n",
    "    ax.set_yticklabels([0,1,2,3,4,5,6,7,8,9],fontsize=16)\n",
    "\n",
    "    #setting horizontal axes labeling to top.\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax.tick_params(top=False,left=False)\n",
    "\n",
    "\n",
    "    thres = cm.max()//2\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            per = cm[i,j]/cm.sum() * 100\n",
    "            text = ax.text(j, i, \"%d\\n%.2f%%\"%(cm[i, j], per),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\" if cm[i,j] > thres else \"black\")\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrix   \n",
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, cm):\n",
    "    col = cm[:, label]\n",
    "    prec = cm[label, label] / col.sum()\n",
    "    return prec\n",
    "    \n",
    "def recall(label, cm):\n",
    "    row = cm[label, :]\n",
    "    rec = cm[label, label] / row.sum()\n",
    "    return rec\n",
    "\n",
    "def precision_average(prec):\n",
    "    count = len(prec)    \n",
    "    prec_mac_avg = np.sum(prec) / count\n",
    "    return prec_mac_avg\n",
    "\n",
    "def recall_average(rec):\n",
    "    count = len(rec)\n",
    "    rec_mac_avg = np.sum(rec) / count\n",
    "    return rec_mac_avg\n",
    "\n",
    "def accuracy(cm):\n",
    "    diagonal_sum = cm.trace()\n",
    "    sum_of_all_elements = cm.sum()\n",
    "    acc = diagonal_sum / sum_of_all_elements \n",
    "    return acc\n",
    "\n",
    "def f1_score(prec,rec):\n",
    "    f1 = (2 * prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "def summary(cm):\n",
    "    print(\"+=======+===============+===============+==========+\")\n",
    "    print(\"| Label\\t| Precision \\t| Recall \\t| F1 Score |\")\n",
    "    print(\"+=======+===============+===============+==========+\")\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    for label in range(10):\n",
    "        prec.append(precision(label, cm))\n",
    "        rec.append(recall(label, cm))\n",
    "        f1.append(f1_score(prec[label], rec[label]))\n",
    "        print(\"| %d \\t|  %.3f \\t|  %.3f \\t|  %.3f   |\"%(label, prec[label], rec[label], f1[label]))\n",
    "\n",
    "\n",
    "    print(\"+=======+===============+===============+==========+\") \n",
    "    avg_precision = precision_average(prec)\n",
    "    avg_recall = recall_average(rec)\n",
    "    acc = accuracy(cm)\n",
    "    print(\"\\nAvg Precision:\\t\"+ str(avg_precision) )\n",
    "    print(\"Avg Recall:\\t\"+ str(avg_recall) )\n",
    "    print(\"Accuracy:\\t\"+ str(acc))\n",
    "# plt.bar(range(0,10),p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(x_orig, y_orig, predicted_labels, prediction_prob, dataset):\n",
    "#     print(x_orig.shape,y_orig.shape, predicted_labels.shape)\n",
    "    if(dataset == \"training\"):\n",
    "        visual_title = \"Sample Training Data Set\"\n",
    "        rng = range(30,40)\n",
    "    elif(dataset == \"dev\"):\n",
    "        visual_title = \"Sample Dev Data Set\"\n",
    "        rng = range(110,120)\n",
    "    elif(dataset == \"test\"):\n",
    "        visual_title = \"Sample Test Data Set\"\n",
    "        rng = range(110,120)        \n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    fig.suptitle(visual_title)\n",
    "\n",
    "    for ax,i in zip(axes.flatten(),rng):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest', cmap='Greys')\n",
    "        ax.set(title = \"True: \"+ str(y_orig[0,i])+\" | Predicted: \"+str(predicted_labels[0,i]))\n",
    "        ax.set(xlabel= \"Prediction Prob: %f\"%(prediction_prob[0,i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(train_x_sample, train_y_sample.T,predicted_labels_train, prediction_prob_train,dataset = \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mislabelled_images(x_orig,y_orig,predicted_labels,prediction_prob,dataset):\n",
    "    true_prediction = np.equal(predicted_labels,y_orig)\n",
    "    mislabelled_indices = np.asarray(np.where(true_prediction == False))\n",
    "#     print(mislabelled_indices)\n",
    "    print(\"Total Mislabelled Images: \"+str(len(mislabelled_indices[0])))\n",
    "    \n",
    "    if(dataset == \"training\"):\n",
    "        visual_title = \"Sample Mislabelled Training Images\"\n",
    "    elif(dataset == \"dev\"):\n",
    "        visual_title = \"Sample Mislabelled Dev Images\"\n",
    "    elif(dataset == \"test\"):\n",
    "        visual_title = \"Sample Mislabelled Test Images\"\n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    fig.suptitle(visual_title)\n",
    "\n",
    "    for ax,i in zip(axes.flatten(),mislabelled_indices[1]):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest')\n",
    "        ax.set(title = \"True: \"+ str(y_orig[0,i])+\" | Predicted: \"+str(predicted_labels[0,i]))\n",
    "        ax.set(xlabel= \"Prediction Prob: %f\"%(prediction_prob[0,i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mislabelled_images(train_x_sample, train_y_sample.T,predicted_labels_train, prediction_prob_train,dataset = \"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Real Time images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dataPrep import one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"8_2.jpg\" \n",
    "label = np.array([8]).reshape(1,1)\n",
    "\n",
    "fname = \"dataset/\" + image_name\n",
    "\n",
    "# image_data = np.asarray(plt.imread(fname))\n",
    "image_data =np.asarray(Image.open(fname).convert('L').resize((28,28)))\n",
    "# print(image_data)\n",
    "image_flattened = image_data.reshape(image_data.shape[0]*image_data.shape[1],-1)\n",
    "# print(image_flattened.shape)\n",
    "image_norm =(image_flattened/255.)\n",
    "\n",
    "label_encoded = one_hot_encoding(label)\n",
    "# print(label_encoded)\n",
    "\n",
    "pridected_label,pred_prob,acc = predict(image_norm, label_encoded, parameters)\n",
    "\n",
    "plt.title(\"True Label: \"+ str(label.squeeze()))\n",
    "plt.xlabel(\"Predicted: %d | With Prob: %.4f | Acc: %.1f\"%(pridected_label.squeeze(), pred_prob.squeeze(),acc))\n",
    "plt.imshow(image_data, interpolation ='nearest',cmap='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
