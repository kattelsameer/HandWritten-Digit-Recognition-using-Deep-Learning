{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time    #for calculating time\n",
    "import math    #for using floor in creating minibatches\n",
    "import pickle  #for saving model\n",
    "\n",
    "\n",
    "#core packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#custom module\n",
    "from  dataset import load_dataset, train_dev_split, prep_dataset\n",
    "from dataset import visualize_data_distribution, visualize_dataset, label_description\n",
    "\n",
    "# from finalModelUtils import relu, relu_grad, softmax\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size : 50%\n",
      "\n",
      "Data\t\t\t Datatype\t\t Dataset Size\n",
      "=================================================================\n",
      "Training Set Images:\t<class 'numpy.ndarray'>\t (30000, 28, 28)\n",
      "Training Set Labels:\t<class 'numpy.ndarray'>\t (30000, 1)\n",
      "Test Set Images:\t<class 'numpy.ndarray'>\t (5000, 28, 28)\n",
      "Test Set Labels:\t<class 'numpy.ndarray'>\t (5000, 1)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset_size_in_per = 50\n",
    "\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig = load_dataset(dataset = \"mnist\", size_in_per = dataset_size_in_per)\n",
    "\n",
    "print(\"Sample Size : %d%%\\n\"%(dataset_size_in_per))\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Dataset Size\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_orig))+\"\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_orig))+\"\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Dev set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t\t Datatype\t\t Shape\n",
      "========================================================================\n",
      "Training Set Images:\t\t<class 'numpy.ndarray'>\t (25500, 28, 28)\n",
      "Training Set Labels:\t\t<class 'numpy.ndarray'>\t (25500, 1)\n",
      "Development Set Images:\t\t<class 'numpy.ndarray'>\t (4500, 28, 28)\n",
      "Development Set Labels:\t\t<class 'numpy.ndarray'>\t (4500, 1)\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "train_x_split, train_y_split, dev_x_split, dev_y_split = train_dev_split(train_x_orig, train_y_orig)\n",
    "\n",
    "print(\"Data\\t\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"========================================================================\")\n",
    "print(\"Training Set Images:\\t\\t\" + str(type(train_x_split))+\"\\t\",str(train_x_split.shape))\n",
    "print(\"Training Set Labels:\\t\\t\" + str(type(train_y_split))+\"\\t\",str(train_y_split.shape))\n",
    "print(\"Development Set Images:\\t\\t\" + str(type(dev_x_split))+\"\\t\",str(dev_x_split.shape))\n",
    "print(\"Development Set Labels:\\t\\t\" + str(type(dev_y_split))+\"\\t\",str(dev_y_split.shape))\n",
    "print(\"========================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_distribution(train_y_split, dataset_type = \"training\")\n",
    "visualize_data_distribution(dev_y_split,  dataset_type = \"dev\")\n",
    "visualize_data_distribution(test_y_orig,  dataset_type = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_x_split, train_y_split, dataset = \"mnist\", dataset_type = \"training\")\n",
    "visualize_dataset(dev_x_split, dev_y_split, dataset = \"mnist\", dataset_type = \"dev\")\n",
    "visualize_dataset(test_x_orig, test_y_orig, dataset = \"mnist\", dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(25500, 28, 28)\t\t(784, 25500)\n",
      "Training Set Labels:\t(25500, 1)\t\t(10, 25500)\n",
      "Dev Set Images:\t\t(4500, 28, 28)\t\t(784, 4500)\n",
      "Dev Set Labels:\t\t(4500, 1)\t\t(10, 4500)\n",
      "Test Set Images:\t(5000, 28, 28)\t\t(784, 5000)\n",
      "Test Set Labels:\t(5000, 1)\t\t(10, 5000)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "train_x_norm, train_y_encoded = prep_dataset(train_x_split, train_y_split, num_class = 10)\n",
    "dev_x_norm, dev_y_encoded= prep_dataset(dev_x_split, dev_y_split, num_class = 10)\n",
    "test_x_norm, test_y_encoded = prep_dataset(test_x_orig, test_y_orig, num_class = 10)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_x_split.shape)+\"\\t\\t\"+ str(train_x_norm.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_y_split.shape)+\"\\t\\t\"+ str(train_y_encoded.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(dev_x_split.shape)+\"\\t\\t\"+ str(dev_x_norm.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(dev_y_split.shape)+\"\\t\\t\"+ str(dev_y_encoded.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_norm.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_encoded.shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu function\n",
    "def relu(Z):\n",
    "    \"\"\"Compute the ReLU activation of Z.\n",
    "        \n",
    "        Arguments:\n",
    "            Z (numpy.ndarray): Input Sum to a hidden unit, Z = W * X + b.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Following values\n",
    "            - **A** (numpy.ndarray): Activation obtained by applying ReLU function to the sum. Size same as that of Z.\n",
    "            - **cache** (numpy.ndarray): Value stored for use during backward propagation.\n",
    "        \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> Z = np.random.randn(1,6)\n",
    "            >>> **A,cache = relu(Z)**\n",
    "            >>> print(A)\n",
    "            \n",
    "            Output: \n",
    "                [[1.62434536 0.     0.      0.      0.86540763 0.    ]]\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0.0,Z)\n",
    "    \n",
    "    cache = Z #storing Z for later use during back propagation\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu gradient function\n",
    "def relu_grad(dA, cache):\n",
    "    \"\"\"Compute the backward ReLU activation of dA.\n",
    "        \n",
    "        Arguments:\n",
    "            dA (numpy.ndarray): Gradient of activation of the previous layer.\n",
    "            cache (numpy.ndarray): Value of Z stored during forward prop.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: - **dZ**: array of gradient/derivative of the dA, Same size of dA.\n",
    "            \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> dA = np.random.randn(1,6)\n",
    "            >>> cache = np.random.randn(1,6)\n",
    "            >>> **dZ = relu_grad(dA,cache)**\n",
    "            >>> print(dZ)\n",
    "            \n",
    "            Output:\n",
    "                [[ 1.62434536  0.         -0.52817175  0.          0.86540763  0.        ]]\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0 #implementing integrated form of (gradiant of ReLU function * gradient of the loss function)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"Compute the softmax activtion of Z.\n",
    "        \n",
    "        Argument:\n",
    "            Z (numpy.ndarray): Input Sum to a hidden unit, Z = W * X + b.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Following Values\n",
    "            - **A** (numpy.ndarray): Activation obtained by applying softmax function to the sum. Size same as that of Z.\n",
    "            - **cache** (numpy.ndarray): Value stored for use during backward propagation.\n",
    "            \n",
    "        Example:\n",
    "            >>> np.random.seed(2)\n",
    "            >>> Z= np.random.rand(7,1)\n",
    "            >>> **A,cache = softmax(Z)**\n",
    "            >>> print(A)\n",
    "            \n",
    "            Output:\n",
    "                [[0.15477477]\n",
    "                 [0.10270926]\n",
    "                 [0.17340649]\n",
    "                 [0.15467071]\n",
    "                 [0.15237489]\n",
    "                 [0.13925557]\n",
    "                 [0.12280831]]\n",
    "    \"\"\"\n",
    "    shift = Z - np.max(Z) #Avoiding underflow or overflow errors due to floating point instability in softmax\n",
    "    t = np.exp(shift)\n",
    "    A = np.divide(t,np.sum(t,axis = 0))\n",
    "    \n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_mini_batches(X, Y, minibatch_size = 64, seed=1):\n",
    "    \"\"\"Returns the minibatches of X and corresponding Y of the given size.\n",
    "    \n",
    "        Arguments:\n",
    "            X (numpy.ndarray): Inputs Array.\n",
    "            Y (numpy.ndarray): Output Labels.\n",
    "            minibatch_size (int): Size of each minibatch.\n",
    "            seed (int): Seed value for randomness.\n",
    "        \n",
    "        Returns:\n",
    "            list: - **minibatches**: List of minibatches where each minibatch contains a minibatch of X and a minibatch of its corresponding Y.\n",
    "            \n",
    "        Examples:\n",
    "            >>> X = np.random.randn(20,20)\n",
    "            >>> Y = np.random.rand(1,20)\n",
    "            >>> **minibatches = rand_mini_batches(X,Y,minibatch_size = 4, seed = 2)**\n",
    "            >>> print(minibatches[0][0].shape)\n",
    "            >>> print(minibatches[0][1].shape)\n",
    "            \n",
    "            Outputs:\n",
    "                (20, 4)\n",
    "                (1, 4)\n",
    "    \"\"\"\n",
    "   \n",
    "    classes = Y.shape[0]\n",
    "    np.random.seed(seed) # varying the seed value so that the minibatchs become random in each epoch\n",
    "    m = X.shape[1]       # number of training examples\n",
    "    minibatches = []\n",
    "        \n",
    "    #Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((classes,m))\n",
    "\n",
    "    #Partition (shuffled_X, shuffled_Y) except for the last batch\n",
    "    num_complete_minibatches = math.floor(m/minibatch_size) # number of mini batches of size minibatch_size \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[:, k * minibatch_size : (k+1)*minibatch_size]\n",
    "        minibatch_Y = shuffled_Y[:, k * minibatch_size : (k+1)*minibatch_size]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "    \n",
    "    # Last batch (last minibatch <= minibatch_size)\n",
    "    if m % minibatch_size != 0:\n",
    "        minibatch_X = shuffled_X[:, num_complete_minibatches * minibatch_size : m]\n",
    "        minibatch_Y = shuffled_Y[:, num_complete_minibatches * minibatch_size : m]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "    \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time Conversion\n",
    "def convert_time(millisec):\n",
    "    \"\"\"Converts time in miliseconf to higher values.\n",
    "    \n",
    "        Arguments:\n",
    "            milisec (int): Time in mili-seconds.\n",
    "        \n",
    "        Returns: \n",
    "            tuple: Following values\n",
    "            - **hours** (int):  Time in hours.\n",
    "            - **mins** (int): Time in minutes.\n",
    "            - **secs** (int):  Time in seconds.\n",
    "            - **milisec** (int): Time in mili-seconds.\n",
    "            \n",
    "        Example:\n",
    "            >>> **hr,mins,sec,milisec = convert_time(millisec = 12450)**\n",
    "            >>> print(\"%dhr %dmins %ds %dms\"%(hr,mins,sec,milisec))\n",
    "            \n",
    "            Outputs:\n",
    "                0hr 0mins 12s 450ms\n",
    "    \"\"\"\n",
    "    #converting millisecons to hours, minutes, seconds and millisecond\n",
    "    #the large numbers like 3.6e+6 comes from the relation between the time units\n",
    "    hours = millisec // 3.6e+6\n",
    "    mins = (millisec % 3.6e+6) // 60000\n",
    "    secs = ((millisec % 3.6e+6) % 60000) // 1000\n",
    "    millisec = ((millisec % 3.6e+6) % 60000) % 1000\n",
    "    \n",
    "    return (hours, mins, secs, millisec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing and Visualizing Evaluation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Confusion Matrix\n",
    "def confusion_matrix(y_orig,prediction):\n",
    "    \"\"\"Returns a confusion matrix for a given output labels and prediction.\n",
    "    \n",
    "        Arguments:\n",
    "            y_orig (numpy.ndarray): Original Output Labels of shape(m,1); m = # of examples.\n",
    "            prediction (numpy.ndarray): Predicted Labels of the dataset of shape (1,m).\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray:- **cm**: 2D confusion matrix.\n",
    "            \n",
    "        Example:\n",
    "            >>> y = np.array([[1,2,3,3,0,2,5,4,2,2]]).reshape(10,1)\n",
    "            >>> pred = np.array([[2,2,1,3,0,2,5,4,2,2]]).reshape(1,10)\n",
    "            >>> prediction = {\"First Prediction\":pred}\n",
    "            >>> **cm_train = confusion_matrix(y,prediction)**\n",
    "            >>> print(cm_train)\n",
    "            \n",
    "            Output:\n",
    "                [[1 0 0 0 0 0]\n",
    "                 [0 0 1 0 0 0]\n",
    "                 [0 0 4 0 0 0]\n",
    "                 [0 1 0 1 0 0]\n",
    "                 [0 0 0 0 1 0]\n",
    "                 [0 0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "    first_predict = prediction[\"First Prediction\"]\n",
    "    \n",
    "    y_predicted = first_predict[0].T\n",
    "    \n",
    "    m = y_orig.shape[0]\n",
    "    classes = len(np.unique(y_orig)) # or simply take classes = 10 for mnist or fashion-mnist\n",
    "    \n",
    "    cm = np.zeros((classes,classes)) #creating the matrix frame for the confusion matrix\n",
    "    \n",
    "    # generating the values in the confusion metrix\n",
    "    for i in range(m):\n",
    "        cm[y_orig[i],y_predicted[i]] += 1\n",
    "   \n",
    "    return cm.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Confusion Matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, dataset_type, dataset = \"mnist\"):\n",
    "    \"\"\"Plots the Heatmap of the confusion matrix.\n",
    "    \n",
    "        Arguments:\n",
    "            cm (numpy.ndarray): 2D confusion matrix.\n",
    "            dataset_type (str): Type of dataset. May be training or dev or test.\n",
    "            dataset (str): Dataset used to train the model. Default to 'mnist'\n",
    "            \n",
    "        Example:\n",
    "            >>> plot_confusion_matrix(cm, dataset_type)\n",
    "    \"\"\"\n",
    "    # plotting the metrix\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = plt.imshow(cm,cmap=\"GnBu\") #RdYlGn, PiYG, Accent,Blues,viridis, YlGnBu\n",
    "    \n",
    "    # plotting the color bar of the plot size\n",
    "    fig.colorbar(im,ax=ax,fraction=0.045)\n",
    "    \n",
    "    if(len(dataset_type) != 0):\n",
    "        visual_title = \"Confusion Matrix for %s Set \"%dataset_type.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "        \n",
    "        \n",
    "    #getting the label description\n",
    "    label_desc = label_description(dataset)\n",
    "    desc = [label_desc[i] for i in range(0,10)]\n",
    "    \n",
    "    # annotating the plot\n",
    "    ax.set_title(visual_title,fontsize=24,pad = 20)    \n",
    "    ax.set_xticks(np.arange(10))\n",
    "    ax.set_yticks(np.arange(10))    \n",
    "    ax.set_xlabel(\"Predicted\", fontsize = 20)\n",
    "    ax.set_ylabel(\"Expexted\", fontsize = 20)\n",
    "    \n",
    "    ax.set_xticklabels(desc)\n",
    "    ax.set_yticklabels(desc)\n",
    "    \n",
    "    #setting horizontal axes labeling to top.\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "\n",
    "    # creating the threshold for color change in the visualization\n",
    "    thres = cm.max()//2\n",
    "    \n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            #calculating the percentage of the total image denoted by the cell\n",
    "            per = cm[i,j]/cm.sum() * 100\n",
    "            #putting up the text inside the plot cells\n",
    "            ax.text(j, i, \"%d\\n%.2f%%\"%(cm[i, j], per),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\" if cm[i,j] > thres else \"black\")\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating precision, Recall and F1-Score\n",
    "def precision(label, cm):\n",
    "    \"\"\"Returns the precision for the prediction of an individual label.\n",
    "    \n",
    "        Arguments:\n",
    "            label (int): unique labels (each class).\n",
    "            cm (numpy.ndarray): 2D confusion matrix.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.float64: - **prec**: Precision for the given label.\n",
    "            \n",
    "        Example:\n",
    "            if cm = [[1 0 0 0 0 0]\n",
    "                     [0 0 1 0 0 0]\n",
    "                     [0 0 4 0 0 0]\n",
    "                     [0 1 0 1 0 0]\n",
    "                     [0 0 0 0 1 0]\n",
    "                     [0 0 0 0 0 1]]\n",
    "            >>> label = 2\n",
    "            >>> prec = precision(label, cm)\n",
    "            >>> print(prec)\n",
    "            \n",
    "            Output:\n",
    "                0.8\n",
    "    \"\"\"\n",
    "    col = cm[:, label] #selecting the True Positive and false positive values\n",
    "    prec = cm[label, label] / col.sum() #perc = TP / (TP + FP)\n",
    "    return prec\n",
    "    \n",
    "def recall(label, cm):\n",
    "    \"\"\"Returns the recall for the prediction of an individual label.\n",
    "    \n",
    "        Arguments:\n",
    "            label (int): unique labels (each class).\n",
    "            cm (numpy.ndarray): 2D confusion matrix.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.float64: - **rec**: Recall for the given label.\n",
    "            \n",
    "        Example:\n",
    "            if cm = [[1 0 0 0 0 0]\n",
    "                     [0 0 1 0 0 0]\n",
    "                     [0 0 4 0 0 0]\n",
    "                     [0 1 0 1 0 0]\n",
    "                     [0 0 0 0 1 0]\n",
    "                     [0 0 0 0 0 1]]\n",
    "            >>> label = 2\n",
    "            >>> rec = recall(label, cm)\n",
    "            >>> print(rec)\n",
    "            \n",
    "            Output:\n",
    "                1.0\n",
    "    \"\"\"\n",
    "    row = cm[label, :] #selecting the True Positive and false Negative values\n",
    "    rec = cm[label, label] / row.sum() # rec = TP / (TP + FN)\n",
    "    return rec\n",
    "\n",
    "def f1_score(prec,rec):\n",
    "    \"\"\"Returns the f1-score for the prediction of an individual label.\n",
    "    \n",
    "        Arguments:\n",
    "            prec (numpy.float64): precision for a label.\n",
    "            rec (numpy.float64): recall for a label.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.float64: - **f1**: f1-score for the precision and recall.\n",
    "            \n",
    "        Example:\n",
    "            >>> prec = 0.8\n",
    "            >>> rec = 1.0\n",
    "            >>> f1 = f1_score(prec,rec)\n",
    "            >>> print(f1)\n",
    "            \n",
    "            Output:\n",
    "                0.888888888888889\n",
    "    \"\"\"\n",
    "    f1 = (2 * prec * rec) / (prec + rec)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculating macro precision, recall and f1-score\n",
    "def macro_precision_average(precs):\n",
    "    \"\"\"Returns the macro average of the precisions for the prediction of all the label.\n",
    "    \n",
    "        Arguments:\n",
    "            precs (list): lists of precisions of type numpy.float64.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.float64: - **prec_mac_avg**: macro average of the precisions.\n",
    "            \n",
    "        Example:\n",
    "            >>> avg_precision = macro_precision_average(prec)\n",
    "    \"\"\"\n",
    "    count = len(precs)    \n",
    "    prec_mac_avg = np.sum(precs) / count\n",
    "    return prec_mac_avg\n",
    "\n",
    "def macro_recall_average(recs):\n",
    "    \"\"\"Returns the macro average of the recall for the prediction of all the label.\n",
    "    \n",
    "        Arguments:\n",
    "            recs (list): lists of recalls of type numpy.float64.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.float64: - **rec_mac_avg**: macro average of the recalls.\n",
    "            \n",
    "        Example:\n",
    "            >>> avg_recall = macro_recall_average(rec)\n",
    "    \"\"\"\n",
    "    count = len(recs)\n",
    "    rec_mac_avg = np.sum(recs) / count\n",
    "    return rec_mac_avg\n",
    "\n",
    "def macro_f1_score(f1s):\n",
    "    \"\"\"Returns the macro average of the f1-score for the prediction of all the label.\n",
    "    \n",
    "        Arguments:\n",
    "            f1s (list): lists of f1-scores of type numpy.float64.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.float64: - **f1_mac_avg**: macro average of the f1-scores.\n",
    "            \n",
    "        Example:\n",
    "            >>> avg_f1 = macro_f1_score(f1)\n",
    "    \"\"\"\n",
    "    count = len(f1s)\n",
    "    f1_mac_avg = np.sum(f1s) / count\n",
    "    return f1_mac_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculating the accuracy\n",
    "def accuracy(cm):\n",
    "    \"\"\"Returns the accuracy of the prediction.\n",
    "    \n",
    "        Arguments:\n",
    "            cm (numpy.ndarray): 2D confusion matrix.\n",
    "\n",
    "        Returns:\n",
    "            numpy.float64: - **acc**: Accuracy of the prediction.\n",
    "            \n",
    "        Example:\n",
    "            if cm = [[1 0 0 0 0 0]\n",
    "                     [0 0 1 0 0 0]\n",
    "                     [0 0 4 0 0 0]\n",
    "                     [0 1 0 1 0 0]\n",
    "                     [0 0 0 0 1 0]\n",
    "                     [0 0 0 0 0 1]]\n",
    "            >>> acc = accuracy(cm)\n",
    "            \n",
    "            Outputs:\n",
    "                0.8\n",
    "    \"\"\"\n",
    "    diagonal_sum = cm.trace() # getting the total truely classified images\n",
    "    sum_of_all_elements = cm.sum() # getting the total number of images\n",
    "    acc = diagonal_sum / sum_of_all_elements \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating model metrices\n",
    "def model_metrics(cm):\n",
    "    \"\"\"Returns the metrices and macro metrices for the evaluation of the model.\n",
    "        Metrices includes:\n",
    "            Precision, Recall and F1-score\n",
    "        Macro Metrices includes:\n",
    "            Macro Precision Average, Macro Recall Average and Macro F1-score Average\n",
    "    \n",
    "        Arguments:\n",
    "            cm (numpy.ndarray): 2D confusion matrix.\n",
    "\n",
    "        Returns: tuple: Following Values\n",
    "            - **metrices** (dict): Model Metrices including list of Precision, Recall and F1-score.\n",
    "            - **macro_metrices** (dict): Model Macro metrices including Macro Precision Average, Macro Recall Average and Macro F1-score Average.\n",
    "            - **acc** (numpy.float64): Accuracy of the prediction.\n",
    "            \n",
    "        Example:\n",
    "            >>> metrics, macro_metrics, acc = model_metrics(cm)\n",
    "    \"\"\"\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    metrics = {}\n",
    "    macro_metrics = {}\n",
    "    # calculating precision, recall and f1-score for all the classes or labels\n",
    "    for label in range(10):\n",
    "        precs.append(precision(label, cm))\n",
    "        recs.append(recall(label, cm))\n",
    "        f1s.append(f1_score(precs[label], recs[label]))\n",
    "    \n",
    "    #calculating the macro average metrices\n",
    "    avg_precision = macro_precision_average(precs) #calculating the macro metrices\n",
    "    avg_recall = macro_recall_average(recs)\n",
    "    avg_f1 = macro_f1_score(f1s)\n",
    "    acc = accuracy(cm)\n",
    "    \n",
    "    metrics = {\"Precision\":precs,\n",
    "               \"Recall\":recs,\n",
    "               \"F1-Score\":f1s}\n",
    "    macro_metrics = {\"Precision\":avg_precision,\n",
    "                     \"Recall\":avg_recall,\n",
    "                     \"F1-Score\":avg_f1}\n",
    "    \n",
    "    return metrics, macro_metrics, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##displaying the model summary\n",
    "def metric_summary(metrics, macro_metrics, accuracy):\n",
    "    \"\"\"Displays the metric summary after evaluation.\n",
    "    \n",
    "        Arguments:\n",
    "            metrices (dict): Model Metrices including list of Precision, Recall and F1-score.\n",
    "            macro_metrices (dict): Model Macro metrices including Macro Precision Average, Macro Recall Average and Macro F1-score Average.\n",
    "            accuracy (numpy.float64): Accuracy of the prediction.\n",
    "            \n",
    "        Example:\n",
    "            >>> metric_summary(metrics, macro_metrics, acc)\n",
    "    \"\"\"\n",
    "    print(\"+===============+===============+===============+===============+\")\n",
    "    print(\"| Label \\t| Precision \\t| Recall \\t| F1 Score \\t|\")\n",
    "    print(\"+===============+===============+===============+===============+\")\n",
    "    prec = metrics[\"Precision\"]\n",
    "    rec = metrics[\"Recall\"]\n",
    "    f1 = metrics[\"F1-Score\"]\n",
    "    \n",
    "    for label in range(len(prec)):\n",
    "        print(\"| %d \\t\\t|  %.5f \\t|  %.5f \\t|  %.5f \\t|\"%(label, prec[label], rec[label], f1[label]))\n",
    "\n",
    "    print(\"+===============+===============+===============+===============+\") \n",
    "    \n",
    "    avg_precision = macro_metrics[\"Precision\"]\n",
    "    avg_recall = macro_metrics[\"Recall\"]\n",
    "    avg_f1 = macro_metrics[\"F1-Score\"]\n",
    "    acc = accuracy\n",
    "    \n",
    "    print(\"| Macro Avg \\t|  %.5f \\t|  %.5f \\t|  %.5f \\t|\"%( avg_precision, avg_recall, avg_f1))\n",
    "    print(\"+===============+===============+===============+===============+\") \n",
    "    \n",
    "    print(\"\\n Accuracy \\t\\t  %.5f\"%(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Predictions and acc-loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Training Result\n",
    "\n",
    "def visualize_training_results(train_accs, val_accs, train_loss, val_loss):\n",
    "    \"\"\"Visualize the traininig accuracy and loss, validation accuracy and loss over the training time.\n",
    "    \n",
    "        Arguments:\n",
    "            train_accs (list): training accuracies obtained in all the minibatches in all epochs\n",
    "            val_accs (list): validation accuracies obtained in all the minibatches in all epochs\n",
    "            train_loss (list): training losses obtained in all the minibatches in all epochs\n",
    "            val_loss (list): validation losses obtained in all the minibatches in all epochs\n",
    "            \n",
    "        Example:\n",
    "            >>> visualize_training_results(train_accs, val_accs, train_losses, val_losses)    \n",
    "    \"\"\"\n",
    "    \n",
    "    #creating subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols = 1,figsize=(10,15))\n",
    "    fig.subplots_adjust(wspace=.2, hspace = .5)\n",
    "    \n",
    "    #plotting the loss\n",
    "    axes[0].plot(np.squeeze(train_loss), label = 'Training Loss', color = 'blue')\n",
    "    axes[0].plot(np.squeeze(val_loss), label = 'Validation Loss', color = 'red')\n",
    "    axes[0].legend(loc='upper right') #setting up legend location to upper right corner of the plot\n",
    "    axes[0].set_title(\"Training and Validation Loss \" , fontsize = 16, pad = 10)\n",
    "    axes[0].set_xlabel(\"No. of Epochs\", fontsize = 12)\n",
    "    axes[0].set_ylabel(\"Loss\", fontsize = 12)\n",
    "    axes[0].set_ylim(bottom = 0)  \n",
    "    axes[0].grid(color='grey', alpha = 0.5)\n",
    "    \n",
    "    #plotting the accuracy\n",
    "    axes[1].plot(np.squeeze(train_accs), label = 'Training Accuracy', color = 'blue')\n",
    "    axes[1].plot(np.squeeze(val_accs), label = 'Validation Accuracy', color = 'red')\n",
    "    axes[1].legend(loc='lower right') #setting up legend location to lower right corner of the plot\n",
    "    axes[1].set_title(\"Accuracy \" , fontsize = 16, pad = 10)\n",
    "    axes[1].set_xlabel(\"No. of Epochs\", fontsize = 12)\n",
    "    axes[1].set_ylabel(\"Accuracy\", fontsize = 12)\n",
    "    axes[1].set_ylim(top = 1)  \n",
    "    axes[1].grid(color='grey', alpha = 0.5)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Prediction\n",
    "def visualize_prediction(x_orig, y_orig, prediction, dataset_type, dataset = \"mnist\"):\n",
    "    \"\"\"Displays 10 random images along with their true and predicted labels. \n",
    "        Both initial prediction and second guess are displayed.\n",
    "    \n",
    "        Arguments:\n",
    "            x_orig (numpy.ndarray): original input data.\n",
    "            y_orig (numpy.ndarray): original output labels.\n",
    "            prediction (numpy.ndarray): predictions obtained after training the model.\n",
    "            dataset_type (str): Type of dataset. May be training, dev  or test.\n",
    "            dataset (str): Dataset used to train the model. Default to 'mnist'\n",
    "            \n",
    "        Example:\n",
    "            >>> visualize_prediction(x_orig, y_orig.T, prediction, dataset_type = \"training\")\n",
    "    \"\"\"\n",
    "    if(len(dataset_type) != 0):\n",
    "        visual_title = \"Sample %s Data Set \"%dataset_type.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "    \n",
    "    #getting the random index of 8 images to plot\n",
    "    index = np.random.randint(0,1000,8)\n",
    "    \n",
    "    #getting the label description\n",
    "    label_desc = label_description(dataset)\n",
    "    \n",
    "    #plotting the images along with the predictions\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle(visual_title)\n",
    "\n",
    "    first_lbl, first_prob = prediction[\"First Prediction\"]\n",
    "    sec_lbl, sec_prob = prediction[\"Second Prediction\"]\n",
    "\n",
    "    for ax,i in zip(axes.flatten(),index):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest', cmap='Greys')\n",
    "        ax.set(title = \"True Label: %d | %s\"%(y_orig[0,i], label_desc[y_orig[0,i]] ))\n",
    "        ax.set(xlabel= \"Prediction: %d | With Prob: %.4f \\n2nd Guess: %d | With Prob: %.4f\"%(first_lbl[0,i], first_prob[0,i], sec_lbl[0,i], sec_prob[0,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Mislabeled Data\n",
    "def visualize_mislabelled_images(x_orig,y_orig,prediction,dataset_type, dataset = \"mnist\"):\n",
    "    \"\"\"Displays 10 wrongly predicted images along with their true and predicted labels. \n",
    "        Both initial prediction and second guess are displayed.\n",
    "    \n",
    "        Arguments:\n",
    "            x_orig (numpy.ndarray): original input data.\n",
    "            y_orig (numpy.ndarray): original output labels.\n",
    "            prediction (numpy.ndarray): predictions obtained after training the model.\n",
    "            dataset_type (str): Type of dataset. May be training, dev  or test.\n",
    "            dataset (str): Dataset used to train the model. Default to 'mnist'\n",
    "            \n",
    "        Example:\n",
    "            >>> visualize_mislabelled_images(x_orig, y_orig.T, prediction, dataset_type = \"training\")\n",
    "    \"\"\"\n",
    "    \n",
    "    first_lbl, first_prob = prediction[\"First Prediction\"]\n",
    "    sec_lbl, sec_prob = prediction[\"Second Prediction\"]\n",
    "    \n",
    "    true_prediction = np.equal(first_lbl,y_orig)\n",
    "    mislabelled_indices = np.asarray(np.where(true_prediction == False))\n",
    "    print(\"Total Mislabelled Images: \"+str(len(mislabelled_indices[0])))\n",
    "    \n",
    "    if(len(dataset_type) != 0):\n",
    "        visual_title = \"Sample Mislabelled %s Set Images \"%dataset_type.capitalize()\n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "    \n",
    "    #getting the label description\n",
    "    label_desc = label_description(dataset)     \n",
    "    \n",
    "    #plotting the mislabelled images along with the predictions\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "    fig.suptitle(visual_title)\n",
    "\n",
    "    for ax,i in zip(axes.flatten(),mislabelled_indices[1]):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest', cmap = \"Greys\")\n",
    "        ax.set(title = \"True Label: %d | %s\"%(y_orig[0,i], label_desc[y_orig[0,i]] ))\n",
    "        ax.set(xlabel= \"Prediction: %d | With Prob: %.4f \\n2nd Guess: %d | With Prob: %.4f\"%(first_lbl[0,i], first_prob[0,i], sec_lbl[0,i], sec_prob[0,i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving Model\n",
    "def save_model(file_name, model):\n",
    "    \"\"\"Saves the parameters of the trained model into a pickle file.\n",
    "    \n",
    "        Arguments:\n",
    "            file_name (str): name of the file to be saved.\n",
    "            model (dict): trained model to be saved in the file. Consists of Parameters and activations \n",
    "            \n",
    "        Example:\n",
    "            >>> save_model(file_name = path + fname, model)\n",
    "    \"\"\"\n",
    "    with open(file_name ,'wb') as output_file:\n",
    "        pickle.dump(model,output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Model\n",
    "def load_model(file_name):\n",
    "    \"\"\"Load the saved model from a pickle file\n",
    "    \n",
    "        Arguments:\n",
    "            file_name (str): name of the file to be loaded from\n",
    "        \n",
    "        Returns:\n",
    "            model (dict): trained model to be saved in the file. Consists of Parameters and activations \n",
    "            \n",
    "        Example:\n",
    "            >>> model = load_model(file_name = path + fname)\n",
    "    \"\"\"\n",
    "    try: \n",
    "        with open(file_name ,'rb') as input_file:\n",
    "            model = pickle.load(input_file)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except(OSError, IOError) as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(X,Y,hidden_layers):\n",
    "    \"\"\"\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "            \n",
    "        Example:\n",
    "            Here, shape of x = (784,m)\n",
    "                  shape of y = (10,m)\n",
    "            >>> layers_dim = init_layers(x, y, hidden_layers = [32,16])\n",
    "            >>> print(layers_dim)\n",
    "            \n",
    "            Outputs:\n",
    "                [784, 32, 16, 10]\n",
    "    \"\"\"\n",
    "    input_nodes = X.shape[0]\n",
    "    output_nodes = Y.shape[0]\n",
    "    \n",
    "    layers_dim = [input_nodes]\n",
    "    \n",
    "    for i in hidden_layers:\n",
    "        layers_dim.append(i)\n",
    "    \n",
    "    layers_dim.append(output_nodes)\n",
    "    \n",
    "    return layers_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layers_dim, initialization = \"random\"):\n",
    "    \"\"\"\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "            \n",
    "        Example:\n",
    "            Here, layers_dim = [784, 32, 16, 10]\n",
    "            >>> parameters = init_parameters(layers_dim, initialization = \"random\")\n",
    "            >>> print(\"Layer\\tWeight\\t\\tBias\")\n",
    "            >>> print(\"================================\")\n",
    "            >>> for l in range(1,len(layers_dim)):\n",
    "            ...     print(str(l) +\"\\t\" + str(parameters['W'+str(l)].shape) +\"\\t\"+ str(parameters['b'+str(l)].shape))\n",
    "\n",
    "            \n",
    "            Outputs:\n",
    "                Layer    Weight         Bias\n",
    "                ================================\n",
    "                1        (32, 784)      (32, 1)\n",
    "                2        (16, 32)       (16, 1)\n",
    "                3        (10, 16)       (10, 1)\n",
    "    \"\"\"\n",
    "    L = len(layers_dim)\n",
    "    params = {}\n",
    "        \n",
    "    for l in range(1,L):\n",
    "        #initializing Weights\n",
    "        if initialization == \"he\":\n",
    "            # he-initialization\n",
    "            params['W' + str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) * np.sqrt(np.divide(2,layers_dim[l-1])) \n",
    "        elif initialization == \"random\":\n",
    "            # random initialization scaled by 0.01\n",
    "            params['W' + str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) * 0.01 \n",
    "        else:\n",
    "             raise ValueError(\"Initialization must be 'random' or 'he'\")\n",
    "        \n",
    "        #initializing biases\n",
    "        params['b' + str(l)] = np.zeros((layers_dim[l],1))\n",
    "     \n",
    "        assert(params['W' + str(l)].shape == (layers_dim[l],layers_dim[l-1])), \"Dimention of W mismatched in init_params function\"\n",
    "        assert(params['b' + str(l)].shape == (layers_dim[l],1)), \"Dimention of b mismatched in init_params function\"\n",
    "   \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hyperParams(alpha, num_epoch, minibatch_size, lambd = 0, keep_probs = []):\n",
    "    \"\"\"\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "            \n",
    "        Example:\n",
    "            >>> hyperParams = init_hyperParams(alpha = 0.0001, num_epoch = 10, minibatch_size = 1024,lambd = 0.7,keep_probs = [0.8,0.8])\n",
    "    \"\"\"\n",
    "    hyperParams = {'learning_rate':alpha,\n",
    "                   'num_epoch':num_epoch,\n",
    "                   'mini_batch_size':minibatch_size,\n",
    "                   'lambda':lambd,\n",
    "                   'keep_probs':keep_probs,\n",
    "                   'beta1':0.9,\n",
    "                   'beta2':0.999,\n",
    "                   'epsilon':1e-8\n",
    "                  }\n",
    "    \n",
    "    return hyperParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Sum for individual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sum(A_prev,W,b):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> A = np.random.randn(3,2)\n",
    "            >>> W = np.random.randn(1,3)\n",
    "            >>> b = np.random.randn(1,1)\n",
    "            >>> Z, c = forward_sum(A,W,b)\n",
    "            >>> print(\"Z = \"+ str(Z))\n",
    "            \n",
    "            Output:\n",
    "                Z = [[ 3.26295337 -1.23429987]]\n",
    "        \n",
    "    \"\"\"\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    \n",
    "    cache = (A_prev,W,b)\n",
    "    \n",
    "    assert (Z.shape == (W.shape[0], m)), \"Dimention of Z mismatched in forward_prop function\"\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Activation for individual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A_prev,W,b,activation):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> A_prev = np.random.randn(3,2)\n",
    "            >>> W = np.random.randn(1,3)\n",
    "            >>> b = np.random.randn(1,1)\n",
    "\n",
    "            >>> A,c = forward_activation(A_prev,W,b,activation = 'relu')\n",
    "            >>> print(\"A with Relu = \" + str(A))\n",
    "\n",
    "            >>> A,c = forward_activation(A_prev,W,b,activation = 'softmax')\n",
    "            >>> print(\"A with Softmax = \" + str(A))\n",
    "            \n",
    "            Output:\n",
    "                A with Relu = [[3.26295337 0.        ]]\n",
    "                A with Softmax = [[1. 1.]]\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, sum_cache = forward_sum(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == 'softmax':\n",
    "        Z, sum_cache = forward_sum(A_prev,W,b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "#         Z, sum_cache = forward_sum(A_prev,W,b)\n",
    "#         A, activation_cache = tanh(Z)\n",
    "        pass\n",
    "    \n",
    "    cache = (sum_cache,activation_cache)\n",
    "    \n",
    "    assert(A.shape == Z.shape), \"Dimention of A mismatched in forward_activation function\"\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout for individual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_dropout(A,keep_probs):\n",
    "     #implementing dropout\n",
    "    D = np.random.rand(A.shape[0],A.shape[1])\n",
    "    D = (D < keep_probs).astype(int)\n",
    "    A = np.multiply(A,D)\n",
    "    A = np.divide(A,keep_probs)\n",
    "    \n",
    "    dropout_mask = D\n",
    "    \n",
    "    assert (dropout_mask.shape == A.shape), \"Dimention of dropout_mask mismatched in forward_dropout function\"\n",
    "    return A,dropout_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Prop for L Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters, keep_probs = [], regularizer = None):\n",
    "    \"\"\"\n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> X = np.random.randn(3,2)\n",
    "            >>> W1 = np.random.randn(3,3)\n",
    "            >>> b1 = np.random.randn(3,1)\n",
    "            >>> W2 = np.random.randn(2,3)\n",
    "            >>> b2 = np.random.randn(2,1)\n",
    "            >>> parameters = {\"W1\": W1,\n",
    "                              \"b1\": b1,\n",
    "                              \"W2\": W2,\n",
    "                              \"b2\": b2}\n",
    "            >>> AL, caches, _ = forward_prop(X, parameters)\n",
    "            >>> print(\"AL without dropout = \" + str(AL))\n",
    "\n",
    "            >>> AL, caches, _ = forward_prop(X, parameters,keep_probs = [0.9], regularizer = \"dropout\")\n",
    "            >>> print(\"\\nAL with dropout = \" + str(AL))\n",
    "\n",
    "            >>> print(\"\\nLength of caches list = \" + str(len(caches)))\n",
    "            \n",
    "            Output:\n",
    "                AL without dropout = [[0.25442549 0.64096177]\n",
    "                 [0.74557451 0.35903823]]\n",
    "\n",
    "                AL with dropout = [[0.20251119 0.61487938]\n",
    "                 [0.79748881 0.38512062]]\n",
    "\n",
    "                Length of caches list = 2\n",
    "    \n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    num_class = parameters[\"W\"+str(L)].shape[0]\n",
    "    \n",
    "    dropout_masks = []\n",
    "\n",
    "    # len(keep_probs) == L-1: no dropouts in the Output layer, no dropout at all for prediction\n",
    "    if regularizer == \"dropout\":\n",
    "        assert(len(keep_probs) == L-1 ) \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = forward_activation(A_prev,parameters['W' + str(l)],parameters['b' + str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "        if regularizer == \"dropout\":\n",
    "            A , dropout_mask = forward_dropout(A,keep_probs[l-1])\n",
    "            dropout_masks.append(dropout_mask)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    AL, cache = forward_activation(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (num_class,X.shape[1])), \"Dimention of AL mismatched in forward_prop function\"\n",
    "    \n",
    "    return AL,caches,dropout_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_cost(AL, Y, caches, lambd = 0, regularizer = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        Example:\n",
    "            >>> AL = np.array([[4.21200131e-01, 1.55876995e-04],\n",
    "                           [6.91917292e-02, 1.18118501e-05],\n",
    "                           [5.09608140e-01, 9.99832311e-01]])\n",
    "            >>> cost = softmax_cross_entropy_cost(AL, Y, caches)\n",
    "            >>> print(\"Cost without l2 = \" + str(cost))\n",
    "\n",
    "            >>> cost = softmax_cross_entropy_cost(AL, Y, caches, lambd = 0.7, regularizer = 'l2')\n",
    "            >>> print(\"Cost with l2 = \" + str(cost))\n",
    "            \n",
    "            Output:\n",
    "                Cost without l2 = 0.6742809046007259\n",
    "                Cost with l2 = 8.875542970361\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(caches)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(1./m) * np.sum(np.sum(np.multiply(Y,np.log(AL + 1e-8)), axis = 0,keepdims=True))# add very small number 1e-8 to avoid log(0)\n",
    "\n",
    "    if regularizer == \"l2\":\n",
    "        norm = 0\n",
    "        for l in range(L):\n",
    "            current_cache = caches[l]\n",
    "            sum_cache, _ = current_cache\n",
    "            _,W,_ = sum_cache\n",
    "            norm += np.sum(np.square(W))\n",
    "\n",
    "        L2_cost = (lambd/(2*m)) * norm \n",
    "        cost = cost + L2_cost\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    cost = np.squeeze(cost)      # Making sure your cost's shape is not returned as ndarray\n",
    "    \n",
    "    assert(cost.shape == ()),\"Dimention of cost mismatched in softmax_cross_entropy_cost function\"\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Gradients for individual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_grad(dZ, cache, lambd, regularizer):\n",
    "    \"\"\"\n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> dZ = np.random.randn(3,4)\n",
    "            >>> A = np.random.randn(5,4)\n",
    "            >>> W = np.random.randn(3,5)\n",
    "            >>> b = np.random.randn(3,1)\n",
    "            >>> cache = (A, W, b)\n",
    "\n",
    "            >>> dA_prev, dW, db = backward_grad(dZ, cache, lambd=0, regularizer=None)\n",
    "            >>> print(\"Without L2 Regularization\")\n",
    "            >>> print (\"dA_prev = \"+ str(dA_prev))\n",
    "            >>> print (\"dW = \" + str(dW))\n",
    "            >>> print (\"db = \" + str(db))\n",
    "\n",
    "            >>> l2_dA_prev, l2_dW, l2_db = backward_grad(dZ, cache, lambd = 0.9, regularizer = 'l2')\n",
    "            >>> print(\"\\nWith L2 Regularization\")\n",
    "            >>> print (\"dA_prev = \"+ str(l2_dA_prev))\n",
    "            >>> print (\"dW = \" + str(l2_dW))\n",
    "            >>> print (\"db = \" + str(l2_db))\n",
    "            \n",
    "            Output:\n",
    "                Without L2 Regularization\n",
    "                dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    "                           [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    "                           [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    "                           [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    "                           [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "                dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
    "                      [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
    "                      [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
    "                db = [[-0.14713786]\n",
    "                      [-0.11313155]\n",
    "                      [-0.13209101]]\n",
    "\n",
    "                With L2 Regularization\n",
    "                dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    "                           [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    "                           [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    "                           [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    "                           [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "                dW = [[-0.0814752  -0.28784277 -1.02688866  0.73478408 -0.24353767]\n",
    "                      [ 0.90783172  0.74875962 -0.43216662  0.6696189  -0.78903459]\n",
    "                      [ 0.81102242  0.13703735 -0.07696496  0.4081879  -0.05995309]]\n",
    "                db = [[-0.14713786]\n",
    "                      [-0.11313155]\n",
    "                      [-0.13209101]]\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if regularizer == \"l2\":\n",
    "        dW = (1/m) * np.dot(dZ,A_prev.T) + np.multiply(np.divide(lambd,m),W )\n",
    "    else:\n",
    "        dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims=True )\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    \n",
    "    assert (dW.shape == W.shape), \"Dimention of dW mismatched in backward_grad function\"\n",
    "    assert (db.shape == b.shape), \"Dimention of db mismatched in backward_grad function\"\n",
    "    assert (dA_prev.shape == A_prev.shape), \"Dimention of dA_prev mismatched in backward_grad function\"\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Backward Activation for individual layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA, cache, lambd ,regularizer, activation):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Example:\n",
    "        >>> np.random.seed(2)\n",
    "        >>> dA = np.random.randn(1,2)\n",
    "        >>> A = np.random.randn(3,2)\n",
    "        >>> W = np.random.randn(1,3)\n",
    "        >>> b = np.random.randn(1,1)\n",
    "        >>> Z = np.random.randn(1,2)\n",
    "        >>> sum_cache = (A, W, b)\n",
    "        >>> activation_cache = Z\n",
    "        >>> cache = (sum_cache, activation_cache)\n",
    "\n",
    "        >>> dA_prev, dW, db = backward_activation(dA, cache, lambd = 0 ,regularizer = None, activation = \"relu\")\n",
    "        >>> print(\"With Relu\")\n",
    "        >>> print (\"dA_prev = \"+ str(dA_prev))\n",
    "        >>> print (\"dW = \" + str(dW))\n",
    "        >>> print (\"db = \" + str(db))\n",
    "\n",
    "        >>> dA_prev, dW, db = backward_activation(dA, cache, lambd = 0 ,regularizer = None, activation = \"softmax\")\n",
    "        >>> print(\"\\nWith Softmax\")\n",
    "        >>> print (\"dA_prev = \"+ str(dA_prev))\n",
    "        >>> print (\"dW = \" + str(dW))\n",
    "        >>> print (\"db = \" + str(db))\n",
    "        \n",
    "        Output: \n",
    "            With Relu\n",
    "            dA_prev = [[ 0.44090989 -0.        ]\n",
    "                       [ 0.37883606 -0.        ]\n",
    "                       [-0.2298228   0.        ]]\n",
    "            dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
    "            db = [[-0.20837892]]\n",
    "\n",
    "            With Softmax\n",
    "            dA_prev = [[ 0.44090989  0.05952761]\n",
    "                       [ 0.37883606  0.05114697]\n",
    "                       [-0.2298228  -0.03102857]]\n",
    "            dW = [[ 0.39899183  0.3973954  -0.06975568]]\n",
    "            db = [[-0.23651234]]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    sum_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_grad(dA,activation_cache)\n",
    "        dA_prev, dW, db = backward_grad(dZ, sum_cache, lambd, regularizer = regularizer)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = dA\n",
    "        dA_prev, dW, db = backward_grad(dZ, sum_cache, lambd, regularizer = regularizer)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        pass\n",
    "#         dZ = tanh_grad(dA,activation_cache)\n",
    "#         dA_prev, dW, db = backward_grad(dZ, sum_cache, lambd, regularizer = regularizer)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Backward Propagation for L layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_dropout(dA_prev_temp, D, keep_prob):\n",
    "    dA_prev = np.multiply(dA_prev_temp,D)\n",
    "    dA_prev = np.divide(dA_prev,keep_prob)\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y, caches, dropout_masks = [], keep_probs = [], lambd = 0, regularizer = None):\n",
    "    \"\"\"\n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(3)\n",
    "            >>> AL = np.random.randn(1, 2)\n",
    "            >>> Y = np.array([[1, 0]])\n",
    "\n",
    "            >>> A1 = np.random.randn(4,2)\n",
    "            >>> W1 = np.random.randn(3,4)\n",
    "            >>> b1 = np.random.randn(3,1)\n",
    "            >>> Z1 = np.random.randn(3,2)\n",
    "            >>> cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "            >>> A2 = np.random.randn(3,2)\n",
    "            >>> W2 = np.random.randn(1,3)\n",
    "            >>> b2 = np.random.randn(1,1)\n",
    "            >>> Z2 = np.random.randn(1,2)\n",
    "            >>> cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "            >>> caches = (cache_activation_1, cache_activation_2)\n",
    "\n",
    "            >>> grads = backward_prop(AL, Y, caches)\n",
    "            >>> for key,value in grads.items():\n",
    "            ...     print(str(key)+\" : \"+str(value))\n",
    "            \n",
    "            Output:\n",
    "                dA1 : [[-0.80745758 -0.44693186]\n",
    "                       [ 0.88640102  0.49062745]\n",
    "                       [-0.10403132 -0.05758186]]\n",
    "                dW2 : [[ 0.50767257 -0.42243102 -1.15550109]]\n",
    "                db2 : [[0.61256916]]\n",
    "                dA0 : [[ 0.          0.53064147]\n",
    "                       [ 0.         -0.3319644 ]\n",
    "                       [ 0.         -0.32565192]\n",
    "                       [ 0.         -0.75222096]]\n",
    "                dW1 : [[0.41642713 0.07927654 0.14011329 0.10664197]\n",
    "                       [0.         0.         0.         0.        ]\n",
    "                       [0.05365169 0.01021384 0.01805193 0.01373955]]\n",
    "                db1 : [[-0.22346593]\n",
    "                       [ 0.        ]\n",
    "                       [-0.02879093]]\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dA = np.subtract(AL,Y)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_activation(dA, current_cache,lambd = lambd, regularizer = regularizer, activation = 'softmax')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        if regularizer == \"dropout\":\n",
    "            #implementing dropout\n",
    "            D = dropout_masks[l]\n",
    "            dA_prev_temp = backward_dropout(grads[\"dA\" + str(l + 1)], D, keep_probs[l])\n",
    "            dA_prev, dW_temp, db_temp = backward_activation(dA_prev_temp, current_cache, lambd = lambd, regularizer = regularizer, activation = 'relu')\n",
    "        else:\n",
    "            dA_prev, dW_temp, db_temp = backward_activation(grads[\"dA\" + str(l + 1)], current_cache, lambd = lambd, regularizer = regularizer, activation = 'relu')\n",
    "            \n",
    "        \n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize adam\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "   \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate, optimizer = \"bgd\", beta1 = 0, beta2 = 0,  epsilon = 0, v = {}, s = {}, t = 0):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(2)\n",
    "            >>> W1 = np.random.randn(3,4)\n",
    "            >>> b1 = np.random.randn(3,1)\n",
    "            >>> W2 = np.random.randn(1,3)\n",
    "            >>> b2 = np.random.randn(1,1)\n",
    "            >>> parameters = {\"W1\": W1,\n",
    "                          \"b1\": b1,\n",
    "                          \"W2\": W2,\n",
    "                          \"b2\": b2}\n",
    "            >>> np.random.seed(3)\n",
    "            >>> dW1 = np.random.randn(3,4)\n",
    "            >>> db1 = np.random.randn(3,1)\n",
    "            >>> dW2 = np.random.randn(1,3)\n",
    "            >>> db2 = np.random.randn(1,1)\n",
    "            >>> grads = {\"dW1\": dW1,\n",
    "                     \"db1\": db1,\n",
    "                     \"dW2\": dW2,\n",
    "                     \"db2\": db2}\n",
    "\n",
    "            >>> parameters,_,_ = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "            >>> print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "            >>> print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "            >>> print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "            >>> print (\"b2 = \"+ str(parameters[\"b2\"]))\n",
    "            \n",
    "            Output:\n",
    "                W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    "                      [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    "                      [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
    "                b1 = [[-0.04659241]\n",
    "                      [-1.28888275]\n",
    "                      [ 0.53405496]]\n",
    "                W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
    "                b2 = [[-0.84610769]]\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2           \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                       \n",
    "    \n",
    "    for l in range(L):\n",
    "        if optimizer == 'adam':\n",
    "            # Moving average of the gradients.\n",
    "            v[\"dW\" + str(l+1)] = np.add(beta1 * v[\"dW\" + str(l+1)], (1 - beta1) * grads[\"dW\" + str(l+1)])\n",
    "            v[\"db\" + str(l+1)] = np.add(beta1 * v[\"db\" + str(l+1)], (1 - beta1) * grads[\"db\" + str(l+1)])\n",
    "\n",
    "            # Compute bias-corrected first moment estimate.\n",
    "            v_corrected[\"dW\" + str(l+1)] = np.divide(v[\"dW\" + str(l+1)], (1 - np.power(beta1,t)))\n",
    "            v_corrected[\"db\" + str(l+1)] = np.divide(v[\"db\" + str(l+1)], (1 - np.power(beta1,t)))\n",
    "\n",
    "            # Moving average of the squared gradients. \n",
    "            s[\"dW\" + str(l+1)] = np.add(beta2 * s[\"dW\" + str(l+1)], (1 - beta2) * np.square(grads[\"dW\" + str(l+1)]))\n",
    "            s[\"db\" + str(l+1)] = np.add(beta2 * s[\"db\" + str(l+1)], (1 - beta2) * np.square(grads[\"db\" + str(l+1)]))\n",
    "\n",
    "            # Compute bias-corrected second raw moment estimate. \n",
    "            s_corrected[\"dW\" + str(l+1)] = np.divide(s[\"dW\" + str(l+1)], (1 - np.power(beta2,t)))\n",
    "            s_corrected[\"db\" + str(l+1)] = np.divide(s[\"db\" + str(l+1)], (1 - np.power(beta2,t)))\n",
    "\n",
    "            # Update parameters. \n",
    "            parameters[\"W\" + str(l+1)] = np.subtract(parameters[\"W\" + str(l+1)],  learning_rate * np.divide(v_corrected[\"dW\" + str(l+1)], np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon))\n",
    "            parameters[\"b\" + str(l+1)] = np.subtract(parameters[\"b\" + str(l+1)],  learning_rate * np.divide(v_corrected[\"db\" + str(l+1)], np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon))\n",
    "        else:\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "            \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model using acc and loss\n",
    "def evaluate(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    \n",
    "        Example:\n",
    "            >>> np.random.seed(1)\n",
    "            >>> X = np.random.randn(3,2)\n",
    "            >>> Y = np.array([[1, 0, 0],[0,1,1]]).reshape(3,2)\n",
    "            >>> W1 = np.random.randn(5,3)\n",
    "            >>> b1 = np.random.randn(5,1)\n",
    "            >>> W2 = np.random.randn(3,5)\n",
    "            >>> b2 = np.random.randn(3,1)\n",
    "            >>> parameters = {\"W1\": W1,\n",
    "            ...               \"b1\": b1,\n",
    "            ...               \"W2\": W2,\n",
    "            ...               \"b2\": b2}\n",
    "            >>> acc, loss = evaluate(X, Y, parameters)\n",
    "            >>> print(\"acc = %f | cost = %f\"%(acc,loss))\n",
    "            \n",
    "            Output:\n",
    "                acc = 0.500000 | cost = 0.769464\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # predicting output using fordward propogation \n",
    "    probas, caches, _ = forward_prop(X, parameters)\n",
    "    #computing loss\n",
    "    loss = softmax_cross_entropy_cost(probas, Y, caches) \n",
    "    \n",
    "    #deriving the predictrueted labels\n",
    "    true_labels = np.argmax(Y,axis=0).reshape(1,m)\n",
    "    #deriving the predicted labels\n",
    "    predicted_labels = np.argmax(probas,axis=0).reshape(1,m)\n",
    "    \n",
    "    #identifing correctly predicted labels\n",
    "    correct_prediction = np.equal(predicted_labels,true_labels)\n",
    "    \n",
    "    #computing accuracy\n",
    "    num_correct_prediction = np.sum(correct_prediction)\n",
    "    accuracy = (num_correct_prediction/m)\n",
    "    \n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Training\n",
    "\n",
    "def train(X_train, Y_train, X_dev, Y_dev, layers_dim, hyperParams, initialization = \"random\", optimizer = 'bgd',regularizer = None, verbose = 3):\n",
    "    # loading the hyper parameters\n",
    "    learning_rate = hyperParams['learning_rate']\n",
    "    num_epoch = hyperParams['num_epoch']\n",
    "    b1 = hyperParams['beta1']\n",
    "    b2 = hyperParams['beta2']\n",
    "    ep = hyperParams['epsilon']\n",
    "    lambd = hyperParams['lambda']\n",
    "    keep_probs = hyperParams['keep_probs']\n",
    "\n",
    "    #initializing the variables\n",
    "    seed = 1\n",
    "    m = Y_train.shape[1]\n",
    "#     costs = []      # keep track of epoch cost    \n",
    "    train_accs = []  # keep track of training accuracy\n",
    "    val_accs = []     # keep track of Validation accuracy\n",
    "    train_losses = []  # keep track of training loss\n",
    "    val_losses = []     # keep track of Validation loss\n",
    "    \n",
    "   \n",
    "    #selecting the minibatch size for each optimizer\n",
    "    if optimizer == 'sgd':\n",
    "        mini_batch_size = 1\n",
    "    elif optimizer == 'bgd':\n",
    "        mini_batch_size = m\n",
    "    elif optimizer == 'mgd' or optimizer == 'adam':\n",
    "        mini_batch_size = hyperParams['mini_batch_size']\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer value out of scope\")\n",
    "        \n",
    "    #initializing the model parameters\n",
    "    parameters = init_parameters(layers_dim, initialization)\n",
    "    \n",
    "    #initializing adam parameters, used only when optimizer = 'adam'\n",
    "    t = 0\n",
    "    v,s = initialize_adam(parameters)\n",
    "    \n",
    "    train_toc = time.time() # for calculating entire training time\n",
    "    print(\"Training The Model...\")\n",
    "    \n",
    "    #Gradient Descent begins\n",
    "    for i in range(1, num_epoch+1):\n",
    "        seed += 1\n",
    "        time_trained = 0\n",
    "#         batch_cost = []\n",
    "        batch_times = []\n",
    "        accs = []\n",
    "        losses = []\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"\\nEpoch %d/%d\"%(i,num_epoch))\n",
    "        \n",
    "        minibatches = rand_mini_batches(X_train, Y_train, mini_batch_size, seed)\n",
    "        total_minibatches = len(minibatches)\n",
    "        \n",
    "        for ind, minibatch in enumerate(minibatches):\n",
    "            batch_toc = time.time() # for calculating time of an epoch cycle\n",
    "            \n",
    "            #retriving minibatch of X and Y from training set\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            #forward Propagation\n",
    "            AL, caches, dropout_masks = forward_prop(minibatch_X, parameters, keep_probs = keep_probs, regularizer = regularizer)\n",
    "            \n",
    "            #Computing cross entropy cost\n",
    "            cross_entropy_cost = softmax_cross_entropy_cost(AL, minibatch_Y, caches, lambd = lambd, regularizer = regularizer) #accumulating the batch costs\n",
    "#             batch_cost.append(cross_entropy_cost)   \n",
    "            \n",
    "            #Backward Propagation\n",
    "            grads = backward_prop(AL, minibatch_Y, caches, dropout_masks = dropout_masks, keep_probs = keep_probs, lambd = lambd, regularizer = regularizer)\n",
    "                \n",
    "            #Updating parameters\n",
    "            t += 1\n",
    "            parameters, v, s = update_parameters(parameters, grads, learning_rate, optimizer = optimizer, beta1 = b1, beta2 = b2,  epsilon = ep, v = v, s = s, t = t)\n",
    "            \n",
    "            # Calculating training time for each batch \n",
    "            batch_tic = time.time()\n",
    "            batch_times.append(batch_tic - batch_toc)\n",
    "            time_trained = np.sum(batch_times)\n",
    "            \n",
    "            #calculating training progress\n",
    "            per = ((ind+1) / total_minibatches) * 100\n",
    "            inc = int(per // 10) * 2\n",
    "            \n",
    "            #calculating accuracy and loss of the training batch\n",
    "            acc,loss = evaluate(minibatch_X, minibatch_Y, parameters)\n",
    "            accs.append(acc)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            #averaging all the accs and losses till now\n",
    "            train_acc = np.mean(accs)\n",
    "            train_loss = np.mean(losses)\n",
    "            \n",
    "            #Verbosity 0: Silent mode\n",
    "            #Verbosity 1: Epoch mode\n",
    "            #Verbosity 2: Progress bar mode\n",
    "            #Verbosity 3 or greater: Metric mode\n",
    "                \n",
    "            if verbose == 2:\n",
    "                print (\"%d/%d [%s>%s %.0f%%] - %.2fs\"%(ind+1, total_minibatches, '=' * inc,'.'*(20-inc), per, time_trained),end='\\r')\n",
    "            elif verbose > 2:\n",
    "                print (\"%d/%d [%s>%s %.0f%%] - %.2fs | loss: %.4f | acc: %.4f\"%(ind+1, total_minibatches, '=' * inc,'.'*(20-inc), per, time_trained, train_loss, train_acc),end='\\r')\n",
    "            \n",
    "        #----------------------------------------------batch ends-------------------------------------------\n",
    "        \n",
    "        #accumulating the acc and loss of the last iteration of each epoch\n",
    "        train_accs.append(np.mean(accs))\n",
    "        train_losses.append(np.mean(losses))\n",
    "                \n",
    "        #evaluating the model using validation accuracy and loss\n",
    "        val_acc, val_loss= evaluate(X_dev, Y_dev, parameters)  \n",
    "        val_accs.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        #calculating Epoch Cost\n",
    "#         costs.append(np.mean(batch_cost))\n",
    "\n",
    "        time_per_batch = int(np.mean(batch_times)*1000)\n",
    "\n",
    "        if verbose == 2:\n",
    "            print (\"%d/%d [%s 100%%] - %.2fs %dms/step\"%(total_minibatches, total_minibatches, '=' * 20, time_trained, time_per_batch ),end='\\r')\n",
    "        elif verbose > 2:\n",
    "            print (\"%d/%d [%s 100%%] - %.2fs %dms/step | loss: %.4f | acc: %.4f | val_loss: %.4f | val_acc: %.4f\"%(total_minibatches, total_minibatches, '=' * 20, time_trained, time_per_batch, train_loss, train_acc, val_loss, val_acc),end='\\r')\n",
    "                \n",
    "        \n",
    "    #-------------------------------------------Gradient Descent ends-----------------------------------------------\n",
    "    \n",
    "    train_tic = time.time() # for calculating entire training time\n",
    "    hrs, mins, secs , ms = convert_time((train_tic - train_toc)*1000)\n",
    "    print(\"\\n\\nTotal Training Time = %dhr %dmins %dsecs %.2fms\"%(hrs, mins, secs, ms))\n",
    "\n",
    "     \n",
    "    history = {\"parameters\":parameters,\n",
    "               \"accuracy\": train_accs,\n",
    "               \"loss\":train_losses ,\n",
    "               \"val_accuracy\":val_accs,\n",
    "               \"val_loss\":val_losses\n",
    "            }\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training The Model...\n",
      "\n",
      "Epoch 1/5\n",
      "255/255 [==================== 100%] - 0.65s 2ms/step | loss: 0.5892 | acc: 0.8451 | val_loss: 0.3120 | val_acc: 0.9120\n",
      "Epoch 2/5\n",
      "255/255 [==================== 100%] - 0.81s 3ms/step | loss: 0.2800 | acc: 0.9247 | val_loss: 0.2662 | val_acc: 0.9251\n",
      "Epoch 3/5\n",
      "255/255 [==================== 100%] - 0.62s 2ms/step | loss: 0.2461 | acc: 0.9355 | val_loss: 0.2417 | val_acc: 0.9351\n",
      "Epoch 4/5\n",
      "255/255 [==================== 100%] - 0.70s 2ms/step | loss: 0.2280 | acc: 0.9409 | val_loss: 0.2301 | val_acc: 0.9398\n",
      "Epoch 5/5\n",
      "255/255 [==================== 100%] - 0.64s 2ms/step | loss: 0.2148 | acc: 0.9436 | val_loss: 0.2190 | val_acc: 0.9418\n",
      "\n",
      "Total Training Time = 0hr 0mins 4secs 851.52ms\n"
     ]
    }
   ],
   "source": [
    "layers_dim = init_layers(train_x_norm, train_y_encoded, hidden_layers = [64,32])\n",
    "hyperParams = init_hyperParams(alpha = 0.001, num_epoch = 5, minibatch_size = 100,lambd = 0.7,keep_probs = [0.8,0.8])\n",
    "history = train(train_x_norm, train_y_encoded, dev_x_norm, dev_y_encoded,layers_dim, hyperParams, initialization = \"he\", optimizer = 'adam',regularizer = \"l2\", verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = history[\"parameters\"]\n",
    "train_acc = history[\"accuracy\"]\n",
    "train_loss = history[\"loss\"]\n",
    "val_acc = history[\"val_accuracy\"]\n",
    "val_loss = history[\"val_loss\"]\n",
    "epochs = len(val_acc)\n",
    "\n",
    "\n",
    "visualize_training_results(train_acc, val_acc, train_loss, val_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epochs\t | Train Acc\t | Train Loss\t | Val Acc\t | Val Loss\n",
      "--------------------------------------------------------------------\n",
      " 1\t | 0.845059\t | 0.589182\t | 0.912000\t | 0.311981\n",
      " 2\t | 0.924706\t | 0.280021\t | 0.925111\t | 0.266231\n",
      " 3\t | 0.935451\t | 0.246098\t | 0.935111\t | 0.241690\n",
      " 4\t | 0.940863\t | 0.228040\t | 0.939778\t | 0.230139\n",
      " 5\t | 0.943647\t | 0.214757\t | 0.941778\t | 0.219020\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Epochs\\t | Train Acc\\t | Train Loss\\t | Val Acc\\t | Val Loss\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "for i in range(epochs):\n",
    "    print(\" %d\\t | %f\\t | %f\\t | %f\\t | %f\"%(i+1,train_acc[i] ,train_loss[i],val_acc[i] ,val_loss[i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "def predict(X, parameters, second_guess = False):\n",
    "    prediction = {}\n",
    "    \n",
    "    # Computing the Output predictions. \n",
    "    # no keep_probs : no dropout during prediction \n",
    "    probas, caches, _ = forward_prop(X, parameters)\n",
    "    \n",
    "    #getting the number of examples\n",
    "    m = probas.shape[1]\n",
    "\n",
    "    #deriving the predicted labels with their probabilities\n",
    "    predicted_labels = np.argmax(probas,axis=0).reshape(1,m)\n",
    "    predicted_prob = np.max(probas,axis = 0).reshape(1,m)\n",
    "    \n",
    "    #Computing the second guess\n",
    "    if second_guess == True:\n",
    "        second_max = np.array(probas, copy=True)\n",
    "        second_max[predicted_labels,np.arange(m)] = 0 #zeroing out the first max prediction\n",
    "        sec_predicted_labels = np.argmax(second_max,axis=0).reshape(1,m) #selecting the second max predicted label\n",
    "        sec_predicted_prob = np.max(second_max,axis = 0).reshape(1,m) #selecting the second max prediction\n",
    "\n",
    "        prediction[\"Second Prediction\"] = [sec_predicted_labels, sec_predicted_prob]      \n",
    "\n",
    "    prediction[\"First Prediction\"] = [predicted_labels, predicted_prob]\n",
    "    \n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = predict(train_x_norm, parameters, second_guess = True)\n",
    "prediction_dev = predict(dev_x_norm,parameters, second_guess = True)\n",
    "prediction_test = predict(test_x_norm, parameters, second_guess = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train = confusion_matrix(train_y_split, prediction_train)\n",
    "cm_dev = confusion_matrix(dev_y_split, prediction_dev)\n",
    "cm_test = confusion_matrix(test_y_orig, prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrix   \n",
    "plot_confusion_matrix(cm_train, dataset_type = \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_dev, dataset_type = \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_test, dataset_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x_norm_orig, test_y_encoded_orig = prep_dataset(test_x_orig, test_y_orig, num_class = 10)\n",
    "\n",
    "# print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "# print(\"=================================================================\")\n",
    "# print(\"Test Set Images:\\t\" + str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_norm_orig.shape))\n",
    "# print(\"Test Set Labels:\\t\" + str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_encoded_orig.shape))\n",
    "# print(\"=================================================================\")\n",
    "\n",
    "# prediction_test_orig = predict(test_x_norm_orig, parameters, second_guess = True)\n",
    "\n",
    "# cm_test_orig = confusion_matrix(test_y_orig, prediction_test_orig)\n",
    "# metrics, macro_metrics, acc = model_metrics(cm_test_orig)\n",
    "# metric_summary(metrics, macro_metrics, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+===============+===============+===============+===============+\n",
      "| Label \t| Precision \t| Recall \t| F1 Score \t|\n",
      "+===============+===============+===============+===============+\n",
      "| 0 \t\t|  0.97796 \t|  0.96941 \t|  0.97366 \t|\n",
      "| 1 \t\t|  0.96088 \t|  0.98005 \t|  0.97037 \t|\n",
      "| 2 \t\t|  0.94554 \t|  0.92102 \t|  0.93312 \t|\n",
      "| 3 \t\t|  0.96412 \t|  0.89504 \t|  0.92830 \t|\n",
      "| 4 \t\t|  0.93075 \t|  0.95184 \t|  0.94118 \t|\n",
      "| 5 \t\t|  0.91500 \t|  0.95861 \t|  0.93630 \t|\n",
      "| 6 \t\t|  0.93313 \t|  0.97731 \t|  0.95471 \t|\n",
      "| 7 \t\t|  0.96072 \t|  0.95381 \t|  0.95725 \t|\n",
      "| 8 \t\t|  0.93152 \t|  0.93040 \t|  0.93096 \t|\n",
      "| 9 \t\t|  0.94428 \t|  0.92370 \t|  0.93388 \t|\n",
      "+===============+===============+===============+===============+\n",
      "| Macro Avg \t|  0.94639 \t|  0.94612 \t|  0.94597 \t|\n",
      "+===============+===============+===============+===============+\n",
      "\n",
      " Accuracy \t\t  0.94651\n"
     ]
    }
   ],
   "source": [
    "metrics, macro_metrics, acc = model_metrics(cm_train)\n",
    "metric_summary(metrics, macro_metrics, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+===============+===============+===============+===============+\n",
      "| Label \t| Precision \t| Recall \t| F1 Score \t|\n",
      "+===============+===============+===============+===============+\n",
      "| 0 \t\t|  0.97717 \t|  0.96180 \t|  0.96942 \t|\n",
      "| 1 \t\t|  0.97012 \t|  0.97988 \t|  0.97497 \t|\n",
      "| 2 \t\t|  0.94612 \t|  0.91458 \t|  0.93008 \t|\n",
      "| 3 \t\t|  0.94079 \t|  0.88636 \t|  0.91277 \t|\n",
      "| 4 \t\t|  0.93202 \t|  0.96372 \t|  0.94760 \t|\n",
      "| 5 \t\t|  0.90617 \t|  0.93862 \t|  0.92211 \t|\n",
      "| 6 \t\t|  0.91685 \t|  0.98821 \t|  0.95119 \t|\n",
      "| 7 \t\t|  0.95364 \t|  0.94530 \t|  0.94945 \t|\n",
      "| 8 \t\t|  0.93503 \t|  0.94379 \t|  0.93939 \t|\n",
      "| 9 \t\t|  0.93379 \t|  0.90088 \t|  0.91704 \t|\n",
      "+===============+===============+===============+===============+\n",
      "| Macro Avg \t|  0.94117 \t|  0.94231 \t|  0.94140 \t|\n",
      "+===============+===============+===============+===============+\n",
      "\n",
      " Accuracy \t\t  0.94178\n"
     ]
    }
   ],
   "source": [
    "metrics, macro_metrics, acc = model_metrics(cm_dev)\n",
    "metric_summary(metrics, macro_metrics, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+===============+===============+===============+===============+\n",
      "| Label \t| Precision \t| Recall \t| F1 Score \t|\n",
      "+===============+===============+===============+===============+\n",
      "| 0 \t\t|  0.95248 \t|  0.97463 \t|  0.96343 \t|\n",
      "| 1 \t\t|  0.97112 \t|  0.98175 \t|  0.97641 \t|\n",
      "| 2 \t\t|  0.95088 \t|  0.93256 \t|  0.94163 \t|\n",
      "| 3 \t\t|  0.94926 \t|  0.89800 \t|  0.92292 \t|\n",
      "| 4 \t\t|  0.92915 \t|  0.95228 \t|  0.94057 \t|\n",
      "| 5 \t\t|  0.90377 \t|  0.94530 \t|  0.92406 \t|\n",
      "| 6 \t\t|  0.92085 \t|  0.96559 \t|  0.94269 \t|\n",
      "| 7 \t\t|  0.95652 \t|  0.92543 \t|  0.94072 \t|\n",
      "| 8 \t\t|  0.94387 \t|  0.91348 \t|  0.92843 \t|\n",
      "| 9 \t\t|  0.94036 \t|  0.93294 \t|  0.93663 \t|\n",
      "+===============+===============+===============+===============+\n",
      "| Macro Avg \t|  0.94183 \t|  0.94220 \t|  0.94175 \t|\n",
      "+===============+===============+===============+===============+\n",
      "\n",
      " Accuracy \t\t  0.94220\n"
     ]
    }
   ],
   "source": [
    "metrics, macro_metrics, acc = model_metrics(cm_test)\n",
    "metric_summary(metrics, macro_metrics, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizating  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(train_x_orig, train_y_orig.T, prediction_train, dataset_type = \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(dev_x_split, dev_y_split.T, prediction_dev, dataset_type = \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(test_x_split, test_y_split.T, prediction_test,dataset_type = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Mislabelled Images in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mislabelled_images(train_x_orig, train_y_orig.T,prediction_train,dataset_type = \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mislabelled_images(dev_x_split, dev_y_split.T, prediction_dev,dataset_type = \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mislabelled_images(test_x_split, test_y_split.T, prediction_test,dataset_type = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Real Time images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f01d32563a0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEcCAYAAACvV4OAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7xcVX338c833CFQ4CFAiqSJiMhFjRAxAdGgFRDBINUKKhKloBW0eEeetgaslccH8calRrlXgvoSBCwFAbkUQSWh3MM1BAmJJCkKCVFCkl//WGtgZ87Mnjln5pwz++T7fr3mdc6stS9rZvb8Zu2111pbEYGZWVWNGu4CmJl1wkHMzCrNQczMKs1BzMwqzUHMzCrNQczMKm39TlaWdBDwbWA94AcRcVrZ8ttss02MHz++k12aWYn58+ezdOlSdbINSf3pd3VtRBzUyf46NeAgJmk94CzgHcAC4A5JV0bEA83WGT9+PLNnzx7oLs2shUmTJg31LrcZ6h3W66QmtjfwaETMA5B0KTANaBrEzKwaRo1qr6VpzZo1g1yS1jppE9sBeLLwfEFOM7OKk9TWoxd0UhNr9Ar6nEtLOg44DmDcuHEd7M7MhkqvBKh2dFITWwDsWHj+CmBh/UIRMTMiJkXEpDFjxnSwOzMbCu3Wwnol0HUSxO4AdpY0QdKGwBHAld0plpkNpyoFsQGfTkbEKkknANeSulicFxH3d61kZjZs2m3Y7wUd9ROLiKuBq7tUFjPrEb1Sy2pHR0HMzEaeXjpVbIeDmJn14SBmZpXmIGZmleYgZmaVJWnduTppZiOTa2JmVmkOYmZWaQ5iZlZpDmJmVllu2DezynNNzMwqzUHMzCrNQczMKssDwM2s8qoUxKpzCcLMhsyoUaPaerQiaUdJN0qaK+l+Sf+Q07eWdJ2kR/LfrXK6JH1H0qOS7pG0Z8uydvxqzWzE6eL01KuAz0bErsBk4HhJuwEnATdExM7ADfk5wDuBnfPjOOCcVjtwEDOztXTzRiERsSgi7sz/LwPmkm7tOA24MC92IXBY/n8acFEkvwa2lDS2bB8OYmbWx2DcKETSeOANwG+A7SJiEaRAB2ybF+v3/WzdsG9mffQjQG0jaXbh+cyImNlge6OBnwInRsRzJdtv6362RQ5iZtZHP4LY0oiY1GJbG5AC2A8j4rKc/LSksRGxKJ8uLs7pbd3Ptsink2a2ltrYyS5dnRRwLjA3Is4oZF0JHJ3/Pxq4opD+4XyVcjLwbO20sxnXxKwjDz/8cGn+DTfc0DRv/vz5pesuXFj6A8yECRNK8w855JCmeXvvvXfpuuu6LvYT2xc4CrhX0l057WTgNODHko4Bfge8L+ddDRwMPAqsAD7SagcOYmbWR7eCWETcSuN2LoC3N1g+gOP7sw8HMTPro0o99h3EzKwPBzEzqyxPimhmleeamJlVmoOYmVXaOhPEJM0HlgGrgVWteu5aY+mqcnOtDqjly5c3zVu8eHHTPIDNN9+8NH+DDTYozT/11FNL86+99tqmeStXrixdd82aNaX5rVx88cVN8y699NLSdd/0pjd1tO8qWxcnRdw/IpZ2YTtm1iPWtSBmZiNMla5OdlrSAH4haY6k47pRIDMbfoMxFc9g6bQmtm9ELJS0LXCdpAcj4pbiAjm4HQcwbty4DndnZoOtlwJUOzqqiUXEwvx3MXA50GdUbUTMjIhJETFpzJgxnezOzIZIlWpiAw5ikjaTtHntf+AA4L5uFczMhk+Vglgnp5PbAZfnF7I+cElEXNOVUpnZsOqVANWOAQexiJgHvL6LZVlntTpgnnzyydL8T37yk03zbrvtttJ1t95669L8j3/846X5u+22W2n+Flts0TRvyy23LF333nvvLc1v9dqWLm3e8+dLX/pS6bpXXXVVaf5mm21Wml9lHjtpZpW3TtTEzGzkchAzs0pzEDOzSnMQM7PKcsO+mVWea2LWLytWrCjN/9SnPlWaf/XVVzfNa3UwLlmypDT/Bz/4QWn+nXfeWZq/4YYbluaXefHFF0vzDz/88NL866+/vmne/fffX7ruQw89VJq/5557luZXnYOYmVWag5iZVVYvDSlqh4OYmfXhIGZmleark2ZWaa6JmVlluU3MzCrPQcz6Zfbs2aX511xTPk3bJpts0jRv0003LV134403Ls0vm0oHWvflKusn1upWda1uFzdpUvkdAsv6iS1btqx03ccff7w03/3EeoeDmJn14SBmZpXlsZNmVnmuiZlZpTmImVmlOYiZWaU5iJlZZblh3/qtVZ+kNWvWlOavXLmyad6RRx5Zuu7pp59emt+qP1Unty7r9Ne+1S3byra/3nrrla67+eabD6hMI0WVamLVCbdmNmS6dQdwSedJWizpvkLaDElPSborPw4u5H1J0qOSHpJ0YDtldRAzs7W0G8DarK1dABzUIP2bETExP67O+90NOALYPa9ztqTyKjMOYmbWQLeCWETcAjzT5m6nAZdGxAsR8TjwKLB3q5UcxMysjy7WxJo5QdI9+XRzq5y2A/BkYZkFOa2Ug5iZ9TFq1Ki2HsA2kmYXHse1sflzgJ2AicAi4Bs5vVFULJ8lAF+dNLM6/axlLY2I8ulE6kTE04V9fR/4eX66ANixsOgrgIWttueamJn1MZink5LGFp6+B6hdubwSOELSRpImADsDv221vZY1MUnnAYcAiyNij5y2NfAjYDwwH/jbiPhD+y/DisaOHdt6oRJl/ch+9atfla77/PPPl+Z3WrYyreYTmzVrVmn+b39bfnyvv37zw/t1r3td6bpTpkwpzR/putVPTNIsYCrptHMB8GVgqqSJpFPF+cDHACLifkk/Bh4AVgHHR8TqVvto53TyAuBM4KJC2knADRFxmqST8vMvtveyzKzXdSuIRUSj3tbnliz/VeCr/dlHy9PJJpdIpwEX5v8vBA7rz07NrHfVhh212bA/7AbasL9dRCwCiIhFkrbtYpnMbJhVadjRoF+dzJdcjwMYN27cYO/OzLqgSkFsoPXBp2tXGPLfxc0WjIiZETEpIiaNGTNmgLszs6E0BJ1du2agQexK4Oj8/9HAFd0pjpn1gioFsXa6WDS6RHoa8GNJxwC/A943mIU0s6HTSwGqHS2DWJNLpABv73JZ1llvfetbS/MnT55cmn/77bc3zXvkkUdK1z3qqKNK888///zS/B12aDm0ralzzjmnNP/kk08uzW/Vz6zsi/iZz3ymdN11fT6xXrny2A4POzKzPkZUTczM1j0OYmZWWSOuTczM1j0OYmZWaQ5iZlZpvjpp/bLRRhuV5p955pml+e9617ua5j3zTPn05rfccktp/rvf/e7S/LPPPrs0/957722aN2PGjNJ1W3nxxRdL87/yla80zTv88MNL1+2k+0bVuU3MzCrPQczMKs1BzMwqzUHMzCqrNiliVTiImVkfromZWaU5iJlZpTmIWb+06pP02te+tjT/u9/9btO8Y489tnTdF154oTT/gQceKM0/9NBDS/PLXtvKlStL191kk01K80899dTS/BNOOGFA5TIHMTOrMHd2NbPK89VJM6s018TMrNIcxMysstwmZmaV5yBmZpXmIGb90ukBM23atKZ5W2yxRem6n/jEJ0rz582bV5q/fPny0vz11luvaV6r1/21r32tNP8jH/lIaX5ZX7AqfUmHg69OmllluU3MzCrPQczMKs1BzMwqzUHMzCrLkyKaWeW5JmZmlTaigpik84BDgMURsUdOmwEcCyzJi50cEVcPViGtXFl/qP3226903X322ac0v1U/sVb3fiwr2/rrlx9+559/fmn+QQcdVJq//fbbl+Zbc1UKYu2c+F4ANDpavhkRE/PDAcxsBKn1FWv16AUtg1hE3AKU30bazEaMdgNYO0FM0nmSFku6r5C2taTrJD2S/26V0yXpO5IelXSPpD3bKW8nlyBOyDs6r1YIMxsZRo0a1dajDRfQ90zuJOCGiNgZuCE/B3gnsHN+HAec01ZZ21mogXOAnYCJwCLgG80WlHScpNmSZi9ZsqTZYmbWQ7pVE2tyJjcNuDD/fyFwWCH9okh+DWwpaWyrfQwoiEXE0xGxOiLWAN8H9i5ZdmZETIqISWPGjBnI7sxsiA1ym9h2EbEIIP/dNqfvADxZWG5BTis1oC4WksbWCgG8B7ivbHkzq45+BqhtJM0uPJ8ZETMHuusGaS1vS9VOF4tZwFRSYRcAXwamSpqYdzAf+Fh/Smpmva0fQWxpREzq5+afrlWE8uni4py+ANixsNwrgIWtNtYyiEXEkQ2Sz22npDY01qxZ0zTv85//fOm6l112WWn+VluVX7Np1c/sxhtvbJq3atWq0nVvvfXW0vxjjjmmNL/stW288cal667rBrn7xJXA0cBp+e8VhfQTJF0KvAl4tnDG15R77JtZH90aO9nkTO404MeSjgF+B7wvL341cDDwKLACKJ/1MnMQM7O1dLMja5MzOYC3N1g2gOP7uw8HMTPro1d647fDQczM+nAQM7NKcxAzs8rypIjWb2XT1UDrX8VLLrmkad6555b3hmnV1eArX/lKaf6xxx5bmn/ppZc2zTvxxBNL1201zc8vf/nL0vybb765ad6BBx5Yuu66zjUxM6s0BzEzqzQHMTOrNAcxM6usXpq1tR0OYmbWh69OmlmluSZmZpXl00nrulbTep911llN85YtW1a6bqu+Wq36gbUyefLkpnmtZvp95pny+9O0msrnwQcfbJrnfmLlHMTMrNIcxMys0hzEzKyyPHbSzCrPNTEzqzQHMTOrNAcxM6s0BzHrl1YHzM9//vPS/Lvvvrtp3ujRo0vXfeihh0rzTznllNL8xx57rDS/bE6vVv3A1l+//PBsdTu5t7+9z70orA1u2DezynNNzMwqzUHMzCrNQczMKssDwM2s8hzEzKzSfHXSzCptRNXEJO0IXARsD6wBZkbEtyVtDfwIGA/MB/42Iv4weEVdd+23336l+ePHj2+a98QTT5Su+7Of/ayj/Fb3zNxwww1L8zvZ9uc+97nS/D322GPA+16XVa1NrJ064yrgsxGxKzAZOF7SbsBJwA0RsTNwQ35uZiNALZC1evSClkEsIhZFxJ35/2XAXGAHYBpwYV7sQuCwwSqkmQ2tKgWxfrWJSRoPvAH4DbBdRCyCFOgkbdtkneOA4wDGjRvXSVnNbIhUqWG/7ZJKGg38FDgxIp5rd72ImBkRkyJiUqs51c1s+LVbC+uVmlhbQUzSBqQA9sOIuCwnPy1pbM4fCywenCKa2VAbUUFMqaTnAnMj4oxC1pXA0fn/o4Erul88MxsOVQpi7bSJ7QscBdwr6a6cdjJwGvBjSccAvwPeNzhFHPladSV41ateVZo/a9aspnnf+MY3Ste9/vrrS/P/+Mc/luavXLmyNH/TTTdtmrfXXnuVrvvhD3+4NP/II48szS97X3vlC9irqvT+tAxiEXEr0OwVecImsxFoRAUxM1u3eFJEM6u8btbEJM0HlgGrgVURMambI36qE27NbMgMQsP+/hExMSIm5eddG/HjIGZmfQzB1cmujfhxEDOztQxCZ9cAfiFpTh7BA3UjfoCGI37a4TYxM+ujHwFqG0mzC89nRsTMumX2jYiFeWjidZIe7EohMwexHtBpI+rEiROb5l188cWl6z7++OOl+Y888khp/urVq0vzd9lll6Z5r3zlK0vX7VSVugn0mn5cnVxaaOdqKCIW5r+LJV0O7E0e8ZPHXXc04senk2bWR7dOJyVtJmnz2v/AAcB9dHHEj2tiZraWLg8p2g64PG9vfeCSiLhG0h10acSPg5iZ9dGtIBYR84DXN0j/H7o04sdBzMz6qFJ7ooOYmfXhYUdmVlm9NM1OOxzEzKwPBzGrjAkTJnSUbyOTg5iZVZqDmJlVmoOYmVWWJ0U0s8pzTczMKs1BzMwqzUHMzCrLnV3NrPIcxMys0nx10swqzTUxM6sst4mZWeU5iJlZpTmImVmlValhv2VJJe0o6UZJcyXdL+kfcvoMSU9Juis/Dh784prZYBuEm+cOqnZqYquAz0bEnfnWS3MkXZfzvhkRpw9e8cxsOPRKgGpHyyCWbzFeu934MklzgR0Gu2BmNnyqFMT6deIraTzwBuA3OekESfdIOk/SVl0um5kNkyqdTrYdxCSNBn4KnBgRzwHnADsBE0k1tW80We84SbMlzV6yZEkXimxmg23EBTFJG5AC2A8j4jKAiHg6IlZHxBrg+8DejdaNiJkRMSkiJo0ZM6Zb5TazQVKbFLGdRy9o5+qkgHOBuRFxRiF9bGGx9wD3db94ZjYcqlQTa+fq5L7AUcC9ku7KaScDR0qaCAQwH/jYoJTQzIZcrwSodrRzdfJWoNErurr7xTGzXjCigpiZrVt66VSxHQ5iZtaHg5iZVVqvXHlsh4OYma3Fp5NmVnkOYmZWaQ5iZlZpDmJmVlm1YUdV4SBmZn24JmZmleYgZmaVVqUgVp0TXzMbMt2cxULSQZIekvSopJO6XVYHMTNbSzdvFCJpPeAs4J3AbqTZb3brZnl9OmlmfXTx6uTewKMRMQ9A0qXANOCBbu3ANTEz66OLp5M7AE8Wni+gyzcaGtKa2Jw5c5ZKeqKQtA2wdCjL0A+9WrZeLRe4bAPVzbL9VacbmDNnzrWStmlz8Y0lzS48nxkRMwvPG0W6GHjp+hrSIBYRa02yL2l2REwayjK0q1fL1qvlApdtoHqtbBFxUBc3twDYsfD8FcDCLm7fp5NmNqjuAHaWNEHShsARwJXd3IEb9s1s0ETEKkknANcC6wHnRcT93dzHcAexma0XGTa9WrZeLRe4bAPVy2XrWERczSDek0MRXW1jMzMbUm4TM7NKG5YgNtjDEDohab6keyXdVXfpeDjKcp6kxZLuK6RtLek6SY/kv1v1UNlmSHoqv3d3STp4mMq2o6QbJc2VdL+kf8jpw/relZSrJ963qhry08k8DOFh4B2ky693AEdGRNd68HZC0nxgUkQMe58iSW8BlgMXRcQeOe3rwDMRcVr+AdgqIr7YI2WbASyPiNOHujx1ZRsLjI2IOyVtDswBDgOmM4zvXUm5/pYeeN+qajhqYi8NQ4iIlUBtGILViYhbgGfqkqcBF+b/LyR9CYZck7L1hIhYFBF35v+XAXNJvcSH9b0rKZd1YDiC2KAPQ+hQAL+QNEfSccNdmAa2i4hFkL4UwLbDXJ56J0i6J59uDsupbpGk8cAbgN/QQ+9dXbmgx963KhmOIDbowxA6tG9E7EkadX98Pm2y9pwD7ARMBBYB3xjOwkgaDfwUODEinhvOshQ1KFdPvW9VMxxBbNCHIXQiIhbmv4uBy0mnv73k6dy2UmtjWTzM5XlJRDwdEasjYg3wfYbxvZO0ASlQ/DAiLsvJw/7eNSpXL71vVTQcQWzQhyEMlKTNcoMrkjYDDgDuK19ryF0JHJ3/Pxq4YhjLspZagMjewzC9d0rTK5wLzI2IMwpZw/reNStXr7xvVTUsnV3zJeRv8fIwhK8OeSEakPRKUu0L0miGS4azbJJmAVNJsxw8DXwZ+BnwY2Ac8DvgfREx5A3sTco2lXRKFMB84GO1NqghLtubgf8C7gXW5OSTSe1Pw/belZTrSHrgfasq99g3s0pzj30zqzQHMTOrNAcxM6s0BzEzqzQHMTOrtMoHMUn35YHHtefzJX2uw23eJOnMjgs3RCRNl3TTIG275XuR9798MPY/EJKmSop+3OzCKqzrQUzSBfkACkkvSpon6fTceXQovBE4u50FS758hwNf6mqp2iTpjZKul/QHSX+UdIOkjnpwS/q4pOdz5+Ja2oaSVki6t27ZnfNn97actNZ70Y0fibpt1Y6VFfkH6WPd2Ha3SNpK0sWSns2PiyVt2WKd0ZK+K2mBpD8pTTv16UL+1jn/wZz/pKRzJP2fJtvbWNLd+X2aVEh/vaRZef3afj4vqfKVk/4YrBd7PTAWeCXwj8AngKbTjOShGF0REUsiYkWH23gmzzIwpPKYumtIw7D2AaaQxtJdWxtJMEC/BDZl7eEsbwKeBV4tqXgXqqnAC8BtMCTvxamkY+V1pI68/ybp/Y0WlLR+7vU+lC4BamNpD8r/X9xinTOAdwFHAbsCXwVOk3RUzv9L0qQHXwBeC3wIeAswq8n2TicN16u3F7Ak72d3UofjfwZ6ao6+QRcRXX0AFwA/r0v7PrAo/z+V1DP5YOC3wErgkJx3KGmOpT8Dj5M+/A0L29mWNFTkT8ATwEdJQzRmFJaZD3yu8HwL0gDbRXm7c4H3F8pRfMzI69wEnFnYxlakqVv+kPd9PbB7IX86aW6tt+fyPA/cCEzo53s3KZdjQiFtQk6bVLLedOCmFtt+CvinwvN/Jn0Zf0XquV5LvwS4sfD8pfci/7/We9bJ66//rHLaw8Cs/P+MvL3pwGPAamA0sBFpxMfT+TP9NfDmwjZqn+0hwF15mTnAXv38PHbN29m3kPbmnLZLyXr3AafUpd1cPKYarHMwqRf/FnXp04D7C2Vpehzk5b8OzOn297qXH0NV7fwTUF/b+n+kWtprgN9IOhD4IXAm6Vflo8B7gX8trHMB8Crgr0lzQX0YGN9sp/lX+z+BtwIfAXYDPkMKnLcBJwIrSDWBsTSvLV5AqrlMI9VmVgDXSNqksMxGpNOuj5JqUFsC/1Yoy/h8OjC9WXmBh0i/rMdI2kjSRsCxpCEynd4h5kZg/8Lz/UlB6aa69Kl52UYOJ9UIarWn4pi/0tffD39m7WNlAvAB4H3A63P+10k/RB8lTWdzL+nzGLv2pjgd+CLpx2Ee8B+SNq1l5s9jRklZppCC822FtF+RgvQ+JevdChwqace8n31Iw4quKVlnC1IN+KWzCEmvIP0Af5D0HWrHFqQf23VHt6MidTUx0pd+KfCjul/Jv6lb7xYKNYWcdhjpIBLwavr+Kv4V6dd5RiFtPvnXnTR77Bpg1yZlnU6aUbM+/SZern3snPf7lkL+X5BOxf6usJ21fp1JB95KYFR+vgPwIPCeFu/fbsAj+XWtJtVAXt1inem0rokdQ/oibARsnP/fiTTIfW5e5jX5dby50XtR//7W7b/09TcpU/GzWr+wnb/PaTOAF0nzgNXW2Sxv98OFtPXy+/QvdcfYBwvLjAb+WPvMctqDwAkl5TsZmNcgfR7wpZL1NgTOy2V4MT8+XrL8lvkz/07da7oZ+Gx+Pp7WNfI9SUH+b5otMxIfg3XLtoNyg/n6pF/VK4BP1i1TP3/9XsDekorTBY8CNgG2J1Wn15BOQQGIiCcklU3j8wbSaezcAb2KpLbf2wv7fTY3iO9WWO6FiHio8Hwh6bVvSZoS+SlSkGgq1+zOy/v6AOlA/hxwhaRJEfF8B6/jRlLwmkL6UVgaEY9J+j2wk6TtSTWyFbw8UV9/lL7+kvW+mmtDG5GC0/8HvlfIXxARTxee75S3+6taQkSslnQ7a38esPZntrz+M4uI0s+jtliDNDVJr/kksC/wblKzx1uA0yXNj4i1amP5gtdVpNP9LxSyTiYFv+IsHE1J2gX4D+BbEfHTdtYZKQYriN0CHEf6EBZGxIsNlqn/Qo4CTgF+0mDZJTSeTLGVbjQCl22jeCCvapLXn1P2D5C+pPtGxGoASR8gnR68B/j3fmxr7cJEzJP0BKmWIlINi4h4XtKcnD4VuLXJ59XKQF//GaTpaVaQfnDqg0P9cVL7PBoFkW7PZvB7YFtJqpUrN1GMIbXH9ZF/iL5Game8KiffI2ki6QfpmsKyo3n5foyHRMSfC5t6O7Af8GLdtYxfS/pRRHywsJ3XkH6kLo2IdatRn8G7OrkiIh6NiCf68YW4E3hNXq/+sYrUID+K1IUCAEnjSFd6yrY5VtKuTfJXkmo7ZR7I+51S2O8WpKtK3b65yaakL+KaQtqanNaNz6rWLlZrD6u5CXgbKYj9ssU22nnP+uN/8me8sEEAa+TRXIY31xKUbj4zhb6fx+TCMpsBe5COo3bdTjoNnVJIm0I6pb2t4RqplrgBqSmgaDWFzzBfbb6G9F4eHBH1XX0+QmoDnJgftTsgfZDUzlfbzm6kz+8nEfFp1kG91J/kVOADkk6VtIek10h6r9LdfcinKtcA35M0Jf+yXUB5g+cNpFOjn0o6UGkixndIqt0gYj6wcU7bptjoWxMRj5BOh78naT9JryXViJ4jXclri6Qdcr+g95Qsdh2pYfZsSbtK2h04n/QFaBVc2nEj6Yv9JtYOYjeTJqfcluaN+jXzgf3y6xnyzqT5lPocUpeFg/MP1DnAdvTtH/iP+bPdnXSavpLCZ5Y/jxNK9jWXl4+5yZKmkE51f147da7/XCNNN31zLt/UfMxNJ12EujyvsznwC9JV7+nAZpK2z48N83Yej4j7ag/SVVuAxyJiQd7O7qTP6ybgXwvb2L6fb2ul9UwQi4hrSX1r9ie1e/2W1N/ld4XFppO6XvyS1I5wCelL1Wyba0j9e35FCjxzgW+TGl6JiNtIV9BmkU5Zv9B4S3wkl+fK/HdT4KCIaPeKEaRf511IFwWalfdBUjeT15JqAbeSpu9+Z+3A7dCNpNe+OCIeK6TfSmp7fI7UFaHMP5OmF3+M9J4Nhy+SJjc8n9SF4nWkz6N+IsGTSPPV30m6QHNIXbviLqRJHct8ELibFHSuzf8fVchv9LkeQZrB+Iek2uFJwD+RrrxDav+dTGqfe5jU/af2KLvqWe99pB+e99dtY52aUNGTIo4A+Zd+ekRMHeaimA25nqmJmZkNhIOYmVWag9jIcBfpIofZOsdtYmZWaa6JmVmlVS6ISVreYhC14YkS68kTJY5YgxLEJH1J0h2SnpO0RNJVkvYYjH012f/mkk5RmmRvhaRnJM2RdHJVD2JJH5J0l6Q/S1oq6aIOt+eJEgdIA5soUZJmSFqoNIHhTbmzaqNlm02COEbStXkbLyhNhniWpL+oW/9ASbdLWpaPlSskvbo7r773DFZNbCqp9/Q+pOEsq4DrJW09SPt7iaStSB1FjwG+SRomMoXUSXMX0vQtlSLpU6SB0aeThs7sTxpF0AlPlDhwA5ko8QvAZ0mDw98ILAauU+PJLptNgriG1Ov/UNKsLtNJYyy/X1tA0gTSsfFfpAkQ/prUkflqRhaSBAQAAAZHSURBVKqhmCqDNP5sNXBoIW0+aT6x75F6ii8APl+33qtIQyr+TJpr6xDS1DzTS/Z1Tl5mhyb5qitD/bQyN7H21DMbkuY+W0AajHwHcGAhfwPgO6RZG14AngROK+QfDtxDGh71DGlIynbNyt+gvFvm/b6jn+/5dDxRYk9MlEgatL4I+L+FtE2AZcDH6pbt7ySInyJPOJqfvze/J+sV0vbP29qm0+9yLz6Gqk1sc1Ktr36ytk+TJrTbkxQovp7Hp6E0T/jlvDz4+qOkA3ijZjvJ6xwB/HukqW/6qH3D+uF80qSKHyANB7oQuErS63P+p0gzTBxBGtryflLAJY9huzSvsytpSpaXfrHV3kSJB5AGCW8n6QFJT0m6XNIr+/k6GvFEiUMzUeIE0nRSv6glRBqydktxHfVzEkRJf0l6/28uJM8mzR7zd5LWyzW9o4E7ImJpq21W0lBEStI4t/9m7V+H+eRf10LaI8A/5v8PIP2ijGvwize9yX62y/mfrku/jXTgLQf+s64MTWtipGlx1hTLkNN/Bpyd//8OaaC5GpRnz1yev2pS3pYTJZLG3a3Myx1EOv37OWmeqk1L1puOJ0rsiYkSSYEqGhxH5wHXFsrb1iSIpLG+K3L+VcAmdfn7kaYRWpWP3znAtt38TvfSY9BrYpLOIAWfv4k8R1bBPXXPF5IGtEKquTwVEcUB4L9h7Wlq2vV+0nQml5Oq8e3ak3Qq8EC+Kro8X3F7F+nLDqmT6UTg4dzI+i69fLeZu0nz8d8n6aeS/r7Y1hQRT0XEayLi8pIyjCLVQj4VEddExG9JwWBbUttIJ4oTJU4hT5RIqlkM9kSJZb6a3+c/AWcxwIkSSW2jpRMlkmpsa02UGBGtbtc3kIkSG61XXKc/kyB+mnRsHka6Gc+3Xtpg+szOBS4itb1NJZ22/lgj9C5IgzUpIgCSvkk6zdo/IuY1WKR+rrHivFkDaaxdQvplXWvGzoh4MpfnWdIMDDVrGuyneNoyKpfpjQ3K+qe87TsljSfVkt5GOnW8W9I7Is04egBpxoIDSDWfr0l6a0Tc3eZrqs1I8NJcWZFmll0IjGtzGw2FJ0ociH5PlJjXgXRK+WQhfdvCOm1PghgRv8/bfFDS/wD/Jelf8nF+PPB8RLw0I4ukD+X97kOasWREGbTILOnbpHaLt0WaYqa/HgB2UL7ZQrY3JWWONPXOj4AP1a3XzBIKbTiSNmbtAPjfpC/I9tF3osaX2twiYllE/CQi/p5US3sb6aIEkdweEaeQguFCUs2wXbXaxS6Fco7O5X6iH9tpxhMlDv5EiY+Tgs47CvvemBS0auu0NQliA7XvQ62teFMaT8hYXHZkGYxzVNIpwHOkL8H2hcfowjLzKW+PGkW6SnMj6UOdQprL60XKr05uTToonwL+jnRg7ESa7/w+4IbCsl8j/RJOJd1h6ZJc7mJ7z7+TgsV7SVX3SaRphg/P+Z8BjiSd/r6KNF/Zs6SDaTLpCuwbSbWmaaSq/Yfyuu3ePORnuez7kk59fpLfv47axPJyHyZdVX0B2KmQflB+LwLYu9nnlJ//gtROtwP5ChgNbsLCy+1STa+SNTou6vJnAPc1SP8W6Qfi4PxZzCS1gY6t2/cDpGCyO+kH72lgs8J2StvE8jL/SToNnZyPy3uBqwr5fT5XUiB6jtQQvwfpgs9CYPMm+xhPXZsY6crq0Xn98aQfzAeA2wvLvI10hvFl0oWmPUkTO/6u+DpH0mNwNtr3fo61x4yyg7XBl+PVpMbOF0iN/u+mRReLvN4WwL/kD/hP+XEP6fZv29YtN4sUdJ4i3eS3vgwb5C/OPNKv/e9JkyPulfOPJU26tywfpDcD++S8XfMB/3R+DY8CX2hwoLZ6PZsDPyB10fgDqTF3pxbrTKe9ILZjLsOTdemjST8Yz1K4INPkc5pMav/7M3VdLOrWm8rgBbFiF4sXaN7F4t35WHghf25vbHDszmi2/7zM1rw8u+9z+f8tyz5XUo1+Bi/f//RmYI+SfdS2UQxif02qCf6RdEw/TLqqv1Xdukfk17acdLZxFbDbYHzXe+HhAeAjlDxRoq0jRuY5spmtMxzEzKzSHMRGLk+UaOsEt4mZWaW5JmZmleYgZmaV5iBmZpXmIGZmleYgZmaV9r/CyHkUnkfv9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_name = \"8_1.jpg\" \n",
    "\n",
    "fname = \"Sample Images/\" + image_name\n",
    "image_data = 255 - np.asarray(Image.open(fname).convert('L').resize((28,28)))\n",
    "\n",
    "def predict_real_time(image_data, second_guess = True):\n",
    "    image_flattened = image_data.reshape(image_data.shape[0]*image_data.shape[1],-1)\n",
    "    image_norm =(image_flattened/255.)\n",
    "\n",
    "    prediction = predict(image_norm, parameters, second_guess = second_guess)\n",
    "    \n",
    "    return prediction\n",
    "    \n",
    "prediction = predict_real_time(image_data, second_guess = True)\n",
    "\n",
    "first_lbl, first_prob = prediction[\"First Prediction\"]\n",
    "sec_lbl, sec_prob = prediction[\"Second Prediction\"]\n",
    "\n",
    "# plt.title(\"True Label: \"+ str(label.squeeze()))\n",
    "plt.xlabel(\"Prediction: %d | With Prob: %.4f \\n2nd Guess: %d | With Prob: %.4f\"%(first_lbl, first_prob, sec_lbl, sec_prob), fontsize = 14)\n",
    "plt.imshow(image_data, interpolation ='nearest',cmap='binary')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "d = datetime.datetime.now()\n",
    "\n",
    "path = \"Saved Models/\"\n",
    "fname = str(d) + \" model val_98.46 test_98.32\"\n",
    "\n",
    "print(fname)\n",
    "\n",
    "model = {\"Parameters\": parameters\n",
    "         \"Activations\": [\"relu\",\"relu\",\"softmax\"]}\n",
    "## Save the following info if needed\n",
    "#          \"Hyper Parameters\": hyperParams,\n",
    "#          \"initialization\":\"random\",\n",
    "#          \"regularizer\":\"l2\",\n",
    "#          \"optimizer\":\"adam\"\n",
    "save_model(file_name = path+fname, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Saved Models/\"\n",
    "model = load_model(file_name =path+ \"2020-06-06 20:07:26.699179modelval_98-46_test_98-32\")\n",
    "print(type(loaded_params), type(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(loaded_params) // 2\n",
    "print(\"Total Layers %d: \"%L)\n",
    "for l in range(L):\n",
    "    print(loaded_params[\"W\" + str(l+1)].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Optimize the code (Code Refactoring)\n",
    "- [ ] Prepare the doc String\n",
    "- [x] add verbose: Integer. 0, 1, 2 or 3. Verbosity mode.\n",
    "- [ ] Batch norm\n",
    "- [ ] Maxout\n",
    "- [ ] Drop Connect\n",
    "- [ ] Data Augmentation if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
