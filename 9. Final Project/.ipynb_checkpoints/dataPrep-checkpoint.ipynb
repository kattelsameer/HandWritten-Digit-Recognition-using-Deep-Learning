{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Libraries for importing data from binary file\n",
    "import os\n",
    "# import os.path #for accessing the file path\n",
    "import struct  #for unpacking the binary data\n",
    "\n",
    "import time    #for calculating time\n",
    "\n",
    "from urllib.request import urlopen #for downloading the dataset\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "import gzip as unzip #to unzip the downloaded dataset\n",
    "\n",
    "#core packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help from  https://gist.github.com/kissgyorgy/6102803 for bufferring part\n",
    "def download_dataset(dataset, to_path):\n",
    "    \n",
    "    if dataset == \"mnist\":\n",
    "        urls = [\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "                \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "                \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "                \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"] \n",
    "    \n",
    "    elif dataset == \"fashion_mnist\":\n",
    "        urls = [\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\",\n",
    "                \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\", \n",
    "                \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\",\n",
    "                \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\" ] \n",
    "    else:\n",
    "        raise ValueError(\"Only 'mnist' and 'fashion_mnist' dataset are supported\")\n",
    "    \n",
    "    path = to_path #destination path\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Directory not found: It should be handled from the load_dataset(parent) module\")\n",
    "        return\n",
    "#         print(\"No destination directory exists to load the data: Creating '\" + path + \"' as a new directory.\\n\")\n",
    "#         os.makedirs(path) #making directories recursively\n",
    "    \n",
    "    num_files = len(urls)\n",
    "\n",
    "\n",
    "    down_status = \"succeeded\"\n",
    "    \n",
    "    for ind, url in enumerate(urls):  \n",
    "        filename = url.split('/')[-1] # getting the filename from the url\n",
    "        \n",
    "        #if file does not exist then download\n",
    "        if not os.path.exists(path+filename):\n",
    "            print(\"\\n%s: downloading...\"%filename)\n",
    "            \n",
    "            try:                \n",
    "                u = urlopen(url)\n",
    "                with open(path + filename, 'wb') as data_file:\n",
    "                    #getting the file size\n",
    "                    file_size = int(u.info()[\"Content-Length\"])\n",
    "                    file_size_mb = file_size/(1024*1024) #for linux system use 1000 instead of 1024\n",
    "\n",
    "\n",
    "                    downloaded_file_size = 0\n",
    "                    block_size = 1024 #setting the block size to read the data from the url\n",
    "                    while True:\n",
    "                        #bufferring the file content\n",
    "                        buffer = u.read(block_size)\n",
    "                        if not buffer:\n",
    "                            break\n",
    "                        #adding up downloaded file size\n",
    "                        downloaded_file_size += len(buffer)\n",
    "                        downloaded_file_size_mb = downloaded_file_size/(1024*1024)\n",
    "\n",
    "                        data_file.write(buffer)\n",
    "                        #calculating the downloaded percentage of the file\n",
    "                        down_percent = downloaded_file_size * 100. / file_size\n",
    "                        inc = int(down_percent)//10\n",
    "                        print (\"%.3f MB  [%.3f MB done %s>%s %.0f%%]\" %( file_size_mb, downloaded_file_size_mb, '=' * inc,'.'*(10-inc), down_percent), end = \"\\r\")\n",
    "\n",
    "            except HTTPError as e:\n",
    "                print('Download Failed: ', e)\n",
    "                down_status = \"failed\"\n",
    "            except URLError as e:\n",
    "                print('Download Failed: ', e)\n",
    "                down_status = \"failed\"\n",
    "        else:\n",
    "            print(\"\\n%s: already exists.\"%filename)\n",
    "    print(\"\\n\\nDataset download %s...\\n\"%down_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(dataset = \"mnist\", to_path = \"dataset/mnist/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(dataset =\"fashion_mnist\" , to_path = \"dataset/fashion_mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompressing the gzip Dataset files to the desired binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tutorialspoint.com/working-with-zip-files-in-python\n",
    "# https://www.geeksforgeeks.org/os-walk-python/\n",
    "\n",
    "\n",
    "def is_gzip(filename):\n",
    "    #checking the extention of the file to determine if it is a gzip file or not\n",
    "    ext = filename.split(\".\")[-1]\n",
    "    if ext == \"gz\":\n",
    "        return True\n",
    "\n",
    "def get_files(path,file_type = \"all\"):\n",
    "    files = []\n",
    "    #accessing all gzip files in the supplied path\n",
    "    for root, dirs, file in os.walk(path):\n",
    "        for fname in file:\n",
    "            if file_type == \"gzip\":\n",
    "                if is_gzip(fname): #accessing only gzip file paths\n",
    "                    files.append(fname)\n",
    "            else:\n",
    "                files.append(fname)\n",
    "    return files\n",
    "      \n",
    "\n",
    "    \n",
    "def decompress_dataset(path, keep_original = True):\n",
    "    files = get_files(path, file_type = \"gzip\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(\"No gzip file to decompress.\")\n",
    "        return\n",
    "    \n",
    "    for filename in files:\n",
    "        try:\n",
    "            with open(path + filename.split(\".\")[0],'wb') as fp: #opening a file on which the zip file content is to be written\n",
    "                with unzip.open(path + filename, 'rb') as fzip: #opening the zip file to be unzipped\n",
    "                    file_data = fzip.read()\n",
    "                fp.write(file_data) \n",
    "            if keep_original == False:\n",
    "                os.remove(path+filename) #removing the gzip file after decompression\n",
    "        except unzip.BadZipFile:\n",
    "            print('Error: Invalid gzip file encountered.')\n",
    "    \n",
    "    print(\"Dataset decompression succeeded...\")\n",
    "    if keep_original == False:\n",
    "        print(\"Original gzip files removed...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompress_dataset(path = \"dataset/fashion_mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriving data from binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_dataset(path, filename):\n",
    "    \"\"\"\n",
    "        Retrive MNIST dataset from  the binary file into numpy arrays        \n",
    "        \n",
    "        Dataset Obtained From:\n",
    "            - link -- http://yann.lecun.com/exdb/mnist/\n",
    "            \n",
    "        Dataset retrival code adapted from(but modified to our need making data retrival 6-8 times faster):\n",
    "            - link -- https://www.cs.virginia.edu/~connelly/class/2015/large_scale/proj2/mnist_python\n",
    "            \n",
    "        Argument:\n",
    "            - **dataset** -- type of dataset to be loaded. may be either 'training' or 'test'\n",
    "        Returns:\n",
    "            - **images** -- 3D array consisting of no. of examples, rows, columns of images \n",
    "            - **labels** -- array  containing labels for each images\n",
    "    \"\"\"\n",
    "    #setting file path based on the dataset\n",
    "    train_img_file_path = path + filename[0]\n",
    "    train_lbl_file_path = path + filename[1]\n",
    "    test_img_file_path = path + filename[2]\n",
    "    test_lbl_file_path = path + filename[3]\n",
    "     \n",
    "    #retriving the training data\n",
    "    with open(train_img_file_path, 'rb') as train_fimg, open(train_lbl_file_path, 'rb') as train_flbl :\n",
    "        #retriving labels\n",
    "        _, size = struct.unpack(\">II\", train_flbl.read(8))\n",
    "        train_labels = np.frombuffer(train_flbl.read(), dtype=np.int8).reshape(size,1)\n",
    "        #retriving images\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", train_fimg.read(16))\n",
    "        train_images = np.frombuffer(train_fimg.read(),dtype=np.uint8).reshape(size, rows, cols)\n",
    "       \n",
    "    #retriving the test data\n",
    "    with open(test_img_file_path, 'rb') as test_fimg, open(test_lbl_file_path, 'rb') as test_flbl :\n",
    "        #retriving labels\n",
    "        _, size = struct.unpack(\">II\", test_flbl.read(8))\n",
    "        test_labels = np.frombuffer(test_flbl.read(), dtype=np.int8).reshape(size,1)\n",
    "        #retriving images\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", test_fimg.read(16))\n",
    "        test_images = np.frombuffer(test_fimg.read(),dtype=np.uint8).reshape(size, rows, cols)\n",
    "       \n",
    "    assert(train_images.shape == (60000, 28, 28))\n",
    "    assert(train_labels.shape == (60000,1))\n",
    "    assert(test_images.shape == (10000, 28, 28))\n",
    "    assert(test_labels.shape == (10000,1))\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()\n",
    "path = \"dataset/mnist/\"\n",
    "    \n",
    "filename = [\"train-images-idx3-ubyte\",\n",
    "            \"train-labels-idx1-ubyte\",\n",
    "            \"t10k-images-idx3-ubyte\",\n",
    "            \"t10k-labels-idx1-ubyte\"] \n",
    "#retriving the data\n",
    "train_x_orig, train_y_orig, test_x_temp, test_y_temp = retrive_dataset(path, filename)\n",
    "\n",
    "tic = time.time()\n",
    "#displaying the retrival info\n",
    "print(\"Time to load data from binary file using numpy: \" + str(1000*(tic-toc)) + \"ms\\n\")\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_temp))+\"\\t\",str(test_x_temp.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_temp))+\"\\t\",str(test_y_temp.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample a portion of the retrived Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriving a small sample of the original dataset for model development and experimentation\n",
    "def sample_dataset(x,y, size_in_per):\n",
    "    \"\"\"\n",
    "        Returns a sample dataset from the fully processed dataset\n",
    "       \n",
    "        Arguments:\n",
    "            - **x** -- original input data\n",
    "            - **y** -- original output labels\n",
    "            - **sample_size** -- sample volume in percentage\n",
    "        Returns:\n",
    "            - **x_sample** -- input sample  from original dataset of size ( dataVol% of x)\n",
    "            - **y_sample** -- output sample  from original dataset of size (datavol% of y)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    sample_m = int(np.multiply(m,np.divide(size_in_per,100))) #int(m*(dataVol/100)) \n",
    "    \n",
    "    #suffling the original dataset\n",
    "    randCol = np.random.permutation(m)\n",
    "    x_suffled = x[randCol,:,:]\n",
    "    y_suffled = y[randCol,:]\n",
    "    \n",
    "    #taking samples of sample_size\n",
    "    x_sample = x_suffled[0:sample_m,:,:]\n",
    "    y_sample = y_suffled[0:sample_m,:]\n",
    "\n",
    "    assert(x_sample.shape == (sample_m,28,28))\n",
    "    assert(y_sample.shape == (sample_m,1))\n",
    "\n",
    "    return x_sample, y_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, size_in_per = 100):\n",
    "    \n",
    "    path = 'dataset/%s/'%(dataset)\n",
    "    \n",
    "    filename = [\"train-images-idx3-ubyte\",\n",
    "                \"train-labels-idx1-ubyte\",\n",
    "                \"t10k-images-idx3-ubyte\",\n",
    "                \"t10k-labels-idx1-ubyte\"] \n",
    "    gzip_filename = [fname+\".gz\" for fname in filename]\n",
    "\n",
    "    \n",
    "    #creating a new destination path if it doesnot exist\n",
    "    if not os.path.exists(path):\n",
    "        print(\"No destination directory exists to load the data from:\\nCreating '\" + path + \"' as a new directory...\\n\")\n",
    "        os.makedirs(path) #making directories recursively\n",
    "    \n",
    "    \n",
    "    if len(get_files(path)) == 0:\n",
    "        print (\"Downloading the %s dataset...\"%dataset)\n",
    "        download_dataset(dataset, to_path = path) #downloading the dataset if the location is empty\n",
    "    \n",
    "    files = get_files(path) #getting all the files in the path after download\n",
    "    \n",
    "    #checking for all the decompressed files\n",
    "    file_check = all(fname in files for fname in filename)  \n",
    "\n",
    "\n",
    "    if file_check == False:       \n",
    "        gzip_filecheck = all(fname in files for fname in gzip_filename) #checking for all the gzip files\n",
    "        if gzip_filecheck == False:\n",
    "            print (\"Downloading missing gzip files of %s dataset...\"%dataset)\n",
    "            download_dataset(dataset, to_path = path) #downloading missing zip file of the dataset\n",
    "        print (\"Decompressing the %s dataset...\"%dataset)\n",
    "        decompress_dataset(path,keep_original = True)#decompressing the dataset\n",
    "    \n",
    "    train_x_temp, train_y_temp, test_x_temp, test_y_temp = retrive_dataset(path, filename)\n",
    "    \n",
    "    #getting the size of the data based on the sample size\n",
    "    #size = 100 means entire data is suffled and returned\n",
    "    train_x_orig, train_y_orig = sample_dataset(train_x_temp,train_y_temp, size_in_per)\n",
    "    test_x_orig, test_y_orig = sample_dataset(test_x_temp,test_y_temp, size_in_per)\n",
    "    \n",
    "\n",
    "    return train_x_orig, train_y_orig, test_x_orig, test_y_orig\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to load data from binary file using numpy: 0.0866 s\n",
      "\n",
      "Sample Size : 100%\n",
      "\n",
      "Data\t\t\t Datatype\t\t Dataset Size\n",
      "=================================================================\n",
      "Training Set Images:\t<class 'numpy.ndarray'>\t (60000, 28, 28)\n",
      "Training Set Labels:\t<class 'numpy.ndarray'>\t (60000, 1)\n",
      "Test Set Images:\t<class 'numpy.ndarray'>\t (10000, 28, 28)\n",
      "Test Set Labels:\t<class 'numpy.ndarray'>\t (10000, 1)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset_size_in_per = 100\n",
    "\n",
    "toc = time.time()\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig = load_dataset(dataset = \"mnist\", size_in_per = dataset_size_in_per)\n",
    "\n",
    "tic = time.time()\n",
    "#displaying the retrival info\n",
    "print(\"\\nTime to load data from binary file using numpy: %.4f s\\n\"%(tic-toc))\n",
    "\n",
    "print(\"Sample Size : %d%%\\n\"%(dataset_size_in_per))\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Dataset Size\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_orig))+\"\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_orig))+\"\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for fashion_mnist\n",
    "# labels = {0:\"T-shirt/top\",\n",
    "#           1:\"Trouser\",\n",
    "#           2:\"Pullover\",\n",
    "#           3:\"Dress\",\n",
    "#           4:\"Coat\",\n",
    "#           5:\"Sandal\",\n",
    "#           6:\"Shirt\",\n",
    "#           7:\"Sneaker\",\n",
    "#           8:\"Bag\",\n",
    "#           9:\"Ankle boot\"}\n",
    "# print(labels[0])\n",
    "# plt.imshow(test_x_orig[2000], cmap = \"Greys\")\n",
    "# a = int(test_y_orig[2000])\n",
    "# print(type(a))\n",
    "# plt.title(str(a)+\" \"+labels[a])\n",
    "# # print(255-test_x_orig[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_test_split(test_x,test_y):\n",
    "    \"\"\"\n",
    "        Randomly splits the test set to dev and test set\n",
    "        \n",
    "        Arguments:\n",
    "            test_x - test set images of size (10000,28,28)\n",
    "            test_y - test set labels of size (10000,1)\n",
    "        \n",
    "        Returns:\n",
    "            dev_x  - dev set images of size (5000,28,28)\n",
    "            dev_y  - dev set labels of size (5000,1)\n",
    "            test_x - test set images of size (5000,28,28)\n",
    "            test_y - test set labels of size (5000,1)\n",
    "    \"\"\"\n",
    "    m = test_y.shape[0]\n",
    "    n = m // 2\n",
    "    #suffling the test dataset\n",
    "    randCol = np.random.permutation(m)\n",
    "    suffled_x = test_x[randCol,:,:]\n",
    "    suffled_y = test_y[randCol,:]\n",
    "    \n",
    "    #splitting the test set into dev and test set , 50% each\n",
    "    dev_x = suffled_x[0:n,:,:]\n",
    "    dev_y = suffled_y[0:n,:]\n",
    "    \n",
    "    test_x = suffled_x[n:m,:,:]\n",
    "    test_y = suffled_y[n:m,:]\n",
    "    \n",
    "    assert(dev_x.shape == (n,28,28))\n",
    "    assert(dev_y.shape == (n,1))\n",
    "    assert(test_x.shape == (n,28,28))\n",
    "    assert(test_y.shape == (n,1))\n",
    "    \n",
    "    return dev_x,dev_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x_orig,dev_y_orig,test_x_orig,test_y_orig = dev_test_split(test_x_temp, test_y_temp)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Dev Set Images:\\t\\t\" + str(type(dev_x_orig))+\"\\t\",str(dev_x_orig.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(type(dev_y_orig))+\"\\t\",str(dev_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_orig))+\"\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_orig))+\"\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Validating Raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset visualization using charts\n",
    "def visual_charts(train_y_orig, dev_y_orig, test_y_orig):\n",
    "    \"\"\"\n",
    "        Plots bar graph showing the number of examples in each class\n",
    "\n",
    "        Arguments:\n",
    "            trainy_orig - labels of training set\n",
    "            dev_y_orig - labels of dev set\n",
    "            test_y_orig - labels of test set\n",
    "    \"\"\"\n",
    "    datasets = {\"Training Set\":train_y_orig,\"Dev Set\": dev_y_orig,\"Test Set\": test_y_orig}\n",
    "    \n",
    "    #setting the plot style\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    #creating subplots\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1,figsize=(10,15))\n",
    "    fig.subplots_adjust(hspace=.2)\n",
    "    i = 0\n",
    "    \n",
    "    #plotting the bar graph for each dataset labels\n",
    "    for dataset,datalabel in datasets.items():\n",
    "        unique, counts = np.unique(datalabel, return_counts=True)    \n",
    "        axes[i].bar(unique, counts)\n",
    "        max_value = np.max(counts)\n",
    "        axes[i].set(xticks = unique, ylim = (0,max_value + max_value // 10))\n",
    "        axes[i].set_title(\"Number of Examples in \" + dataset , fontsize = 16)\n",
    "        axes[i].set_xlabel(\"Classes\", fontsize = 12)\n",
    "        axes[i].set_ylabel(\"Number of Examples\", fontsize = 12)\n",
    "        i += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_charts(train_y_orig, dev_y_orig, test_y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_charts(train_y_sample, dev_y_sample, test_y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(x_orig, y_orig, dataset = \"training\"):\n",
    "    \"\"\"\n",
    "        Plots 10 sample images from the dataset with labels\n",
    "        \n",
    "        Arguments:\n",
    "            x_orig - 3D array representation of input images\n",
    "            y_orig - array of labels\n",
    "            dataset - type of dataset, can be training, dev or test\n",
    "        \n",
    "    \"\"\"\n",
    "    #recovering matplotlib defaults\n",
    "    plt.rcParams.update(plt.rcParamsDefault) \n",
    "    \n",
    "    #checking dataset type\n",
    "    if(dataset == \"training\"):\n",
    "        visual_title = \"Training Data Set\"\n",
    "        rng = range(1040,1050)\n",
    "    elif(dataset == \"dev\"):\n",
    "        visual_title = \"Dev Data Set\"\n",
    "        rng = range(100,110)\n",
    "    elif(dataset == \"test\"):\n",
    "        visual_title = \"Test Data Set\"\n",
    "        rng = range(540,550)        \n",
    "    else:\n",
    "        raise ValueError(\"Dataset set must be training or dev or test set\")\n",
    "     \n",
    "    #creating subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=.1)\n",
    "    fig.suptitle(visual_title)\n",
    "    \n",
    "    #plotting the sample images along with their labels\n",
    "    for ax,i in zip(axes.flatten(),rng):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest')\n",
    "        ax.set(title = \"Label: \"+ str(y_orig[i,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_x_orig, train_y_orig, dataset = \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(dev_x_orig, dev_y_orig, dataset = \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(test_x_orig, test_y_orig, dataset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_input(train_x_orig,dev_x_orig,test_x_orig):\n",
    "    \"\"\"\n",
    "        Flattens the 3D numpy array of the input images\n",
    "        \n",
    "        Arguement:\n",
    "            train_x_orig -  training set images of size (60000,28,28)\n",
    "            dev_x_orig   - dev set images of size (5000,28,28)\n",
    "            test_x_orig  - test set images of size (5000,28,28)\n",
    "\n",
    "        Returns:\n",
    "            train_x_flatten - flattened training set input data of size (784,60000)\n",
    "            dev_flatten     - flattened training set dev data of size (784,5000)\n",
    "            test_x_flatten  - flattened test set input data of size (784,5000)\n",
    "            \n",
    "    \"\"\"\n",
    "    m = train_x_orig.shape[0] #number of examples in training set\n",
    "    n = dev_x_orig.shape[0] # number of examples in dev and test set\n",
    "    \n",
    "    \n",
    "    #flattening the image--The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   \n",
    "    dev_x_flatten = dev_x_orig.reshape(dev_x_orig.shape[0], -1).T    \n",
    "    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "   \n",
    "    \n",
    "    assert(train_x_flatten.shape == (784,m) )\n",
    "    assert(dev_x_flatten.shape == (784,n) )\n",
    "    assert(test_x_flatten.shape == (784,n) )\n",
    "    \n",
    "    return train_x_flatten, dev_x_flatten, test_x_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten,dev_x_flatten,test_x_flatten = flatten_input(train_x_orig,dev_x_orig,test_x_orig)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"=====================================\")\n",
    "print (\"Input Training set:\\t\" + str(train_x_flatten.shape))\n",
    "print (\"Input Dev set:\\t\\t\" + str(dev_x_flatten.shape))\n",
    "print (\"Input Test set:\\t\\t\" + str(test_x_flatten.shape))\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(train_x_flatten,dev_x_flatten,test_x_flatten ):\n",
    "    \"\"\"\n",
    "        Normalizes the pixel values of the flattened images to the range 0-1\n",
    "        \n",
    "        Arguement:\n",
    "            train_x_flatten - flattened training set input data of size (784,60000)\n",
    "            dev_flatten     - flattened training set dev data of size (784,5000)\n",
    "            test_x_flatten  - flattened test set input data of size (784,5000)\n",
    "        Returns:\n",
    "            train_x_norm - normalized training set input data\n",
    "            dev_norm     - normalized training set dev data\n",
    "            test_x_norm  - normalized test set input data\n",
    "    \"\"\"\n",
    "    m = train_x_flatten.shape[1]\n",
    "    n = dev_x_flatten.shape[1]\n",
    "    \n",
    "    # Normalizing the data into the range between 0 and 1.\n",
    "    train_x_norm = np.divide(train_x_flatten,255.)\n",
    "    dev_x_norm = np.divide(dev_x_flatten,255.)\n",
    "    test_x_norm = np.divide(test_x_flatten,255.)\n",
    "    \n",
    "    assert(train_x_norm.shape == (784,m) )\n",
    "    assert(dev_x_norm.shape == (784,n) )\n",
    "    assert(test_x_norm.shape == (784,n) )\n",
    "    \n",
    "    return train_x_norm, dev_x_norm, test_x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_norm, dev_x_norm, test_x_norm = normalize_input(train_x_flatten,dev_x_flatten,test_x_flatten)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"=====================================\")\n",
    "print (\"Input Training set:\\t\" + str(train_x_norm.shape))\n",
    "print (\"Input Dev set:\\t\\t\" + str(dev_x_norm.shape))\n",
    "print (\"Input Test set:\\t\\t\" + str(test_x_norm.shape))\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y_orig,num_classes = 10):\n",
    "    \"\"\"\n",
    "        Transform the output labels into the one-hot encoding representation\n",
    "        \n",
    "        Arguments:\n",
    "            y_orig - raw labels loaded directly from the binary file\n",
    "            num_classes - number of the classes based on which the transformation is to be made\n",
    "        Returns:\n",
    "            y_encoded - encoded ndarray of the labels with data elements of int type\n",
    "    \"\"\"\n",
    "    #encoding the labels\n",
    "    y_encoded = np.eye(num_classes)[y_orig.reshape(-1)].T\n",
    "\n",
    "\n",
    "    assert(y_encoded.shape == (num_classes, y_orig.shape[1]))\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()\n",
    "#encoding the output of the training and the test dataset\n",
    "train_y_encoded = one_hot_encoding(train_y_orig.T)\n",
    "dev_y_encoded = one_hot_encoding(dev_y_orig.T)\n",
    "test_y_encoded = one_hot_encoding(test_y_orig.T)\n",
    "tic = time.time()\n",
    "print(\"Time to encode: \" + str(1000*(tic-toc)) + \" ms\\n\")\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"===================================\")\n",
    "print (\"Output Training set:\\t\" + str(train_y_encoded.shape))\n",
    "print (\"Output Dev set:\\t\\t\" + str(dev_y_encoded.shape))\n",
    "print (\"Output Test set:\\t\" + str(test_y_encoded.shape))\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(train_x_orig, train_y_orig, dev_x_orig, dev_y_orig, test_x_orig, test_y_orig):\n",
    "    \"\"\"\n",
    "        Flatten and Normalize the input images and encode the output labels\n",
    "        \n",
    "        Arguments:\n",
    "            train_x_orig -  training set images of size (60000,28,28)\n",
    "            train_y_orig -  training set labels of size (60000,1)\n",
    "            dev_x_orig   - dev set images of size (5000,28,28)\n",
    "            dev_y_orig   - dev set labels of size (5000,1)\n",
    "            test_x_orig  - test set images of size (5000,28,28)\n",
    "            test_y_orig  - test set labels of size (5000,1)\n",
    "        Returns:\n",
    "            train_x_norm - flattened and normalized training set input data\n",
    "            dev_norm     - flattened and normalized training set dev data\n",
    "            test_x_norm  - flattened and normalized test set input data\n",
    "            train_y_encoded - encoded label of training set\n",
    "            dev_y_encoded   - encoded label of dev set\n",
    "            test_y_encoded  - encoded label of test set\n",
    "    \"\"\"\n",
    "    #flatten the input images\n",
    "    train_x_flatten,dev_x_flatten,test_x_flatten = flatten_input(train_x_orig,dev_x_orig,test_x_orig)\n",
    "    \n",
    "    #normalize the input images\n",
    "    train_x_norm, dev_x_norm, test_x_norm = normalize_input(train_x_flatten,dev_x_flatten,test_x_flatten)\n",
    "    \n",
    "    #encode the output labels\n",
    "    train_y_encoded = one_hot_encoding(train_y_orig.T)\n",
    "    dev_y_encoded = one_hot_encoding(dev_y_orig.T)\n",
    "    test_y_encoded = one_hot_encoding(test_y_orig.T)\n",
    "    \n",
    "    return train_x_norm,train_y_encoded, dev_x_norm,dev_y_encoded, test_x_norm, test_y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_norm,train_y_encoded, dev_x_norm,dev_y_encoded, test_x_norm, test_y_encoded = prep_dataset(train_x_orig, train_y_orig, dev_x_orig, dev_y_orig, test_x_orig, test_y_orig)\n",
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_x_orig.shape)+\"\\t\\t\"+ str(train_x_norm.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_y_orig.shape)+\"\\t\\t\"+ str(train_y_encoded.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(dev_x_orig.shape)+\"\\t\\t\"+ str(dev_x_norm.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(dev_y_orig.shape)+\"\\t\\t\"+ str(dev_y_encoded.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_norm.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_encoded.shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
