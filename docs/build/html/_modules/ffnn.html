
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>ffnn &#8212; HandWritten Digit Recognizer 1.0 documentation</title>
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">HandWritten Digit Recognizer 1.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for ffnn</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Created on Mon Jun  8 10:26:57 2020</span>

<span class="sd">@author: befrenz</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">time</span>    <span class="c1">#for calculating time</span>

<span class="c1">#core packages</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1">#custom module</span>
<span class="kn">from</span>  <span class="nn">dataset</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">dev_test_split</span><span class="p">,</span> <span class="n">prep_dataset</span>
<span class="kn">from</span> <span class="nn">dataset</span> <span class="kn">import</span> <span class="n">visualize_data_distribution</span><span class="p">,</span> <span class="n">visualize_dataset</span>

<span class="kn">from</span> <span class="nn">ModelUtils</span> <span class="kn">import</span> <span class="n">relu</span><span class="p">,</span> <span class="n">relu_grad</span><span class="p">,</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">ModelUtils</span> <span class="kn">import</span> <span class="n">rand_mini_batches</span><span class="p">,</span> <span class="n">convert_time</span>
<span class="kn">from</span> <span class="nn">ModelUtils</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">plot_confusion_matrix</span><span class="p">,</span> <span class="n">model_metrics</span><span class="p">,</span> <span class="n">metric_summary</span>
<span class="kn">from</span> <span class="nn">ModelUtils</span> <span class="kn">import</span> <span class="n">visualize_training_results</span><span class="p">,</span> <span class="n">visualize_prediction</span><span class="p">,</span> <span class="n">visualize_mislabelled_images</span>
<span class="kn">from</span> <span class="nn">ModelUtils</span> <span class="kn">import</span> <span class="n">save_model</span><span class="p">,</span> <span class="n">load_model</span>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># initializing the layers</span>
<div class="viewcode-block" id="init_layers"><a class="viewcode-back" href="../ffnn.html#ffnn.init_layers">[docs]</a><span class="k">def</span> <span class="nf">init_layers</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">hidden_layers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes the layers of networks with the numberof nodes in each layers.</span>
<span class="sd">        </span>
<span class="sd">        Arguments:</span>
<span class="sd">            mnjnj.</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            knlknl.</span>
<span class="sd">            </span>
<span class="sd">        Example:</span>
<span class="sd">            Here, shape of x = (784,m)</span>
<span class="sd">                  shape of y = (10,m)</span>
<span class="sd">            &gt;&gt;&gt; layers_dim = init_layers(x, y, hidden_layers = [32,16])</span>
<span class="sd">            &gt;&gt;&gt; print(layers_dim)</span>
<span class="sd">            </span>
<span class="sd">            Outputs:</span>
<span class="sd">                [784, 32, 16, 10]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_nodes</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">output_nodes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">layers_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_nodes</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">:</span>
        <span class="n">layers_dim</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    
    <span class="n">layers_dim</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_nodes</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">layers_dim</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># initializing parameters</span>
<div class="viewcode-block" id="init_parameters"><a class="viewcode-back" href="../ffnn.html#ffnn.init_parameters">[docs]</a><span class="k">def</span> <span class="nf">init_parameters</span><span class="p">(</span><span class="n">layers_dim</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes the parameters (W,b) for each layer.</span>
<span class="sd">        </span>
<span class="sd">        Arguments:</span>
<span class="sd">            kngnjgjg.</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            nkngkg.</span>
<span class="sd">            </span>
<span class="sd">        Example:</span>
<span class="sd">            Here, layers_dim = [784, 32, 16, 10]</span>
<span class="sd">            &gt;&gt;&gt; parameters = init_parameters(layers_dim, initialization = &quot;random&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Layer\tWeight\t\tBias&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;================================&quot;)</span>
<span class="sd">            &gt;&gt;&gt; for l in range(1,len(layers_dim)):</span>
<span class="sd">            ...     print(str(l) +&quot;\t&quot; + str(parameters[&#39;W&#39;+str(l)].shape) +&quot;\t&quot;+ str(parameters[&#39;b&#39;+str(l)].shape))</span>

<span class="sd">            </span>
<span class="sd">            Outputs:</span>
<span class="sd">                Layer    Weight         Bias</span>
<span class="sd">                ================================</span>
<span class="sd">                1        (32, 784)      (32, 1)</span>
<span class="sd">                2        (16, 32)       (16, 1)</span>
<span class="sd">                3        (10, 16)       (10, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dim</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">):</span>
        <span class="c1">#initializing Weights</span>
        <span class="k">if</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;he&quot;</span><span class="p">:</span>
            <span class="c1"># he-initialization</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> 
        <span class="k">elif</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
            <span class="c1"># random initialization scaled by 0.01</span>
            <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span> 
        <span class="k">else</span><span class="p">:</span>
             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Initialization must be &#39;random&#39; or &#39;he&#39;&quot;</span><span class="p">)</span>
        
        <span class="c1">#initializing biases</span>
        <span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
     
        <span class="k">assert</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])),</span> <span class="s2">&quot;Dimention of W mismatched in init_params function&quot;</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">layers_dim</span><span class="p">[</span><span class="n">l</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="s2">&quot;Dimention of b mismatched in init_params function&quot;</span>
   
    <span class="k">return</span> <span class="n">params</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># initializing hyper parameters</span>
<div class="viewcode-block" id="init_hyperParams"><a class="viewcode-back" href="../ffnn.html#ffnn.init_hyperParams">[docs]</a><span class="k">def</span> <span class="nf">init_hyperParams</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">num_epoch</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keep_probs</span> <span class="o">=</span> <span class="p">[]):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        Arguments:</span>
<span class="sd">            knbknc.</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            getget.</span>
<span class="sd">            </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; hyperParams = init_hyperParams(alpha = 0.0001, num_epoch = 10, minibatch_size = 1024,lambd = 0.7,keep_probs = [0.8,0.8])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hyperParams</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span><span class="n">alpha</span><span class="p">,</span>
                   <span class="s1">&#39;num_epoch&#39;</span><span class="p">:</span><span class="n">num_epoch</span><span class="p">,</span>
                   <span class="s1">&#39;mini_batch_size&#39;</span><span class="p">:</span><span class="n">minibatch_size</span><span class="p">,</span>
                   <span class="s1">&#39;lambda&#39;</span><span class="p">:</span><span class="n">lambd</span><span class="p">,</span>
                   <span class="s1">&#39;keep_probs&#39;</span><span class="p">:</span><span class="n">keep_probs</span><span class="p">,</span>
                   <span class="s1">&#39;beta1&#39;</span><span class="p">:</span><span class="mf">0.9</span><span class="p">,</span>
                   <span class="s1">&#39;beta2&#39;</span><span class="p">:</span><span class="mf">0.999</span><span class="p">,</span>
                   <span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">1e-8</span>
                  <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">hyperParams</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># Forward Propagation</span>
<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## forward sum</span>
<div class="viewcode-block" id="forward_sum"><a class="viewcode-back" href="../ffnn.html#ffnn.forward_sum">[docs]</a><span class="k">def</span> <span class="nf">forward_sum</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(1)</span>
<span class="sd">            &gt;&gt;&gt; A = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; W = np.random.randn(1,3)</span>
<span class="sd">            &gt;&gt;&gt; b = np.random.randn(1,1)</span>
<span class="sd">            &gt;&gt;&gt; Z, c = forward_sum(A,W,b)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Z = &quot;+ str(Z))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                Z = [[ 3.26295337 -1.23429987]]</span>
<span class="sd">        </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">A_prev</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">A_prev</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">m</span><span class="p">)),</span> <span class="s2">&quot;Dimention of Z mismatched in forward_prop function&quot;</span>
    
    <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cache</span></div>

<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## forward Activation</span>
<div class="viewcode-block" id="forward_activation"><a class="viewcode-back" href="../ffnn.html#ffnn.forward_activation">[docs]</a><span class="k">def</span> <span class="nf">forward_activation</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">activation</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(1)</span>
<span class="sd">            &gt;&gt;&gt; A_prev = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; W = np.random.randn(1,3)</span>
<span class="sd">            &gt;&gt;&gt; b = np.random.randn(1,1)</span>

<span class="sd">            &gt;&gt;&gt; A,c = forward_activation(A_prev,W,b,activation = &#39;relu&#39;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;A with Relu = &quot; + str(A))</span>

<span class="sd">            &gt;&gt;&gt; A,c = forward_activation(A_prev,W,b,activation = &#39;softmax&#39;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;A with Softmax = &quot; + str(A))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                A with Relu = [[3.26295337 0.        ]]</span>
<span class="sd">                A with Softmax = [[1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
        <span class="n">Z</span><span class="p">,</span> <span class="n">sum_cache</span> <span class="o">=</span> <span class="n">forward_sum</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;softmax&#39;</span><span class="p">:</span>
        <span class="n">Z</span><span class="p">,</span> <span class="n">sum_cache</span> <span class="o">=</span> <span class="n">forward_sum</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
<span class="c1">#         Z, sum_cache = forward_sum(A_prev,W,b)</span>
<span class="c1">#         A, activation_cache = tanh(Z)</span>
        <span class="k">pass</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">sum_cache</span><span class="p">,</span><span class="n">activation_cache</span><span class="p">)</span>
    
    <span class="k">assert</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;Dimention of A mismatched in forward_activation function&quot;</span>
    
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">cache</span></div>

<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1"># dropout for individual layer</span>
<div class="viewcode-block" id="forward_dropout"><a class="viewcode-back" href="../ffnn.html#ffnn.forward_dropout">[docs]</a><span class="k">def</span> <span class="nf">forward_dropout</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">keep_probs</span><span class="p">):</span>
     <span class="c1">#implementing dropout</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">D</span> <span class="o">&lt;</span> <span class="n">keep_probs</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">keep_probs</span><span class="p">)</span>
    
    <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">D</span>
    
    <span class="k">assert</span> <span class="p">(</span><span class="n">dropout_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;Dimention of dropout_mask mismatched in forward_dropout function&quot;</span>
    
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span><span class="n">dropout_mask</span></div>

<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## forward prop for L layers</span>
<div class="viewcode-block" id="forward_prop"><a class="viewcode-back" href="../ffnn.html#ffnn.forward_prop">[docs]</a><span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_probs</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(1)</span>
<span class="sd">            &gt;&gt;&gt; X = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; W1 = np.random.randn(3,3)</span>
<span class="sd">            &gt;&gt;&gt; b1 = np.random.randn(3,1)</span>
<span class="sd">            &gt;&gt;&gt; W2 = np.random.randn(2,3)</span>
<span class="sd">            &gt;&gt;&gt; b2 = np.random.randn(2,1)</span>
<span class="sd">            &gt;&gt;&gt; parameters = {&quot;W1&quot;: W1,</span>
<span class="sd">                              &quot;b1&quot;: b1,</span>
<span class="sd">                              &quot;W2&quot;: W2,</span>
<span class="sd">                              &quot;b2&quot;: b2}</span>
<span class="sd">            &gt;&gt;&gt; AL, caches, _ = forward_prop(X, parameters)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;AL without dropout = &quot; + str(AL))</span>

<span class="sd">            &gt;&gt;&gt; AL, caches, _ = forward_prop(X, parameters,keep_probs = [0.9], regularizer = &quot;dropout&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;\nAL with dropout = &quot; + str(AL))</span>

<span class="sd">            &gt;&gt;&gt; print(&quot;\nLength of caches list = &quot; + str(len(caches)))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                AL without dropout = [[0.25442549 0.64096177]</span>
<span class="sd">                 [0.74557451 0.35903823]]</span>

<span class="sd">                AL with dropout = [[0.20251119 0.61487938]</span>
<span class="sd">                 [0.79748881 0.38512062]]</span>

<span class="sd">                Length of caches list = 2</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">caches</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">num_class</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">dropout_masks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># len(keep_probs) == L-1: no dropouts in the Output layer, no dropout at all for prediction</span>
    <span class="k">if</span> <span class="n">regularizer</span> <span class="o">==</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">keep_probs</span><span class="p">)</span> <span class="o">==</span> <span class="n">L</span><span class="o">-</span><span class="mi">1</span> <span class="p">)</span> 
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="n">A_prev</span> <span class="o">=</span> <span class="n">A</span> 
        <span class="n">A</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_activation</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)],</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">regularizer</span> <span class="o">==</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span>
            <span class="n">A</span> <span class="p">,</span> <span class="n">dropout_mask</span> <span class="o">=</span> <span class="n">forward_dropout</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">keep_probs</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">dropout_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dropout_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="n">AL</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_activation</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)],</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)],</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
    <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
    
    <span class="k">assert</span><span class="p">(</span><span class="n">AL</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_class</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="s2">&quot;Dimention of AL mismatched in forward_prop function&quot;</span>
    
    <span class="k">return</span> <span class="n">AL</span><span class="p">,</span><span class="n">caches</span><span class="p">,</span><span class="n">dropout_masks</span></div>
    
    
<span class="c1">#====================================================================================================================</span>
<span class="c1"># compute Cross entropy cost</span>
<div class="viewcode-block" id="softmax_cross_entropy_cost"><a class="viewcode-back" href="../ffnn.html#ffnn.softmax_cross_entropy_cost">[docs]</a><span class="k">def</span> <span class="nf">softmax_cross_entropy_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; AL = np.array([[4.21200131e-01, 1.55876995e-04],</span>
<span class="sd">                           [6.91917292e-02, 1.18118501e-05],</span>
<span class="sd">                           [5.09608140e-01, 9.99832311e-01]])</span>
<span class="sd">            &gt;&gt;&gt; cost = softmax_cross_entropy_cost(AL, Y, caches)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Cost without l2 = &quot; + str(cost))</span>

<span class="sd">            &gt;&gt;&gt; cost = softmax_cross_entropy_cost(AL, Y, caches, lambd = 0.7, regularizer = &#39;l2&#39;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Cost with l2 = &quot; + str(cost))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                Cost without l2 = 0.6742809046007259</span>
<span class="sd">                Cost with l2 = 8.875542970361</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">AL</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="c1"># add very small number 1e-8 to avoid log(0)</span>

    <span class="k">if</span> <span class="n">regularizer</span> <span class="o">==</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
            <span class="n">current_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">sum_cache</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">current_cache</span>
            <span class="n">_</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">sum_cache</span>
            <span class="n">norm</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>

        <span class="n">L2_cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambd</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">norm</span> 
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span> <span class="o">+</span> <span class="n">L2_cost</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">pass</span>
    
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>      <span class="c1"># Making sure your cost&#39;s shape is not returned as ndarray</span>
    
    <span class="k">assert</span><span class="p">(</span><span class="n">cost</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">()),</span><span class="s2">&quot;Dimention of cost mismatched in softmax_cross_entropy_cost function&quot;</span>
    
    <span class="k">return</span> <span class="n">cost</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># Back Propagation</span>
<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## calculating backward gradient</span>
<div class="viewcode-block" id="backward_grad"><a class="viewcode-back" href="../ffnn.html#ffnn.backward_grad">[docs]</a><span class="k">def</span> <span class="nf">backward_grad</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(1)</span>
<span class="sd">            &gt;&gt;&gt; dZ = np.random.randn(3,4)</span>
<span class="sd">            &gt;&gt;&gt; A = np.random.randn(5,4)</span>
<span class="sd">            &gt;&gt;&gt; W = np.random.randn(3,5)</span>
<span class="sd">            &gt;&gt;&gt; b = np.random.randn(3,1)</span>
<span class="sd">            &gt;&gt;&gt; cache = (A, W, b)</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; dA_prev, dW, db = backward_grad(dZ, cache, lambd=0, regularizer=None)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;Without L2 Regularization&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dA_prev = &quot;+ str(dA_prev))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dW = &quot; + str(dW))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;db = &quot; + str(db))</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; l2_dA_prev, l2_dW, l2_db = backward_grad(dZ, cache, lambd = 0.9, regularizer = &#39;l2&#39;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;\nWith L2 Regularization&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dA_prev = &quot;+ str(l2_dA_prev))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dW = &quot; + str(l2_dW))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;db = &quot; + str(l2_db))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                Without L2 Regularization</span>
<span class="sd">                dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]</span>
<span class="sd">                           [ 0.60345879 -3.72508701  5.81700741 -3.84326836]</span>
<span class="sd">                           [-0.4319552  -1.30987417  1.72354705  0.05070578]</span>
<span class="sd">                           [-0.38981415  0.60811244 -1.25938424  1.47191593]</span>
<span class="sd">                           [-2.52214926  2.67882552 -0.67947465  1.48119548]]</span>
<span class="sd">                dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]</span>
<span class="sd">                      [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]</span>
<span class="sd">                      [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]</span>
<span class="sd">                db = [[-0.14713786]</span>
<span class="sd">                      [-0.11313155]</span>
<span class="sd">                      [-0.13209101]]</span>

<span class="sd">                With L2 Regularization</span>
<span class="sd">                dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]</span>
<span class="sd">                           [ 0.60345879 -3.72508701  5.81700741 -3.84326836]</span>
<span class="sd">                           [-0.4319552  -1.30987417  1.72354705  0.05070578]</span>
<span class="sd">                           [-0.38981415  0.60811244 -1.25938424  1.47191593]</span>
<span class="sd">                           [-2.52214926  2.67882552 -0.67947465  1.48119548]]</span>
<span class="sd">                dW = [[-0.0814752  -0.28784277 -1.02688866  0.73478408 -0.24353767]</span>
<span class="sd">                      [ 0.90783172  0.74875962 -0.43216662  0.6696189  -0.78903459]</span>
<span class="sd">                      [ 0.81102242  0.13703735 -0.07696496  0.4081879  -0.05995309]]</span>
<span class="sd">                db = [[-0.14713786]</span>
<span class="sd">                      [-0.11313155]</span>
<span class="sd">                      [-0.13209101]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">regularizer</span> <span class="o">==</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span><span class="n">A_prev</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">lambd</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="n">W</span> <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span><span class="n">A_prev</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span> <span class="p">)</span>
    <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ</span><span class="p">)</span>

    
    <span class="k">assert</span> <span class="p">(</span><span class="n">dW</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;Dimention of dW mismatched in backward_grad function&quot;</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">db</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;Dimention of db mismatched in backward_grad function&quot;</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">dA_prev</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;Dimention of dA_prev mismatched in backward_grad function&quot;</span>
    
    
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span></div>

<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## calculating backward activation</span>
<div class="viewcode-block" id="backward_activation"><a class="viewcode-back" href="../ffnn.html#ffnn.backward_activation">[docs]</a><span class="k">def</span> <span class="nf">backward_activation</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">lambd</span> <span class="p">,</span><span class="n">regularizer</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(2)</span>
<span class="sd">            &gt;&gt;&gt; dA = np.random.randn(1,2)</span>
<span class="sd">            &gt;&gt;&gt; A = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; W = np.random.randn(1,3)</span>
<span class="sd">            &gt;&gt;&gt; b = np.random.randn(1,1)</span>
<span class="sd">            &gt;&gt;&gt; Z = np.random.randn(1,2)</span>
<span class="sd">            &gt;&gt;&gt; sum_cache = (A, W, b)</span>
<span class="sd">            &gt;&gt;&gt; activation_cache = Z</span>
<span class="sd">            &gt;&gt;&gt; cache = (sum_cache, activation_cache)</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; dA_prev, dW, db = backward_activation(dA, cache, lambd = 0 ,regularizer = None, activation = &quot;relu&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;With Relu&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dA_prev = &quot;+ str(dA_prev))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dW = &quot; + str(dW))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;db = &quot; + str(db))</span>
<span class="sd">            </span>
<span class="sd">            &gt;&gt;&gt; dA_prev, dW, db = backward_activation(dA, cache, lambd = 0 ,regularizer = None, activation = &quot;softmax&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;\nWith Softmax&quot;)</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dA_prev = &quot;+ str(dA_prev))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;dW = &quot; + str(dW))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;db = &quot; + str(db))</span>
<span class="sd">            </span>
<span class="sd">            Output: </span>
<span class="sd">                With Relu</span>
<span class="sd">                dA_prev = [[ 0.44090989 -0.        ]</span>
<span class="sd">                           [ 0.37883606 -0.        ]</span>
<span class="sd">                           [-0.2298228   0.        ]]</span>
<span class="sd">                dW = [[ 0.44513824  0.37371418 -0.10478989]]</span>
<span class="sd">                db = [[-0.20837892]]</span>
<span class="sd">            </span>
<span class="sd">                With Softmax</span>
<span class="sd">                dA_prev = [[ 0.44090989  0.05952761]</span>
<span class="sd">                           [ 0.37883606  0.05114697]</span>
<span class="sd">                           [-0.2298228  -0.03102857]]</span>
<span class="sd">                dW = [[ 0.39899183  0.3973954  -0.06975568]]</span>
<span class="sd">                db = [[-0.23651234]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    
    
    <span class="n">sum_cache</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
        <span class="n">dZ</span> <span class="o">=</span> <span class="n">relu_grad</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span><span class="n">activation_cache</span><span class="p">)</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">backward_grad</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">sum_cache</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;softmax&quot;</span><span class="p">:</span>
        <span class="n">dZ</span> <span class="o">=</span> <span class="n">dA</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">backward_grad</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">sum_cache</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">)</span>
    
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
        <span class="k">pass</span>
<span class="c1">#         dZ = tanh_grad(dA,activation_cache)</span>
<span class="c1">#         dA_prev, dW, db = backward_grad(dZ, sum_cache, lambd, regularizer = regularizer)</span>
    
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span></div>
    
    
<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1"># implementing backward dropout</span>
<div class="viewcode-block" id="backward_dropout"><a class="viewcode-back" href="../ffnn.html#ffnn.backward_dropout">[docs]</a><span class="k">def</span> <span class="nf">backward_dropout</span><span class="p">(</span><span class="n">dA_prev_temp</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
    <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA_prev_temp</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
    <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">dA_prev</span><span class="p">,</span><span class="n">keep_prob</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">dA_prev</span></div>

<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1"># back prop foL layers</span>
<div class="viewcode-block" id="backward_prop"><a class="viewcode-back" href="../ffnn.html#ffnn.backward_prop">[docs]</a><span class="k">def</span> <span class="nf">backward_prop</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">dropout_masks</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">keep_probs</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">lambd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(3)</span>
<span class="sd">            &gt;&gt;&gt; AL = np.random.randn(1, 2)</span>
<span class="sd">            &gt;&gt;&gt; Y = np.array([[1, 0]])</span>

<span class="sd">            &gt;&gt;&gt; A1 = np.random.randn(4,2)</span>
<span class="sd">            &gt;&gt;&gt; W1 = np.random.randn(3,4)</span>
<span class="sd">            &gt;&gt;&gt; b1 = np.random.randn(3,1)</span>
<span class="sd">            &gt;&gt;&gt; Z1 = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; cache_activation_1 = ((A1, W1, b1), Z1)</span>

<span class="sd">            &gt;&gt;&gt; A2 = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; W2 = np.random.randn(1,3)</span>
<span class="sd">            &gt;&gt;&gt; b2 = np.random.randn(1,1)</span>
<span class="sd">            &gt;&gt;&gt; Z2 = np.random.randn(1,2)</span>
<span class="sd">            &gt;&gt;&gt; cache_activation_2 = ((A2, W2, b2), Z2)</span>

<span class="sd">            &gt;&gt;&gt; caches = (cache_activation_1, cache_activation_2)</span>

<span class="sd">            &gt;&gt;&gt; grads = backward_prop(AL, Y, caches)</span>
<span class="sd">            &gt;&gt;&gt; for key,value in grads.items():</span>
<span class="sd">            ...     print(str(key)+&quot; : &quot;+str(value))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                dA1 : [[-0.80745758 -0.44693186]</span>
<span class="sd">                       [ 0.88640102  0.49062745]</span>
<span class="sd">                       [-0.10403132 -0.05758186]]</span>
<span class="sd">                dW2 : [[ 0.50767257 -0.42243102 -1.15550109]]</span>
<span class="sd">                db2 : [[0.61256916]]</span>
<span class="sd">                dA0 : [[ 0.          0.53064147]</span>
<span class="sd">                       [ 0.         -0.3319644 ]</span>
<span class="sd">                       [ 0.         -0.32565192]</span>
<span class="sd">                       [ 0.         -0.75222096]]</span>
<span class="sd">                dW1 : [[0.41642713 0.07927654 0.14011329 0.10664197]</span>
<span class="sd">                       [0.         0.         0.         0.        ]</span>
<span class="sd">                       [0.05365169 0.01021384 0.01805193 0.01373955]]</span>
<span class="sd">                db1 : [[-0.22346593]</span>
<span class="sd">                       [ 0.        ]</span>
<span class="sd">                       [-0.02879093]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)</span> <span class="c1"># the number of layers</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">AL</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">AL</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># after this line, Y is the same shape as AL</span>
    
    <span class="n">dA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">current_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)],</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">backward_activation</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span><span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
        <span class="n">current_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">regularizer</span> <span class="o">==</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span>
            <span class="c1">#implementing dropout</span>
            <span class="n">D</span> <span class="o">=</span> <span class="n">dropout_masks</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">dA_prev_temp</span> <span class="o">=</span> <span class="n">backward_dropout</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">D</span><span class="p">,</span> <span class="n">keep_probs</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
            <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW_temp</span><span class="p">,</span> <span class="n">db_temp</span> <span class="o">=</span> <span class="n">backward_activation</span><span class="p">(</span><span class="n">dA_prev_temp</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW_temp</span><span class="p">,</span> <span class="n">db_temp</span> <span class="o">=</span> <span class="n">backward_activation</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
            
        
        <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dA_prev</span>
        <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dW_temp</span>
        <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db_temp</span>

    <span class="k">return</span> <span class="n">grads</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># Update Parameters</span>
<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## Initializing Adam</span>
<div class="viewcode-block" id="initialize_adam"><a class="viewcode-back" href="../ffnn.html#ffnn.initialize_adam">[docs]</a><span class="k">def</span> <span class="nf">initialize_adam</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="p">:</span>
   
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> 
    <span class="n">v</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">v</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">v</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">v</span><span class="p">,</span> <span class="n">s</span></div>
<span class="c1">#-------------------------------------------------------------------------------------------------------------------</span>
<span class="c1">## update Parameters</span>
<div class="viewcode-block" id="update_parameters"><a class="viewcode-back" href="../ffnn.html#ffnn.update_parameters">[docs]</a><span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="s2">&quot;bgd&quot;</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>  <span class="n">epsilon</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">s</span> <span class="o">=</span> <span class="p">{},</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(2)</span>
<span class="sd">            &gt;&gt;&gt; W1 = np.random.randn(3,4)</span>
<span class="sd">            &gt;&gt;&gt; b1 = np.random.randn(3,1)</span>
<span class="sd">            &gt;&gt;&gt; W2 = np.random.randn(1,3)</span>
<span class="sd">            &gt;&gt;&gt; b2 = np.random.randn(1,1)</span>
<span class="sd">            &gt;&gt;&gt; parameters = {&quot;W1&quot;: W1,</span>
<span class="sd">                          &quot;b1&quot;: b1,</span>
<span class="sd">                          &quot;W2&quot;: W2,</span>
<span class="sd">                          &quot;b2&quot;: b2}</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(3)</span>
<span class="sd">            &gt;&gt;&gt; dW1 = np.random.randn(3,4)</span>
<span class="sd">            &gt;&gt;&gt; db1 = np.random.randn(3,1)</span>
<span class="sd">            &gt;&gt;&gt; dW2 = np.random.randn(1,3)</span>
<span class="sd">            &gt;&gt;&gt; db2 = np.random.randn(1,1)</span>
<span class="sd">            &gt;&gt;&gt; grads = {&quot;dW1&quot;: dW1,</span>
<span class="sd">                     &quot;db1&quot;: db1,</span>
<span class="sd">                     &quot;dW2&quot;: dW2,</span>
<span class="sd">                     &quot;db2&quot;: db2}</span>

<span class="sd">            &gt;&gt;&gt; parameters,_,_ = update_parameters(parameters, grads, 0.1)</span>

<span class="sd">            &gt;&gt;&gt; print (&quot;W1 = &quot;+ str(parameters[&quot;W1&quot;]))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;b1 = &quot;+ str(parameters[&quot;b1&quot;]))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;W2 = &quot;+ str(parameters[&quot;W2&quot;]))</span>
<span class="sd">            &gt;&gt;&gt; print (&quot;b2 = &quot;+ str(parameters[&quot;b2&quot;]))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]</span>
<span class="sd">                      [-1.76569676 -0.80627147  0.51115557 -1.18258802]</span>
<span class="sd">                      [-1.0535704  -0.86128581  0.68284052  2.20374577]]</span>
<span class="sd">                b1 = [[-0.04659241]</span>
<span class="sd">                      [-1.28888275]</span>
<span class="sd">                      [ 0.53405496]]</span>
<span class="sd">                W2 = [[-0.55569196  0.0354055   1.32964895]]</span>
<span class="sd">                b2 = [[-0.84610769]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>           
    <span class="n">v_corrected</span> <span class="o">=</span> <span class="p">{}</span>                         
    <span class="n">s_corrected</span> <span class="o">=</span> <span class="p">{}</span>                       
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="c1"># Moving average of the gradients.</span>
            <span class="n">v</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">beta1</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
            <span class="n">v</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">beta1</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>

            <span class="c1"># Compute bias-corrected first moment estimate.</span>
            <span class="n">v_corrected</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span><span class="n">t</span><span class="p">)))</span>
            <span class="n">v_corrected</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span><span class="n">t</span><span class="p">)))</span>

            <span class="c1"># Moving average of the squared gradients. </span>
            <span class="n">s</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">beta2</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]))</span>
            <span class="n">s</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">beta2</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]))</span>

            <span class="c1"># Compute bias-corrected second raw moment estimate. </span>
            <span class="n">s_corrected</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">beta2</span><span class="p">,</span><span class="n">t</span><span class="p">)))</span>
            <span class="n">s_corrected</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">beta2</span><span class="p">,</span><span class="n">t</span><span class="p">)))</span>

            <span class="c1"># Update parameters. </span>
            <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>  <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">v_corrected</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_corrected</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span>
            <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>  <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">v_corrected</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_corrected</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
            <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
            
    <span class="k">return</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">s</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1"># Evaluating the model using acc and loss</span>
<div class="viewcode-block" id="evaluate"><a class="viewcode-back" href="../ffnn.html#ffnn.evaluate">[docs]</a><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; np.random.seed(1)</span>
<span class="sd">            &gt;&gt;&gt; X = np.random.randn(3,2)</span>
<span class="sd">            &gt;&gt;&gt; Y = np.array([[1, 0, 0],[0,1,1]]).reshape(3,2)</span>
<span class="sd">            &gt;&gt;&gt; W1 = np.random.randn(5,3)</span>
<span class="sd">            &gt;&gt;&gt; b1 = np.random.randn(5,1)</span>
<span class="sd">            &gt;&gt;&gt; W2 = np.random.randn(3,5)</span>
<span class="sd">            &gt;&gt;&gt; b2 = np.random.randn(3,1)</span>
<span class="sd">            &gt;&gt;&gt; parameters = {&quot;W1&quot;: W1,</span>
<span class="sd">            ...               &quot;b1&quot;: b1,</span>
<span class="sd">            ...               &quot;W2&quot;: W2,</span>
<span class="sd">            ...               &quot;b2&quot;: b2}</span>
<span class="sd">            &gt;&gt;&gt; acc, loss = evaluate(X, Y, parameters)</span>
<span class="sd">            &gt;&gt;&gt; print(&quot;acc = %f | cost = %f&quot;%(acc,loss))</span>
<span class="sd">            </span>
<span class="sd">            Output:</span>
<span class="sd">                acc = 0.500000 | cost = 0.769464</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># predicting output using fordward propogation </span>
    <span class="n">probas</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="c1">#computing loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">softmax_cross_entropy_cost</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">)</span> 
    
    <span class="c1">#deriving the predictrueted labels</span>
    <span class="n">true_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="c1">#deriving the predicted labels</span>
    <span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    
    <span class="c1">#identifing correctly predicted labels</span>
    <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">predicted_labels</span><span class="p">,</span><span class="n">true_labels</span><span class="p">)</span>
    
    <span class="c1">#computing accuracy</span>
    <span class="n">num_correct_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_correct_prediction</span><span class="o">/</span><span class="n">m</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">loss</span></div>
<span class="c1">#====================================================================================================================</span>
<span class="c1"># Final Model Training</span>
<span class="c1"># Final Model Training</span>

<div class="viewcode-block" id="train"><a class="viewcode-back" href="../ffnn.html#ffnn.train">[docs]</a><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">Y_dev</span><span class="p">,</span> <span class="n">layers_dim</span><span class="p">,</span> <span class="n">hyperParams</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;bgd&#39;</span><span class="p">,</span><span class="n">regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># loading the hyper parameters</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span>
    <span class="n">num_epoch</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;num_epoch&#39;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;beta1&#39;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;beta2&#39;</span><span class="p">]</span>
    <span class="n">ep</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;epsilon&#39;</span><span class="p">]</span>
    <span class="n">lambd</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;lambda&#39;</span><span class="p">]</span>
    <span class="n">keep_probs</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;keep_probs&#39;</span><span class="p">]</span>
    
    <span class="c1">#initializing the variables</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># keep track of epoch cost    </span>
    <span class="n">train_accs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># keep track of training accuracy</span>
    <span class="n">val_accs</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># keep track of Validation accuracy</span>
    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># keep track of training loss</span>
    <span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># keep track of Validation loss</span>
    
    <span class="c1">#selecting the minibatch size for each optimizer</span>
    <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
        <span class="n">mini_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;bgd&#39;</span><span class="p">:</span>
        <span class="n">mini_batch_size</span> <span class="o">=</span> <span class="n">m</span>
    <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;mgd&#39;</span> <span class="ow">or</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
        <span class="n">mini_batch_size</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="p">[</span><span class="s1">&#39;mini_batch_size&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Optimizer value out of scope&quot;</span><span class="p">)</span>
        
    <span class="c1">#initializing the model parameters</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">init_parameters</span><span class="p">(</span><span class="n">layers_dim</span><span class="p">,</span> <span class="n">initialization</span><span class="p">)</span>
    
    <span class="c1">#initializing adam parameters, used only when optimizer = &#39;adam&#39;</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">v</span><span class="p">,</span><span class="n">s</span> <span class="o">=</span> <span class="n">initialize_adam</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    
    <span class="c1">#Gradient Descent begins</span>
    <span class="n">train_toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="c1"># for calculating entire training time</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epoch</span><span class="p">):</span>
        <span class="n">seed</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">batch_cost</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_trained</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">batch_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">verbose</span> <span class="o">!=</span><span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Epoch: </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">num_epoch</span><span class="p">))</span>
        
        <span class="n">minibatches</span> <span class="o">=</span> <span class="n">rand_mini_batches</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
        
        
        <span class="k">for</span> <span class="n">minibatch</span> <span class="ow">in</span> <span class="n">minibatches</span><span class="p">:</span>
            <span class="n">batch_toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="c1"># for calculating time of an epoch cycle</span>
            
            <span class="c1">#retriving minibatch of X and Y from training set</span>
            <span class="p">(</span><span class="n">minibatch_X</span><span class="p">,</span> <span class="n">minibatch_Y</span><span class="p">)</span> <span class="o">=</span> <span class="n">minibatch</span>
            
            <span class="c1">#forward Propagation</span>
            <span class="n">AL</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">dropout_masks</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">minibatch_X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_probs</span> <span class="o">=</span> <span class="n">keep_probs</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">)</span>
            
            <span class="c1">#Computing cross entropy cost</span>
            <span class="n">cross_entropy_cost</span> <span class="o">=</span> <span class="n">softmax_cross_entropy_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">minibatch_Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">)</span> <span class="c1">#accumulating the batch costs</span>
            <span class="n">batch_cost</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cross_entropy_cost</span><span class="p">)</span>   
            <span class="c1">#Backward Propagation</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_prop</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">minibatch_Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">dropout_masks</span> <span class="o">=</span> <span class="n">dropout_masks</span><span class="p">,</span> <span class="n">keep_probs</span> <span class="o">=</span> <span class="n">keep_probs</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">,</span> <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">)</span>
                
            <span class="c1">#Updating parameters</span>
            <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">parameters</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="n">b1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">b2</span><span class="p">,</span>  <span class="n">epsilon</span> <span class="o">=</span> <span class="n">ep</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">)</span>
            
            
            <span class="c1">#Verbosity 0: Silent mode</span>
            <span class="c1">#Verbosity 1: progress bar mode</span>
            <span class="c1">#Verbosity 2: visualization mode</span>
            <span class="c1">#Verbosity 3: detailed mode</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="c1">#evaluating the model using training and validation accuracy and loss</span>
                <span class="n">train_acc</span><span class="p">,</span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
                <span class="n">val_acc</span><span class="p">,</span> <span class="n">val_loss</span><span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">X_dev</span><span class="p">,</span> <span class="n">Y_dev</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>  
                <span class="n">train_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
                <span class="n">val_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_acc</span><span class="p">)</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">batch_tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  <span class="c1"># for calculating epoch time</span>
                <span class="n">batch_time</span> <span class="o">+=</span> <span class="p">(</span><span class="n">batch_tic</span> <span class="o">-</span> <span class="n">batch_toc</span><span class="p">)</span>
                
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">batch_trained</span> <span class="o">+=</span> <span class="n">minibatch_Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">batch_trained_per</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_trained</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
                <span class="n">inc</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">batch_trained_per</span><span class="p">)</span><span class="o">//</span><span class="mi">10</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">elif</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2"> [</span><span class="si">%s</span><span class="s2">&gt;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%.0f%%</span><span class="s2">]  - </span><span class="si">%.3f</span><span class="s2">s&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">batch_trained</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="n">inc</span><span class="p">,</span><span class="s1">&#39;.&#39;</span><span class="o">*</span><span class="p">(</span><span class="mi">10</span><span class="o">-</span><span class="n">inc</span><span class="p">),</span> <span class="n">batch_trained_per</span><span class="p">,</span> <span class="n">batch_time</span><span class="p">),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot; - </span><span class="si">%.3f</span><span class="s2">s | loss: </span><span class="si">%.4f</span><span class="s2"> | acc: </span><span class="si">%.4f</span><span class="s2"> | Val loss: </span><span class="si">%.4f</span><span class="s2"> | Val acc: </span><span class="si">%.4f</span><span class="s2"> &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">batch_time</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2"> [</span><span class="si">%s</span><span class="s2">&gt;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%.0f%%</span><span class="s2">] - </span><span class="si">%.3f</span><span class="s2">s | loss: </span><span class="si">%.4f</span><span class="s2"> | acc: </span><span class="si">%.4f</span><span class="s2"> | Val loss: </span><span class="si">%.4f</span><span class="s2"> | Val acc: </span><span class="si">%.4f</span><span class="s2"> &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">batch_trained</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="n">inc</span><span class="p">,</span><span class="s1">&#39;.&#39;</span><span class="o">*</span><span class="p">(</span><span class="mi">10</span><span class="o">-</span><span class="n">inc</span><span class="p">),</span> <span class="n">batch_trained_per</span><span class="p">,</span> <span class="n">batch_time</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>
            
        <span class="n">epoch_cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">batch_cost</span><span class="p">)</span>
        <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_cost</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">train_tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="c1"># for calculating entire training time</span>
        <span class="n">hrs</span><span class="p">,</span> <span class="n">mins</span><span class="p">,</span> <span class="n">secs</span> <span class="p">,</span> <span class="n">ms</span> <span class="o">=</span> <span class="n">convert_time</span><span class="p">((</span><span class="n">train_tic</span> <span class="o">-</span> <span class="n">train_toc</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">*************************** Total Training Time = </span><span class="si">%d</span><span class="s2">hr </span><span class="si">%d</span><span class="s2">mins </span><span class="si">%d</span><span class="s2">secs </span><span class="si">%.2f</span><span class="s2">ms ***************************&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">hrs</span><span class="p">,</span> <span class="n">mins</span><span class="p">,</span> <span class="n">secs</span><span class="p">,</span> <span class="n">ms</span><span class="p">))</span>
    
        <span class="c1">#visualizing the result of the training</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">visualize_training_results</span><span class="p">(</span><span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>    

    <span class="k">return</span> <span class="n">parameters</span></div>

<span class="c1">#====================================================================================================================</span>
<span class="c1">#making Prediction</span>
<span class="c1"># Making Predictions</span>
<div class="viewcode-block" id="predict"><a class="viewcode-back" href="../ffnn.html#ffnn.predict">[docs]</a><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">second_guess</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c1"># Computing the Output predictions. </span>
    <span class="c1"># no keep_probs : no dropout during prediction </span>
    <span class="n">probas</span><span class="p">,</span> <span class="n">caches</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    
    <span class="c1">#getting the number of examples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">probas</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1">#deriving the predicted labels with their probabilities</span>
    <span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">predicted_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    
    <span class="c1">#Computing the second guess</span>
    <span class="k">if</span> <span class="n">second_guess</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">second_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">second_max</span><span class="p">[</span><span class="n">predicted_labels</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1">#zeroing out the first max prediction</span>
        <span class="n">sec_predicted_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">second_max</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="c1">#selecting the second max predicted label</span>
        <span class="n">sec_predicted_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">second_max</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="c1">#selecting the second max prediction</span>

        <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;Second Prediction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">sec_predicted_labels</span><span class="p">,</span> <span class="n">sec_predicted_prob</span><span class="p">]</span>      

    <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;First Prediction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">predicted_labels</span><span class="p">,</span> <span class="n">predicted_prob</span><span class="p">]</span>
    

    <span class="k">return</span> <span class="n">prediction</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">HandWritten Digit Recognizer 1.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Sameer Kattel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.4.
    </div>
  </body>
</html>