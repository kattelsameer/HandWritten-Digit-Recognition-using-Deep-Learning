{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python modules\n",
    "import os # for working with directories\n",
    "import os.path # for working with paths\n",
    "import time\n",
    "import gc\n",
    "\n",
    "#core python modules\n",
    "from PIL import Image # for loading and saving images\n",
    "from scipy import ndimage # for core image processing namely, rotate, blur and shift\n",
    "\n",
    "import numpy as np # for all other maths operations \n",
    "import matplotlib.pyplot as plt # for image visualization\n",
    "import pickle\n",
    "\n",
    "#custom project modules\n",
    "from dataset import get_files \n",
    "from dataset import load_dataset, visualize_dataset, sample_dataset,prep_dataset\n",
    "from ModelUtils import convert_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset form binary data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size_in_per = 100\n",
    "\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig = load_dataset(dataset = \"mnist\", size_in_per = dataset_size_in_per)\n",
    "\n",
    "print(\"Sample Size : %d%%\\n\"%(dataset_size_in_per))\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Dataset Size\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_orig))+\"\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_orig))+\"\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_x_orig, train_y_orig, dataset = \"mnist\", dataset_type = \"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = train_x_orig[1575:1580]\n",
    "lbls = train_y_orig[1575:1580]\n",
    "print(imgs.shape, lbls.shape)\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "axes= fig.subplots(1, imgs.shape[0])\n",
    "for i, img in enumerate(imgs):\n",
    "    axes[i].imshow(img, cmap = \"Greys\")\n",
    "    axes[i].set_title(\"Label: %s\"%lbls[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotating and rotating+zooming\n",
    "\n",
    "def rotate_images(images,labels, save_image = False):\n",
    "    m = labels.shape[0]\n",
    "    \n",
    "    if save_image:\n",
    "        path = \"dataset/mnist_augmented/rotated/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # creating required directories recursively\n",
    "\n",
    "    rotated_images = []\n",
    "    \n",
    "    for i in range(m):    \n",
    "        # randomly selecting angle between the range -45 to -15 and 15 to 45\n",
    "        pos_angle = np.random.randint(low = 0, high = 60)\n",
    "        neg_angle = np.random.randint(low = -60, high = 0)\n",
    "        angle = np.random.choice([pos_angle,neg_angle])\n",
    "\n",
    "        rotated_img = ndimage.rotate(images[i], angle, reshape=False, mode = \"nearest\")\n",
    "        \n",
    "        if(save_image):\n",
    "            img = Image.fromarray((rotated_img * 255).astype(np.uint8))\n",
    "            img.save(path + str(np.squeeze(labels[i]))+\"_rotated_\"+str(i+1)+\".jpg\")\n",
    "            \n",
    "        rotated_images.append(rotated_img)\n",
    "    \n",
    "    return np.asarray(rotated_images), labels\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "axes = fig.subplots(1, imgs.shape[0])\n",
    "\n",
    "rotated_images, labels = rotate_images(imgs,lbls, save_image = False)\n",
    "for i in range(lbls.shape[0]):\n",
    "    axes[i].imshow(rotated_images[i], cmap='Greys')\n",
    "    axes[i].set_title(\"Label: %s\"%labels[i])\n",
    "#     axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blurring using different filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blurring using Gaussian Filter\n",
    "def blur_images(images, labels, filter_mode = \"random\", random_filter = False, save_image = False):\n",
    "    m = labels.shape[0]\n",
    "    \n",
    "    if save_image:\n",
    "        path = \"dataset/mnist_augmented/blurred/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # creating required directories recursively\n",
    "\n",
    "    blurred_images = []\n",
    "\n",
    "    for i in range(m):    \n",
    "        \n",
    "        if random_filter:\n",
    "            filters = ['gaussian', 'maximum', 'minimum', 'median', 'uniform']\n",
    "            filter_mode = np.random.choice(filters)\n",
    "            \n",
    "        if filter_mode == \"gaussian\":\n",
    "            sig = np.random.uniform(low = 0, high = 2.5)\n",
    "            blurred_img = ndimage.gaussian_filter(images[i], sigma=sig,  mode = \"nearest\")\n",
    "\n",
    "        elif filter_mode ==\"maximum\":\n",
    "            s = np.random.uniform(low = 0, high = 4)\n",
    "            blurred_img = ndimage.maximum_filter(images[i], size= s,  mode = \"nearest\")\n",
    "\n",
    "        elif filter_mode == \"minimum\":\n",
    "            s = np.random.uniform(low = 0, high = 4)\n",
    "            blurred_img = ndimage.minimum_filter(images[i], size= s,  mode = \"nearest\")\n",
    "\n",
    "        elif filter_mode == \"median\":\n",
    "            s = np.random.randint(low = 1, high = 6)\n",
    "            blurred_img = ndimage.median_filter(images[i], size= s,  mode = \"nearest\")\n",
    "\n",
    "        elif filter_mode == \"uniform\":\n",
    "            s = np.random.uniform(low = 1, high = 6)\n",
    "            blurred_img = ndimage.uniform_filter(images[i], size= s, mode = \"nearest\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"filter mode should only be 'gaussian', 'maximum', 'minimum', 'median', or 'uniform'\")\n",
    "        \n",
    "        if(save_image):\n",
    "                img = Image.fromarray((blurred_img * 255).astype(np.uint8))\n",
    "                img.save(path + str(np.squeeze(labels[i]))+\"_blurred_\"+filter_mode+\"_\"+str(i+1)+\".jpg\")\n",
    "\n",
    "        blurred_images.append(blurred_img)\n",
    "    \n",
    "    return np.asarray(blurred_images), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "axes = fig.subplots(1, imgs.shape[0])\n",
    "\n",
    "blurred_images, labels = blur_images(imgs,lbls, filter_mode = \"uniform\",random_filter = True, save_image = False)\n",
    "\n",
    "for i in range(lbls.shape[0]):\n",
    "    axes[i].imshow(blurred_images[i], cmap='Greys')\n",
    "    axes[i].set_title(\"Label: %s\"%labels[i])\n",
    "#     axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting image\n",
    "def shift_images(images,labels, shifting =\"both\", save_image = False):\n",
    "    m = labels.shape[0]\n",
    "    \n",
    "    if save_image:\n",
    "        path = \"dataset/mnist_augmented/shifted/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # creating required directories recursively\n",
    "\n",
    "    shifted_images = []\n",
    "    low = -7\n",
    "    high = 7\n",
    "    \n",
    "    for i in range(m):    \n",
    "         # randomly selecting angle between the range -45 to -15 and 15 to 45\n",
    "        \n",
    "        if shifting == \"horizontal\":\n",
    "            xs = 0\n",
    "            ys = np.random.uniform(low, high)\n",
    "            \n",
    "            \n",
    "        elif shifting == \"vertical\":\n",
    "            ys = 0\n",
    "            xs = np.random.uniform(low , high)\n",
    "            \n",
    "        elif shifting == \"both\":\n",
    "            xs = np.random.uniform(low , high)\n",
    "            ys = np.random.uniform(low , high)\n",
    "        else:\n",
    "            raise ValueError(\"Shifting should only be 'horizontal', 'vertical', or 'both'\")\n",
    "            \n",
    "\n",
    "        shifted_img = ndimage.shift(images[i], shift= (xs,ys), mode = \"nearest\")\n",
    "        \n",
    "        if(save_image):\n",
    "            img = Image.fromarray((shifted_img * 255).astype(np.uint8))\n",
    "            img.save(path + str(np.squeeze(labels[i]))+\"_shifted_\"+shifting+\"_\"+str(i+1)+\".jpg\")\n",
    "            \n",
    "        shifted_images.append(shifted_img)\n",
    "    \n",
    "    return np.asarray(shifted_images), labels\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "axes = fig.subplots(1, imgs.shape[0])\n",
    "\n",
    "shifted_images, labels = shift_images(imgs,lbls, shifting =\"both\", save_image = False)\n",
    "\n",
    "for i in range(lbls.shape[0]):\n",
    "    axes[i].imshow(shifted_images[i], cmap='Greys')\n",
    "    axes[i].set_title(\"Label: %s\"%labels[i])\n",
    "#     axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropping image\n",
    "def crop_and_pad_images(images, labels, crop_center = False, save_image = False):\n",
    "   \n",
    "    m = labels.shape[0]\n",
    "    \n",
    "    if save_image:\n",
    "        path = \"dataset/mnist_augmented/cropped_and_padded/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # creating required directories recursively\n",
    "\n",
    "    cropped_images = []\n",
    "    low = 4\n",
    "    high = 10\n",
    "    \n",
    "    for i in range(m):    \n",
    "        lx, ly = images[i].shape\n",
    "        cropped_image = np.zeros((lx,ly))\n",
    "        cropped_image.fill(0) #padding with pixelvalue; 255 for white and 0 for black padding\n",
    "        if crop_center: # this will work as zoom and crop\n",
    "            # randomly selecting  the range for central cropping\n",
    "            c = np.random.randint(low = 4, high = 10)\n",
    "            cropped_image[lx // c: - lx // c, ly // c: - ly // c] = np.copy(images[i][lx // c: - lx // c, ly // c: - ly // c])\n",
    "        else: \n",
    "            # randomly selecting  the range for cropping across different axis\n",
    "            clx = np.random.randint(low, high )\n",
    "            crx= np.random.randint(low, high )\n",
    "            cly = np.random.randint(low, high )\n",
    "            cry = np.random.randint(low, high )\n",
    "            cropped_image[lx // clx: - lx // crx, ly // cly: - ly // cry] = np.copy(images[i][lx // clx: - lx // crx, ly // cly: - ly // cry])\n",
    "\n",
    "        if(save_image):\n",
    "            img = Image.fromarray((cropped_image * 255).astype(np.uint8))\n",
    "            img.save(path + str(np.squeeze(labels[i]))+\"_cropped_\"+str(i+1)+\".jpg\")\n",
    "            \n",
    "        cropped_images.append(cropped_image)\n",
    "    \n",
    "    return np.asarray(cropped_images), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "axes = fig.subplots(1, imgs.shape[0])\n",
    "\n",
    "cropped_images, labels = crop_and_pad_images(imgs,lbls,crop_center = False, save_image = False)\n",
    "\n",
    "for i in range(lbls.shape[0]):\n",
    "    axes[i].imshow(cropped_images[i], cmap='Greys')\n",
    "    axes[i].set_title(\"Label: %s\"%labels[i])\n",
    "#     axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping image\n",
    "\n",
    "\n",
    "def horizontal_flip_images(images,labels, save_image = False):\n",
    "    m = labels.shape[0]\n",
    "    \n",
    "    if save_image:\n",
    "        path = \"dataset/mnist_augmented/h_flipped/\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)  # creating required directories recursively\n",
    "\n",
    "    flipped_images = []\n",
    "    \n",
    "    for i in range(m):    \n",
    "        flip_h = np.fliplr(images[i])\n",
    "\n",
    "        if(save_image):\n",
    "#             https://stackoverflow.com/questions/55319949/pil-typeerror-cannot-handle-this-data-type\n",
    "            img = Image.fromarray((flip_h * 255).astype(np.uint8))\n",
    "            img.save(path + str(np.squeeze(labels[i]))+\"_hFlipped_\"+str(i+1)+\".jpg\")\n",
    "            \n",
    "        flipped_images.append(flip_h)\n",
    "    \n",
    "    return np.asarray(flipped_images), labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "axes = fig.subplots(1, imgs.shape[0])\n",
    "\n",
    "flipped_images, labels = horizontal_flip_images(imgs,lbls,save_image = False)\n",
    "\n",
    "for i in range(lbls.shape[0]):\n",
    "    axes[i].imshow(flipped_images[i], cmap='Greys')\n",
    "    axes[i].set_title(\"Label: %s\"%labels[i])\n",
    "#     axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulating all the Geometric Transformation in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_img(images_orig, labels, horizontal_flip = False, crop_and_pad = False, rotate = False, shift = False, blur = False, save_images = False, include_original = False):\n",
    "    \n",
    "    m = labels.shape[0]\n",
    "    \n",
    "    augmented_images = np.copy(images_orig)\n",
    "    augmented_labels = np.copy(labels)\n",
    "\n",
    "    #horizontal flipping\n",
    "    if horizontal_flip:\n",
    "        flipped_images, flipped_labels = horizontal_flip_images(images_orig, labels, save_image = save_images)\n",
    "        augmented_images = np.concatenate((augmented_images, flipped_images), axis = 0)\n",
    "        augmented_labels = np.concatenate((augmented_labels, flipped_labels), axis = 0)\n",
    "        flipped_images, flipped_labels = 0,0\n",
    "    #random cropping and padding\n",
    "    if crop_and_pad:\n",
    "        cropped_images, cropped_labels = crop_and_pad_images(images_orig, labels, save_image = save_images)\n",
    "        augmented_images = np.concatenate((augmented_images, cropped_images), axis = 0)\n",
    "        augmented_labels = np.concatenate((augmented_labels, cropped_labels), axis = 0)\n",
    "        cropped_images, cropped_labels = 0,0\n",
    "    #random rotating\n",
    "    if rotate:\n",
    "        rotated_images, rotated_labels = rotate_images(images_orig, labels, save_image = save_images)\n",
    "        augmented_images = np.concatenate((augmented_images, rotated_images), axis = 0)\n",
    "        augmented_labels = np.concatenate((augmented_labels, rotated_labels), axis = 0)\n",
    "        rotated_images, rotated_labels = 0,0\n",
    "    #random shifting\n",
    "    if shift:\n",
    "        shifted_images, shifted_labels = shift_images(images_orig,labels, shifting =\"both\", save_image = save_images)\n",
    "        augmented_images = np.concatenate((augmented_images, shifted_images), axis = 0)\n",
    "        augmented_labels = np.concatenate((augmented_labels, shifted_labels), axis = 0)\n",
    "        shifted_images, shifted_labels = 0,0\n",
    "    #random blurring\n",
    "    if blur:\n",
    "        blurred_images, blurred_labels = blur_images(images_orig, labels, random_filter = True, save_image =save_images)\n",
    "        augmented_images = np.concatenate((augmented_images, blurred_images), axis = 0)\n",
    "        augmented_labels = np.concatenate((augmented_labels, blurred_labels), axis = 0)\n",
    "        blurred_images, blurred_labels = 0,0\n",
    "    \n",
    "    #suffeling all the images\n",
    "    augmented_images, augmented_labels = sample_dataset(augmented_images, augmented_labels, size_in_per = 100)\n",
    "    \n",
    "     \n",
    "    if include_original:\n",
    "        return augmented_images, augmented_labels\n",
    "    else:\n",
    "        return augmented_images[m::], augmented_labels[m:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images, augmented_labels = augment_img(train_x_orig, train_y_orig, \n",
    "                                                 horizontal_flip = False, \n",
    "                                                 crop_and_pad = True, \n",
    "                                                 rotate = True, shift = True, \n",
    "                                                 blur = True, \n",
    "                                                 save_images = True, \n",
    "                                                 include_original = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augmented_images.shape, augmented_labels.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "axes = fig.subplots(1, 5)\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].imshow(augmented_images[i], cmap='gray')\n",
    "    axes[i].set_title(\"Label: %s\"%augmented_labels[i])\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n",
    "\n",
    "# visualize_dataset(augmented_images, augmented_labels, dataset = \"mnist\", dataset_type = \"training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating large volume of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(X, Y, minibatch_size=64, seed=1):\n",
    "  \n",
    "    np.random.seed(seed)  # varying the seed value so that the minibatchs become random in each epoch\n",
    "    m = Y.shape[0]  # number of training examples\n",
    "    minibatches = []\n",
    "\n",
    "    # Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Partition (shuffled_X, shuffled_Y) except for the last batch\n",
    "    num_complete_minibatches = np.floor(m / minibatch_size).astype(int)  # number of mini batches of size minibatch_size\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[k * minibatch_size: (k + 1) * minibatch_size, :, :]\n",
    "        minibatch_Y = shuffled_Y[k * minibatch_size: (k + 1) * minibatch_size, :]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "\n",
    "    # Last batch (last minibatch <= minibatch_size)\n",
    "    if m % minibatch_size != 0:\n",
    "        minibatch_X = shuffled_X[num_complete_minibatches * minibatch_size: m, :, :]\n",
    "        minibatch_Y = shuffled_Y[num_complete_minibatches * minibatch_size: m, :]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "    del shuffled_X, shuffled_Y\n",
    "    return minibatches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readable_datasize(orig_size):\n",
    "    size = 0\n",
    "    if orig_size >= 1e6:\n",
    "        size = int(orig_size//1e6) if orig_size % 1e6 == 0 else orig_size/1e6\n",
    "        readable_size = str(size)+\"M\"\n",
    "    elif orig_size >=1e3:\n",
    "        size = int(orig_size//1e3) if orig_size % 1e3 == 0 else orig_size/1e3\n",
    "        readable_size = str(size)+\"K\"\n",
    "    else:\n",
    "        readable_size = str(orig_size)\n",
    "    \n",
    "    return readable_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_data(path, batch_no, data):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)  # creating required directories recursively\n",
    "\n",
    "    filename = \"batch_\" + str(batch_no)\n",
    "    with open(path + filename, 'wb') as output_file:\n",
    "        pickle.dump(data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X_orig, Y_orig, batch_size = 64, aug_count = 1, verbose = 0, pre_process_data = False):\n",
    "    #initializing the variables\n",
    "    seed = 1\n",
    "    path = \"dataset/augmented_data/\"\n",
    "    aug_tic = time.time() # for calculating entire augmentation time\n",
    "\n",
    "    print(\"Generating %s Augmented images...\"%(get_readable_datasize(X_orig.shape[0] * aug_count * 4)))\n",
    "    \n",
    "    for i in range(1, aug_count+1):\n",
    "        seed += 1\n",
    "        time_augmented = 0\n",
    "        batch_times = []\n",
    "        aug_images = np.copy(X_orig[0:1,:,:])\n",
    "        aug_labels = np.copy(Y_orig[0:1,:])\n",
    "      \n",
    "        if verbose > 0:\n",
    "            print(\"\\nAugmentation Count %d/%d\"%(i,aug_count))\n",
    "        \n",
    "        minibatches = generate_minibatches(X_orig, Y_orig, batch_size, seed)\n",
    "        total_minibatches = len(minibatches)\n",
    "        \n",
    "        for ind, minibatch in enumerate(minibatches):\n",
    "            batch_toc = time.time() # for calculating time of an epoch cycle\n",
    "            \n",
    "            #retriving minibatch of X and Y from training set\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            aug_images_batch, aug_labels_batch = augment_img(minibatch_X, minibatch_Y,\n",
    "                                                             crop_and_pad = True,\n",
    "                                                             rotate = True,\n",
    "                                                             shift = True,\n",
    "                                                             blur = True)\n",
    "            \n",
    "            aug_images = np.concatenate((aug_images, aug_images_batch), axis = 0)\n",
    "            aug_labels = np.concatenate((aug_labels, aug_labels_batch), axis = 0)\n",
    "            \n",
    "            \n",
    "            if verbose > 1:\n",
    "            # Calculating Augmentation time for each batch \n",
    "                batch_tic = time.time()\n",
    "                batch_times.append(batch_tic - batch_toc)\n",
    "                time_augmented = np.sum(batch_times)\n",
    "            \n",
    "                #calculating Augmentation progress\n",
    "                per = ((ind+1) / total_minibatches) * 100\n",
    "                inc = int(per // 10) * 2\n",
    "                           \n",
    "                print (\"%d/%d [%s>%s %.0f%%] - %.2fs\"%(ind+1, total_minibatches, '=' * inc,'.'*(20-inc), per, time_augmented),end='\\r')\n",
    "            \n",
    "        #----------------------------------------------batch ends-------------------------------------------\n",
    "        if pre_process_data:\n",
    "            aug_data = prep_dataset(aug_images[1:], aug_labels[1:], num_class = 10)\n",
    "        else:    \n",
    "            aug_data = (aug_images[1:], aug_labels[1:])\n",
    "\n",
    "        save_generated_data(path, batch_no = i, data = aug_data)\n",
    "        del aug_data,aug_images,aug_labels\n",
    "        \n",
    "        if verbose > 1:\n",
    "            time_per_batch = int(np.mean(batch_times)*1000)\n",
    "            print (\"%d/%d [%s 100%%] - %.2fs %dms/step\"%(total_minibatches, total_minibatches, '=' * 20, time_augmented, time_per_batch ),end='\\r')\n",
    "                \n",
    "    #-------------------------------------------Total Augmentation ends-----------------------------------------------\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose > 1:\n",
    "        #calculating entire Augmentation time\n",
    "        hrs, mins, secs , ms = convert_time((time.time() - aug_tic)*1000)\n",
    "        print(\"\\n\\nTotal Augmentation Time = %dhr %dmins %dsecs %.2fms\"%(hrs, mins, secs, ms))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator(train_x_orig, train_y_orig, batch_size = 2048, aug_count = 2, verbose = 2, pre_process_data = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_augmented_data():\n",
    "    path = \"dataset/augmented_data/\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "            raise ValueError(\"Given folder doesnot exist\")\n",
    "\n",
    "    file_names = get_files(path)\n",
    "    print(file_names)\n",
    "    for ind,file in enumerate(file_names):\n",
    "        fname = path + file\n",
    "        try:\n",
    "            with open(fname, 'rb') as input_file:\n",
    "                image_batch, label_batch = pickle.load(input_file)\n",
    "        except(OSError, IOError) as e:\n",
    "            print(e)\n",
    "\n",
    "        if ind == 0:\n",
    "            aug_images = image_batch\n",
    "            aug_labels = label_batch\n",
    "        else:\n",
    "            aug_images = np.concatenate((aug_images, image_batch), axis = 1)\n",
    "            aug_labels = np.concatenate((aug_labels, label_batch), axis = 1)\n",
    "        \n",
    "        del image_batch, label_batch\n",
    "    return aug_images, aug_labels\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_1', 'batch_2']\n",
      "(784, 480000) (10, 480000)\n"
     ]
    }
   ],
   "source": [
    "path = \"dataset/augmented_data/\"\n",
    "aug_images, aug_labels = load_augmented_data()\n",
    "print(aug_images.shape,aug_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generator(X_orig, Y_orig, batch_size = 64, aug_count = 1, verbose = 0, pre_process_data = False):\n",
    "#     #initializing the variables\n",
    "#     seed = 1\n",
    "#     aug_images = np.copy(X_orig[0:1,:,:])\n",
    "#     aug_labels = np.copy(Y_orig[0:1,:])\n",
    "   \n",
    "#     aug_toc = time.time() # for calculating entire augmentation time\n",
    "\n",
    "#     print(\"Generating %s Augmented images...\"%(get_readable_datasize(X_orig.shape[0] * aug_count * 4)))\n",
    "    \n",
    "#     for i in range(1, aug_count+1):\n",
    "#         seed += 1\n",
    "#         time_augmented = 0\n",
    "#         batch_times = []\n",
    "      \n",
    "#         if verbose > 0:\n",
    "#             print(\"\\nAugmentation Count %d/%d\"%(i,aug_count))\n",
    "        \n",
    "#         minibatches = generate_minibatches(X_orig, Y_orig, batch_size, seed)\n",
    "#         total_minibatches = len(minibatches)\n",
    "        \n",
    "#         for ind, minibatch in enumerate(minibatches):\n",
    "#             batch_toc = time.time() # for calculating time of an epoch cycle\n",
    "            \n",
    "#             #retriving minibatch of X and Y from training set\n",
    "#             (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "#             aug_images_batch, aug_labels_batch = augment_img(minibatch_X, minibatch_Y,\n",
    "#                                                              crop_and_pad = True,\n",
    "#                                                              rotate = True,\n",
    "#                                                              shift = True,\n",
    "#                                                              blur = True,\n",
    "#                                                              save_images = False,\n",
    "#                                                              include_original = False)\n",
    "            \n",
    "#             aug_images = np.concatenate((aug_images, aug_images_batch), axis = 0)\n",
    "#             aug_labels = np.concatenate((aug_labels, aug_labels_batch), axis = 0)\n",
    "            \n",
    "#             if verbose > 1:\n",
    "#             # Calculating Augmentation time for each batch \n",
    "#                 batch_tic = time.time()\n",
    "#                 batch_times.append(batch_tic - batch_toc)\n",
    "#                 time_augmented = np.sum(batch_times)\n",
    "            \n",
    "#                 #calculating Augmentation progress\n",
    "#                 per = ((ind+1) / total_minibatches) * 100\n",
    "#                 inc = int(per // 10) * 2\n",
    "                           \n",
    "#                 print (\"%d/%d [%s>%s %.0f%%] - %.2fs\"%(ind+1, total_minibatches, '=' * inc,'.'*(20-inc), per, time_augmented),end='\\r')\n",
    "            \n",
    "#         #----------------------------------------------batch ends-------------------------------------------\n",
    "       \n",
    "#         if verbose > 1:\n",
    "#             time_per_batch = int(np.mean(batch_times)*1000)\n",
    "#             print (\"%d/%d [%s 100%%] - %.2fs %dms/step\"%(total_minibatches, total_minibatches, '=' * 20, time_augmented, time_per_batch ),end='\\r')\n",
    "                \n",
    "#     #-------------------------------------------Total Augmentation ends-----------------------------------------------\n",
    "#     if verbose > 1:\n",
    "#         aug_tic = time.time() # for calculating entire Augmentation time\n",
    "#         hrs, mins, secs , ms = convert_time((aug_tic - aug_toc)*1000)\n",
    "#         print(\"\\n\\nTotal Augmentation Time = %dhr %dmins %dsecs %.2fms\"%(hrs, mins, secs, ms))\n",
    "\n",
    "#     if pre_process_data:\n",
    "#         return prep_dataset(aug_images[1:], aug_labels[1:], num_class = 10)\n",
    "#     else:    \n",
    "#         return aug_images[1:], aug_labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generating data in offline mode\n",
    "# # to generate data in online mode call this function inside an epoch with pre_process_data = True\n",
    "# augmented_images, augmented_labels = data_generator(train_x_orig, train_y_orig, batch_size = 2048, aug_count = 2, verbose = 2, pre_process_data = False)\n",
    "\n",
    "# print(\"Data\\t\\t\\t\",\"Before Augmentation\\t\",\"After Augmentation\")\n",
    "# print(\"=================================================================\")\n",
    "# print(\"Training Set Images:\\t\" + str(train_x_orig.shape)+\"\\t\\t\"+ str(augmented_images.shape))\n",
    "# print(\"Training Set Labels:\\t\" + str(train_y_orig.shape)+\"\\t\\t\"+ str(augmented_labels.shape))\n",
    "# print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "axes = fig.subplots(1, 5)\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].imshow(augmented_images[i], cmap='gray')\n",
    "    axes[i].set_title(\"Label: %s\"%augmented_labels[i])\n",
    "    axes[i].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images_from_file(path):\n",
    "    #checking for the validity of the path\n",
    "    if not os.path.exists(path):\n",
    "            raise ValueError(\"Given folder doesnot exist\")\n",
    "\n",
    "    image_names = sorted(get_files(path))\n",
    "    images = []\n",
    "    lbls = []\n",
    "    for image_name in image_names:\n",
    "        fname = path + image_name\n",
    "        image_data = np.asarray(Image.open(fname).resize((28,28)).convert('L')).reshape(28,28)\n",
    "        if image_data[1,1] > 250: #if background is white, reversing the fore and background color\n",
    "            image_data = 255 - image_data\n",
    "        images.append(image_data.tolist())\n",
    "        lbls.append(image_name[0])\n",
    "\n",
    "    real_images = np.asarray(images)\n",
    "    labels = np.asarray(lbls)\n",
    "    \n",
    "    return real_images, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = \"Sample Images/\"\n",
    "images, labels = load_images_from_file(path = image_path)\n",
    "# print(images.shape)    \n",
    "\n",
    "\n",
    "imgs = images[5:10]\n",
    "lbls = labels[5:10]\n",
    "\n",
    "print(imgs.shape, lbls.shape)\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "axes= fig.subplots(1, imgs.shape[0])\n",
    "for i, img in enumerate(imgs):\n",
    "    axes[i].imshow(img, cmap = \"gray\")\n",
    "    axes[i].set_title(\"Label: %s\"%lbls[i])\n",
    "print(imgs[1][1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
