{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies used:\n",
    "- Standard Utility Modules (os, struct, time, urllib, gzip)\n",
    "- Core 3rd Party packeges (numpy, matplotlib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Libraries\n",
    "# import os\n",
    "import os.path #for accessing the file path\n",
    "import struct  #for unpacking the binary data\n",
    "\n",
    "import time    #for calculating time\n",
    "\n",
    "from urllib.request import urlopen #for downloading the dataset\n",
    "from urllib.error import URLError, HTTPError #for error handling while downloading data\n",
    "\n",
    "import gzip as unzip #for decompressing the downloaded dataset\n",
    "\n",
    "#core packages\n",
    "import numpy as np #for mathematical calculations\n",
    "import matplotlib.pyplot as plt #for visualization\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MNIST dataset form its official site\n",
    "> http://yann.lecun.com/exdb/mnist/index.html\n",
    "\n",
    "- Download  fashion-MNIST dataset form its official site\n",
    "    >https://github.com/zalandoresearch/fashion-mnist\n",
    "<br>\n",
    "Paper: https://arxiv.org/pdf/1708.07747.pdf \n",
    "\n",
    "Fashion MNIST dataset is not a part of this project. It is, however, used to evaluate the algorithms implemented and as a comparision metric for the performance in MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help from  https://gist.github.com/kissgyorgy/6102803 for bufferring part\n",
    "def download_dataset(dataset, to_path):\n",
    "    \"\"\"Downloads the dataset form the original data source.\n",
    "        \n",
    "        Arguments:\n",
    "            dataset (str): Dataset used, mnist or fashion_mnist.\n",
    "            to_path (str): Path of the directory where the dataset is to be downloaded.\n",
    "        \n",
    "        Example:\n",
    "            >>> download_dataset(dataset = \"fashion_mnist\", to_path = \"dataset/fashion_mnist/\")\n",
    "        \n",
    "        References:\n",
    "            https://gist.github.com/kissgyorgy/6102803 (Buffering Part)\n",
    "    \"\"\"\n",
    "    \n",
    "    #defining the source urls for data download\n",
    "    if dataset == \"mnist\":\n",
    "        urls = [\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "                \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "                \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "                \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"] \n",
    "    \n",
    "    elif dataset == \"fashion_mnist\":\n",
    "        urls = [\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\",\n",
    "                \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\", \n",
    "                \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\",\n",
    "                \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\" ] \n",
    "    else:\n",
    "        raise ValueError(\"Only 'mnist' and 'fashion_mnist' dataset are supported\")\n",
    "    \n",
    "    path = to_path #destination path\n",
    "    \n",
    "    #checking the path for data download and creating the required directories if not found\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Destination directory does not exist: Creating '\" + path + \"' as a new directory.\\n\")\n",
    "        os.makedirs(path) #creating required directories recursively\n",
    "#         if this is to be handled form load_dataset() method the following lines of code should be used instead of the above two lines\n",
    "#         print(\"Directory not found: It should be handled from the load_dataset(parent) module\")\n",
    "#         return\n",
    "    \n",
    "    download_status = \"succeeded\" #initializing download status\n",
    "    \n",
    "    #downloading the data files for each url\n",
    "    for url in urls: \n",
    "        filename = url.split('/')[-1] #getting the filename from the url\n",
    "        \n",
    "        #downloading the file if it does not exist\n",
    "        if not os.path.exists(path + filename):\n",
    "            print(\"\\n%s: downloading...\"%filename)\n",
    "            try:  \n",
    "                \n",
    "                #urlretrive() can also be used to obtain data directly.. this method with buffer is used to show the download progress\n",
    "                data_online = urlopen(url) \n",
    "                \n",
    "                #opening a new file of the same filename and writing the content of the url in the file\n",
    "                with open(path + filename, 'wb') as data_local:\n",
    "                    file_size = int(data_online.info()[\"Content-Length\"]) #getting the file size of the file from the url\n",
    "                    file_size_mb = file_size/(1024*1024) #for linux system use 1000 instead of 1024\n",
    "                    downloaded_file_size = 0\n",
    "                    block_size = 1024 #setting the block size to read the data from the url\n",
    "                    while True:\n",
    "                        buffer = data_online.read(block_size) #bufferring the file content\n",
    "                        if not buffer:\n",
    "                            break #breaking if no data left to read form the url\n",
    "                        downloaded_file_size += len(buffer) #adding up downloaded file size\n",
    "                        downloaded_file_size_mb = downloaded_file_size/(1024*1024) #converting file size to MB from bytes\n",
    "                        data_local.write(buffer) #writing in the opened file in the local machine\n",
    "                        \n",
    "                        #calculating the downloaded percentage of the file\n",
    "                        down_percent = downloaded_file_size * 100. / file_size\n",
    "                        inc = int(down_percent)//10 #calculating the increment of the progress bar\n",
    "                        print (\"%.3f MB  [%.3f MB done %s>%s %.0f%%]\" %( file_size_mb, downloaded_file_size_mb, '=' * inc,'.'*(10-inc), down_percent), end = \"\\r\")\n",
    "            except HTTPError as error_message:\n",
    "                print('Download Failed: ', error_message)\n",
    "                download_status = \"failed\"\n",
    "            except URLError as error_message:\n",
    "                print('Download Failed: ', error_message)\n",
    "                download_status = \"failed\"\n",
    "        else:\n",
    "            print(\"\\n%s: already exists.\"%filename)\n",
    "    print(\"\\n\\nDataset download %s...\\n\"%download_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(dataset =\"fashion_mnist\" , to_path = \"dataset/fashion_mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompressing the gzip Dataset files to the desired binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tutorialspoint.com/working-with-zip-files-in-python\n",
    "# https://www.geeksforgeeks.org/os-walk-python/\n",
    "\n",
    "\n",
    "def is_gzip(filename):\n",
    "    \"\"\"Check if the file si gzip or not.\n",
    "        \n",
    "        Argument:\n",
    "            filename (str): file to be checked.\n",
    "        \n",
    "        Return:\n",
    "            Bool: True if the given file is gzip, false otherwise.\n",
    "        \n",
    "        Example:\n",
    "            >>> is_gzip(filename)\n",
    "    \"\"\"\n",
    "    #checking the extention of the file to determine if it is a gzip file or not\n",
    "    ext = filename.split(\".\")[-1]\n",
    "    if ext == \"gz\":\n",
    "        return True\n",
    "\n",
    "def get_files(path,file_type = \"all\"):\n",
    "    \"\"\"Get all the files of file_type form the given path.\n",
    "        \n",
    "        Arguments:\n",
    "            path (str): location where the files are to be checked.\n",
    "            file_type (str,optional) : type of files to be checked. Defaults to \"all\".\n",
    "       \n",
    "        Return:\n",
    "            files (list): name of the files in the given path\n",
    "        \n",
    "        Example:\n",
    "            >>> files = get_files(path, file_type = \"all\")\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    #accessing all files in the supplied path\n",
    "    for root, dirs, file in os.walk(path):\n",
    "        for fname in file:\n",
    "            if file_type == \"gzip\": #if only gzip files are to be returned\n",
    "                if is_gzip(fname): #checking if the file is gzip\n",
    "                    files.append(fname)\n",
    "            else:\n",
    "                files.append(fname) #if all files are to be returned\n",
    "    return files\n",
    "      \n",
    "\n",
    "    \n",
    "def decompress_dataset(path, keep_original = True):\n",
    "    \"\"\"Decompress the gzip files in the given path.\n",
    "        \n",
    "        Arguments:\n",
    "            path (str): location of the file to be decompressed.\n",
    "            keep_original (bool, optional): Keep the gzip file after decompressionif True, remove otherwise. Defaults to True.\n",
    "        \n",
    "        Example:\n",
    "            >>> decompress_dataset(path = \"dataset/fashion_mnist/\")            \n",
    "    \"\"\"\n",
    "    files = get_files(path, file_type = \"gzip\")\n",
    "    \n",
    "    #checking if there are any gzip file to decompress\n",
    "    if len(files) == 0:\n",
    "        print(\"No gzip file to decompress.\")\n",
    "        return\n",
    "    \n",
    "    for filename in files:\n",
    "        try:\n",
    "            with open(path + filename.split(\".\")[0],'wb') as fp: #opening a file on which the gzip file content is to be written\n",
    "                with unzip.open(path + filename, 'rb') as fzip: #opening the gzip file to be decompressed\n",
    "                    file_data = fzip.read() #reading the gzip file\n",
    "                fp.write(file_data) #writing the read content to another local file\n",
    "            if keep_original == False:\n",
    "                os.remove(path+filename) #removing the gzip file after decompression\n",
    "        except unzip.BadZipFile:\n",
    "            print('Error: Invalid gzip file encountered.')\n",
    "    \n",
    "    print(\"Dataset decompression succeeded...\")\n",
    "    if keep_original == False:\n",
    "        print(\"Original gzip files removed...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gzip file to decompress.\n"
     ]
    }
   ],
   "source": [
    "decompress_dataset(path = \"dataset/fashion_mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriving data from binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_dataset(path, filename):\n",
    "    \"\"\"Retrive mnist or fashion_mnist dataset from  the binary file into numpy arrays.        \n",
    "        \n",
    "        Arguments:\n",
    "            path (str): path where the files are located\n",
    "            filename (str): name of the files that are to be retrived\n",
    "        \n",
    "        Returns:\n",
    "            train_images (numpy.ndarray) : 3D array consisting of m examples of training images \n",
    "            train_labels (numpy.ndarray) : labels for each training images\n",
    "            test_images (numpy.ndarray) : 3D array consisting of m examples of test images \n",
    "            test-labels (numpy.ndarray) : labels for each test images\n",
    "        \n",
    "        Example:\n",
    "            >>> train_x, train_y, test_x, test_y = retrive_dataset(path, filename)\n",
    "            \n",
    "        References:\n",
    "            mnist (str/url): http://yann.lecun.com/exdb/mnist/\n",
    "            fashion_mnist (str/url): https://github.com/zalandoresearch/fashion-mnist/\n",
    "            Code for Retrival(str\\retrival): https://www.cs.virginia.edu/~connelly/class/2015/large_scale/proj2/mnist_python\n",
    "                modified to our need making data retrival 8-10 times faster\n",
    "    \"\"\"\n",
    "    #setting file path based on the dataset\n",
    "    train_img_file_path = path + filename[0]\n",
    "    train_lbl_file_path = path + filename[1]\n",
    "    test_img_file_path = path + filename[2]\n",
    "    test_lbl_file_path = path + filename[3]\n",
    "     \n",
    "    #retriving the training data\n",
    "    with open(train_img_file_path, 'rb') as train_fimg, open(train_lbl_file_path, 'rb') as train_flbl :\n",
    "        #retriving labels\n",
    "        _, size = struct.unpack(\">II\", train_flbl.read(8))\n",
    "        train_labels = np.frombuffer(train_flbl.read(), dtype=np.int8).reshape(size,1)\n",
    "        #retriving images\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", train_fimg.read(16))\n",
    "        train_images = np.frombuffer(train_fimg.read(),dtype=np.uint8).reshape(size, rows, cols)\n",
    "       \n",
    "    #retriving the test data\n",
    "    with open(test_img_file_path, 'rb') as test_fimg, open(test_lbl_file_path, 'rb') as test_flbl :\n",
    "        #retriving labels\n",
    "        _, size = struct.unpack(\">II\", test_flbl.read(8))\n",
    "        test_labels = np.frombuffer(test_flbl.read(), dtype=np.int8).reshape(size,1)\n",
    "        #retriving images\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", test_fimg.read(16))\n",
    "        test_images = np.frombuffer(test_fimg.read(),dtype=np.uint8).reshape(size, rows, cols)\n",
    "     \n",
    "    #validating the shape of the retrived data\n",
    "    assert(train_images.shape == (60000, 28, 28))\n",
    "    assert(train_labels.shape == (60000,1))\n",
    "    assert(test_images.shape == (10000, 28, 28))\n",
    "    assert(test_labels.shape == (10000,1))\n",
    "    \n",
    "  \n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/fashion_mnist/\"\n",
    "    \n",
    "filename = [\"train-images-idx3-ubyte\",\n",
    "            \"train-labels-idx1-ubyte\",\n",
    "            \"t10k-images-idx3-ubyte\",\n",
    "            \"t10k-labels-idx1-ubyte\"] \n",
    "#retriving the data\n",
    "train_x_orig, train_y_orig, test_x_temp, test_y_temp = retrive_dataset(path, filename)\n",
    "\n",
    "#displaying the retrival info\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_temp))+\"\\t\",str(test_x_temp.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_temp))+\"\\t\",str(test_y_temp.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample a portion of the retrived Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriving a small sample of the original dataset for model development and experimentation\n",
    "def sample_dataset(x,y, size_in_per):\n",
    "    \"\"\"Returns a sample dataset from the original larger dataset\n",
    "       \n",
    "        Arguments:\n",
    "            x (numpy.ndarray): original input data\n",
    "            y (numpy.ndarray): original output labels\n",
    "            size_in_per (int): sample volume in percentage\n",
    "        \n",
    "        Returns:\n",
    "            x_sample (numpy.ndarray): input sample  from original dataset of size (sample_size% of x)\n",
    "            y_sample (numpy.ndarray): output sample  from original dataset of size (sample_size% of y)\n",
    "            \n",
    "        Example:\n",
    "            >>> train_x_sample, train_y_sample = sample_dataset(train_x,train_y, size_in_per = 100)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    #getting the sample size to be returned from the percentage\n",
    "    sample_m = int(np.multiply(m,np.divide(size_in_per,100))) #int(m*(dataVol/100)) \n",
    "    \n",
    "    #suffling the original dataset\n",
    "    randCol = np.random.permutation(m)\n",
    "    x_suffled = x[randCol,:,:]\n",
    "    y_suffled = y[randCol,:]\n",
    "    \n",
    "    #taking samples of sample_size\n",
    "    x_sample = x_suffled[0:sample_m,:,:]\n",
    "    y_sample = y_suffled[0:sample_m,:]\n",
    "    \n",
    "    #vallidating the sample data size\n",
    "    assert(x_sample.shape == (sample_m,28,28))\n",
    "    assert(y_sample.shape == (sample_m,1))\n",
    "\n",
    "    return x_sample, y_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, size_in_per = 100):\n",
    "    \"\"\"Loads the dataset from its raw form to an nd-array representation.\n",
    "        \n",
    "        Arguments:\n",
    "            dataset (str): dataset used, mnist or fashion_mnist\n",
    "            size_in_per (int, optional): size of the sample to be drawn from the dataset. Defaults to 100\n",
    "        \n",
    "        Returns:\n",
    "            train_x_orig (numpy.ndarray): training set images of size (6000,28,28) \n",
    "            train_y_orig (numpy.ndarray): training set labels of size (60000,1)\n",
    "            test_x_orig (numpy.ndarray): test set images of size (10000,28,28)\n",
    "            test_y_orig (numpy.ndarray): test set labels of size (10000,1)\n",
    "        \n",
    "        Example:\n",
    "            >>> train_x, train_y, test_x, test_y = load_dataset(dataset = \"fashion_mnist\", size_in_per = 25)\n",
    "    \"\"\"\n",
    "    \n",
    "    path = 'dataset/%s/'%(dataset) #generating the dataset path\n",
    "     \n",
    "    filename = [\"train-images-idx3-ubyte\",\n",
    "                \"train-labels-idx1-ubyte\",\n",
    "                \"t10k-images-idx3-ubyte\",\n",
    "                \"t10k-labels-idx1-ubyte\"] \n",
    "    gzip_filename = [fname+\".gz\" for fname in filename] #generating the gzip file name\n",
    "\n",
    "    \n",
    "    #creating a new destination path if it doesnot exist\n",
    "    if not os.path.exists(path):\n",
    "        print(\"No destination directory exists to load the data from:\\nCreating '\" + path + \"' as a new directory...\\n\")\n",
    "        os.makedirs(path) #making directories recursively\n",
    "    \n",
    "    #Downloading the dataset form  the web if the path is empty (i.e if no dataset is found in the specified directory)\n",
    "    if len(get_files(path)) == 0:\n",
    "        print (\"Downloading the %s dataset...\"%dataset)\n",
    "        download_dataset(dataset, to_path = path)\n",
    "    \n",
    "    files = get_files(path) #getting all the files in the path after download\n",
    "    \n",
    "    #checking for all the decompressed files\n",
    "    file_check = all(fname in files for fname in filename)  \n",
    "\n",
    "    #implementating the logic for data retrival form the specified path\n",
    "    if file_check == False:       \n",
    "        gzip_filecheck = all(fname in files for fname in gzip_filename) #checking for all the gzip files\n",
    "        if gzip_filecheck == False:\n",
    "            print (\"Downloading missing gzip files of %s dataset...\"%dataset)\n",
    "            download_dataset(dataset, to_path = path) #downloading missing zip file of the dataset\n",
    "        print (\"Decompressing the %s dataset...\"%dataset)\n",
    "        decompress_dataset(path,keep_original = True)#decompressing the dataset\n",
    "    \n",
    "    train_x_temp, train_y_temp, test_x_temp, test_y_temp = retrive_dataset(path, filename)\n",
    "    \n",
    "    #getting the size of the data based on the sample size\n",
    "    #size = 100 means entire data is suffled and returned\n",
    "    train_x_orig, train_y_orig = sample_dataset(train_x_temp,train_y_temp, size_in_per)\n",
    "    test_x_orig, test_y_orig = sample_dataset(test_x_temp,test_y_temp, size_in_per)\n",
    "    \n",
    "\n",
    "    return train_x_orig, train_y_orig, test_x_orig, test_y_orig\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to load data from binary file using numpy: 0.1621 s\n",
      "\n",
      "Sample Size : 25%\n",
      "\n",
      "Data\t\t\t Datatype\t\t Dataset Size\n",
      "=================================================================\n",
      "Training Set Images:\t<class 'numpy.ndarray'>\t (15000, 28, 28)\n",
      "Training Set Labels:\t<class 'numpy.ndarray'>\t (15000, 1)\n",
      "Test Set Images:\t<class 'numpy.ndarray'>\t (2500, 28, 28)\n",
      "Test Set Labels:\t<class 'numpy.ndarray'>\t (2500, 1)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset_size_in_per = 25\n",
    "\n",
    "toc = time.time()\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig = load_dataset(dataset = \"mnist\", size_in_per = dataset_size_in_per)\n",
    "\n",
    "tic = time.time()\n",
    "#displaying the retrival info\n",
    "print(\"\\nTime to load data from binary file using numpy: %.4f s\\n\"%(tic-toc))\n",
    "\n",
    "print(\"Sample Size : %d%%\\n\"%(dataset_size_in_per))\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Datatype\\t\\t\",\"Dataset Size\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(type(train_x_orig))+\"\\t\",str(train_x_orig.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(type(train_y_orig))+\"\\t\",str(train_y_orig.shape))\n",
    "print(\"Test Set Images:\\t\" + str(type(test_x_orig))+\"\\t\",str(test_x_orig.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(type(test_y_orig))+\"\\t\",str(test_y_orig.shape))\n",
    "print(\"=================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_split(train_x,train_y):\n",
    "    \"\"\"Randomly splits the test set to dev and test set\n",
    "        \n",
    "        Arguments:\n",
    "            train_x (numpy.ndarray): train set images of size (60000,28,28), where m is num of examples in train_x\n",
    "            train_y (numpy.ndarray): train set labels of size (60000,1)\n",
    "        \n",
    "        Returns:\n",
    "            train_x_split (numpy.ndarray): train set images of size (50000,28,28) , where n is num of examples in dev_x\n",
    "            train_y_split (numpy.ndarray): train set labels of size (50000,1)\n",
    "            dev_x_split (numpy.ndarray): dev set images of size (10000,28,28)\n",
    "            dev_y_split (numpy.ndarray): dev set labels of size (10000,1)\n",
    "        \n",
    "        Example:\n",
    "            >>> train_x_split, train_y_split, dev_x_split, dev_y_split = train_dev_split(train_x, train_y)\n",
    "    \"\"\"\n",
    "    m = train_y.shape[0]\n",
    "    if m == 60000:\n",
    "        n = 50000 #for 100% data taking 50000 as training set and 10000 as dev set\n",
    "    elif m == 30000:\n",
    "        n = 25000 #for 50% data taking 25000 as training set and 5000 as dev set\n",
    "    else:\n",
    "        n = int(0.85 * m) #Splitting the training set into train and dev set such that dev set is 15% of training set in size\n",
    "    \n",
    "    #suffling the test dataset\n",
    "    randCol = np.random.permutation(m)\n",
    "    suffled_x = train_x[randCol,:,:]\n",
    "    suffled_y = train_y[randCol,:]\n",
    "    \n",
    "    #splitting the test set into dev and test set , 50% each\n",
    "    train_x_split = suffled_x[0:n,:,:]\n",
    "    train_y_split = suffled_y[0:n,:]\n",
    "    \n",
    "    dev_x_split = suffled_x[n:m,:,:]\n",
    "    dev_y_split = suffled_y[n:m,:]\n",
    "    \n",
    "    assert(train_x_split.shape == (n,28,28))\n",
    "    assert(train_y_split.shape == (n,1))\n",
    "    assert(dev_x_split.shape == (m-n,28,28))\n",
    "    assert(dev_y_split.shape == (m-n,1))\n",
    "    \n",
    "    return train_x_split, train_y_split, dev_x_split, dev_y_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t\t Datatype\t\t Shape\n",
      "========================================================================\n",
      "Training Set Images:\t\t<class 'numpy.ndarray'>\t (12750, 28, 28)\n",
      "Training Set Labels:\t\t<class 'numpy.ndarray'>\t (12750, 1)\n",
      "Development Set Images:\t\t<class 'numpy.ndarray'>\t (2250, 28, 28)\n",
      "Development Set Labels:\t\t<class 'numpy.ndarray'>\t (2250, 1)\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "train_x_split, train_y_split, dev_x_split, dev_y_split = train_dev_split(train_x_orig, train_y_orig)\n",
    "\n",
    "print(\"Data\\t\\t\\t\\t\",\"Datatype\\t\\t\",\"Shape\")\n",
    "print(\"========================================================================\")\n",
    "print(\"Training Set Images:\\t\\t\" + str(type(train_x_split))+\"\\t\",str(train_x_split.shape))\n",
    "print(\"Training Set Labels:\\t\\t\" + str(type(train_y_split))+\"\\t\",str(train_y_split.shape))\n",
    "print(\"Development Set Images:\\t\\t\" + str(type(dev_x_split))+\"\\t\",str(dev_x_split.shape))\n",
    "print(\"Development Set Labels:\\t\\t\" + str(type(dev_y_split))+\"\\t\",str(dev_y_split.shape))\n",
    "print(\"========================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Validating Raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset visualization using charts\n",
    "def visualize_data_distribution(y_orig, dataset_type):\n",
    "    \"\"\"Plots bar graph showing the distribution of examples in each class\n",
    "\n",
    "        Arguments:\n",
    "            y_orig (numpy.ndarray): labels of dev set\n",
    "            dataset_type(str): type of dataset, can be training, dev or test\n",
    "            \n",
    "        Example:\n",
    "            >>> visualize_data_distribution(train_y, dataset_type = \"training\")            \n",
    "    \"\"\"\n",
    "    #setting the plot style\n",
    "#     plt.style.use('seaborn')\n",
    "    \n",
    "    #creating subplots\n",
    "    fig, axes = plt.subplots(figsize=(10,5))\n",
    "    i = 0\n",
    "    \n",
    "    #plotting the bar graph for each dataset labels\n",
    "    unique, counts = np.unique(y_orig, return_counts=True) \n",
    "    axes.bar(unique, counts)\n",
    "    max_value = np.max(counts)\n",
    "    axes.set(xticks = unique, ylim = (0,max_value + max_value // 10))\n",
    "    axes.set_title(\"Distribution of Examples in %s Set\"%dataset_type.capitalize() , fontsize = 16)\n",
    "    axes.set_xlabel(\"Classes\", fontsize = 12)\n",
    "    axes.set_ylabel(\"Number of Examples\", fontsize = 12)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_distribution(train_y_split, dataset_type = \"training\")\n",
    "visualize_data_distribution(dev_y_split,  dataset_type = \"dev\")\n",
    "visualize_data_distribution(test_y_orig,  dataset_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_description(dataset):\n",
    "    \"\"\"Returns the description of the labels\n",
    "        \n",
    "        Arguments:\n",
    "            dataset(str): dataset used, mnist of fashion_mnist\n",
    "\n",
    "        Return:\n",
    "            desc (dict): label description\n",
    "        \n",
    "        Example:\n",
    "            >>> label_desc = label_description(dataset = \"fashion_mnist\")\n",
    "    \"\"\"\n",
    "    \n",
    "    #creating the label description for fashion_mnist dataset\n",
    "    if dataset == \"fashion_mnist\":\n",
    "        desc = {0:\"T-shirt/top\",\n",
    "                1:\"Trouser\",\n",
    "                2:\"Pullover\",\n",
    "                3:\"Dress\",\n",
    "                4:\"Coat\",\n",
    "                5:\"Sandal\",\n",
    "                6:\"Shirt\",\n",
    "                7:\"Sneaker\",\n",
    "                8:\"Bag\",\n",
    "                9:\"Ankle boot\"}\n",
    "    #creating the label description for mnist dataset\n",
    "    elif dataset == \"mnist\":\n",
    "        desc = {0:\"Zero\",\n",
    "                1:\"One\",\n",
    "                2:\"Two\",\n",
    "                3:\"Three\",\n",
    "                4:\"Four\",\n",
    "                5:\"Five\",\n",
    "                6:\"Six\",\n",
    "                7:\"Seven\",\n",
    "                8:\"Eight\",\n",
    "                9:\"Nine\"}\n",
    "    else:\n",
    "        raise ValueError(\"Dataset must be mnist or fashion_mnist\")\n",
    "        \n",
    "    return desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, ' Label: 5 | Five')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_desc = label_description(dataset = \"mnist\")\n",
    "\n",
    "plt.imshow(test_x_orig[200], cmap = \"Greys\")\n",
    "a = int(test_y_orig[200])\n",
    "plt.title(\" Label: %d | %s\"%(a,label_desc[a]))\n",
    "# print(255-test_x_orig[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(x_orig, y_orig, dataset, dataset_type):\n",
    "    \"\"\"Plots 10 sample images  with labels and label description\n",
    "        \n",
    "        Arguments:\n",
    "            x_orig (numpy.ndarray): 3D array representation of input images\n",
    "            y_orig (numpy.ndarray): array of labels\n",
    "            dataset (str): dataset used, mnist of fashion_mnist\n",
    "            dataset_type (str): type of dataset, can be training, dev or test\n",
    "        \n",
    "        Example:\n",
    "            >>> visualize_dataset(train_x, train_y, dataset = \"fashion_mnist\", dataset_type = \"training\")            \n",
    "    \"\"\"\n",
    "    #recovering matplotlib defaults\n",
    "#     plt.rcParams.update(plt.rcParamsDefault) \n",
    "    \n",
    "    #getting the label description\n",
    "    label_desc = label_description(dataset)\n",
    "    \n",
    "    #generating a range of index for image samples to be visualized\n",
    "    rng = np.random.randint(0,1000,10)\n",
    "    #generating the visualization lables based on the dataset_type\n",
    "    visual_title = \"%s Set\"%dataset_type.capitalize()\n",
    "   \n",
    "    #creating subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5,figsize=(16,8))\n",
    "    fig.subplots_adjust(hspace=.1)\n",
    "    fig.suptitle(visual_title, fontsize = 20)\n",
    "    \n",
    "    #plotting the sample images along with their labels\n",
    "    for ax,i in zip(axes.flatten(),rng):\n",
    "        ax.imshow(x_orig[i].squeeze(),interpolation='nearest', cmap=\"Greys\")\n",
    "        ax.set(title = \"Label: %d | %s\"%(y_orig[i,0], label_desc[y_orig[i,0]] ))\n",
    "#         ax.set_xtick(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_x_split, train_y_split, dataset = \"mnist\", dataset_type = \"training\")\n",
    "visualize_dataset(dev_x_split, dev_y_split, dataset = \"mnist\", dataset_type = \"dev\")\n",
    "visualize_dataset(test_x_orig, test_y_orig, dataset = \"mnist\", dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_input(x_orig):\n",
    "    \"\"\"Returns the flattened 2D array of input images\n",
    "        \n",
    "        Arguement:\n",
    "            x_orig (numpy.ndarray): 3D numpy array of the input set images of size (m,28,28)\n",
    "            \n",
    "        Returns:\n",
    "            x_flatten (numpy.ndarray): 2D numpy array of flattened input data of size (784,m)\n",
    "            \n",
    "        Example:\n",
    "            >>> train_x_flatten = flatten_input(train_x)\n",
    "    \"\"\"\n",
    "    m = x_orig.shape[0] #number of examples in dataset set\n",
    "    \n",
    "    #flattening the image--The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    x_flatten = x_orig.reshape(x_orig.shape[0], -1).T   \n",
    "\n",
    "    assert(x_flatten.shape == (784,m) )\n",
    "    \n",
    "    return x_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Shape\n",
      "=====================================\n",
      "Input Training set:\t(784, 50000)\n",
      "Input Dev set:\t\t(784, 10000)\n",
      "Input Test set:\t\t(784, 10000)\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = flatten_input(train_x_split)\n",
    "dev_x_flatten = flatten_input(dev_x_split)\n",
    "test_x_flatten = flatten_input(test_x_orig)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"=====================================\")\n",
    "print (\"Input Training set:\\t\" + str(train_x_flatten.shape))\n",
    "print (\"Input Dev set:\\t\\t\" + str(dev_x_flatten.shape))\n",
    "print (\"Input Test set:\\t\\t\" + str(test_x_flatten.shape))\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(x_flatten):\n",
    "    \"\"\"Normalizes the pixel values of the flattened images to the range 0-1\n",
    "        \n",
    "        Arguement:\n",
    "            x_flatten (numpy.ndarray): flattened input data of size (784,m)\n",
    "\n",
    "        Returns:\n",
    "            x_norm (numpy.ndarray): normalized input data of size (784,m)\n",
    "            \n",
    "        Example:\n",
    "            >>> train_x_norm = normalize_input(train_x_flatten)\n",
    "    \"\"\"\n",
    "    m = x_flatten.shape[1]\n",
    "    \n",
    "    # Normalizing the data into the range between 0 and 1.\n",
    "    x_norm = np.divide(x_flatten,255.)\n",
    "    \n",
    "    assert(x_norm.shape == (784,m) )\n",
    "    \n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Shape\n",
      "=====================================\n",
      "Input Training set:\t(784, 50000)\n",
      "Input Dev set:\t\t(784, 10000)\n",
      "Input Test set:\t\t(784, 10000)\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "train_x_norm = normalize_input(train_x_flatten)\n",
    "dev_x_norm= normalize_input(dev_x_flatten)\n",
    "test_x_norm = normalize_input(test_x_flatten)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"=====================================\")\n",
    "print (\"Input Training set:\\t\" + str(train_x_norm.shape))\n",
    "print (\"Input Dev set:\\t\\t\" + str(dev_x_norm.shape))\n",
    "print (\"Input Test set:\\t\\t\" + str(test_x_norm.shape))\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y_orig, num_class):\n",
    "    \"\"\"Transform the output labels into the one-hot encoding representation\n",
    "        \n",
    "        Arguments:\n",
    "            y_orig (numpy.ndarray): numeric labels(decimal form) loaded directly from the binary file of size (1,m)\n",
    "            num_class (int): number of the classes of the labels\n",
    "        Returns:\n",
    "            y_encoded (numpy.ndarray): encoded ndarray of the labels with binary values of size (num_class,m)\n",
    "            \n",
    "        Example:\n",
    "            >>> train_y_encoded = one_hot_encoding(train_y.T, num_class = 10)\n",
    "    \"\"\"\n",
    "    m = y_orig.shape[1]\n",
    "    #encoding the labels\n",
    "    y_encoded = np.eye(num_class)[y_orig.reshape(-1)].T\n",
    "\n",
    "\n",
    "    assert(y_encoded.shape == (num_class, m))\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to encode: 3.5352706909179688 ms\n",
      "\n",
      "Data\t\t\t Shape\n",
      "===================================\n",
      "Output Training set:\t(10, 50000)\n",
      "Output Dev set:\t\t(10, 10000)\n",
      "Output Test set:\t(10, 10000)\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#encoding the output of the training and the test dataset\n",
    "train_y_encoded = one_hot_encoding(train_y_split.T, num_class = 10 )\n",
    "dev_y_encoded = one_hot_encoding(dev_y_split.T, num_class = 10)\n",
    "test_y_encoded = one_hot_encoding(test_y_orig.T, num_class = 10)\n",
    "tic = time.time()\n",
    "print(\"Time to encode: \" + str(1000*(tic-toc)) + \" ms\\n\")\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Shape\")\n",
    "print(\"===================================\")\n",
    "print (\"Output Training set:\\t\" + str(train_y_encoded.shape))\n",
    "print (\"Output Dev set:\\t\\t\" + str(dev_y_encoded.shape))\n",
    "print (\"Output Test set:\\t\" + str(test_y_encoded.shape))\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(x_orig, y_orig, num_class = 10):\n",
    "    \"\"\"Flatten and Normalize the input images and encode the output labels to one-hot representation.\n",
    "        \n",
    "        Arguments:\n",
    "            x_orig (numpy.ndarray): input images of size (60000,28,28)\n",
    "            y_orig (numpy.ndarray): input labels of size (60000,1)\n",
    "            num_class(int, optional): number of the classes of the labels. Defaults to 10\n",
    "            \n",
    "        Returns:\n",
    "            x_norm (numpy.ndarray): flattened and normalized input data\n",
    "            y_encoded (numpy.ndarray): labels encoded in one-hot representation\n",
    "            \n",
    "        Example:\n",
    "            >>> train_x_norm, train_y_encoded = prep_dataset(train_x, train_y, num_class = 10)\n",
    "    \"\"\"\n",
    "    #flatten the input images\n",
    "    x_flatten = flatten_input(x_orig)\n",
    "    \n",
    "    #normalize the input images\n",
    "    x_norm = normalize_input(x_flatten)\n",
    "    \n",
    "    #encode the output labels\n",
    "    y_encoded = one_hot_encoding(y_orig.T, num_class)\n",
    "    \n",
    "    return x_norm,y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t Before Processing\t After Processing\n",
      "=================================================================\n",
      "Training Set Images:\t(50000, 28, 28)\t\t(784, 50000)\n",
      "Training Set Labels:\t(50000, 1)\t\t(10, 50000)\n",
      "Dev Set Images:\t\t(10000, 28, 28)\t\t(784, 10000)\n",
      "Dev Set Labels:\t\t(10000, 1)\t\t(10, 10000)\n",
      "Test Set Images:\t(10000, 28, 28)\t\t(784, 10000)\n",
      "Test Set Labels:\t(10000, 1)\t\t(10, 10000)\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "train_x_norm, train_y_encoded = prep_dataset(train_x_split, train_y_split, num_class = 10)\n",
    "dev_x_norm, dev_y_encoded= prep_dataset(dev_x_split, dev_y_split, num_class = 10)\n",
    "test_x_norm, test_y_encoded = prep_dataset(test_x_orig, test_y_orig, num_class = 10)\n",
    "\n",
    "print(\"Data\\t\\t\\t\",\"Before Processing\\t\",\"After Processing\")\n",
    "print(\"=================================================================\")\n",
    "print(\"Training Set Images:\\t\" + str(train_x_split.shape)+\"\\t\\t\"+ str(train_x_norm.shape))\n",
    "print(\"Training Set Labels:\\t\" + str(train_y_split.shape)+\"\\t\\t\"+ str(train_y_encoded.shape))\n",
    "print(\"Dev Set Images:\\t\\t\" + str(dev_x_split.shape)+\"\\t\\t\"+ str(dev_x_norm.shape))\n",
    "print(\"Dev Set Labels:\\t\\t\" + str(dev_y_split.shape)+\"\\t\\t\"+ str(dev_y_encoded.shape))\n",
    "print(\"Test Set Images:\\t\" + str(test_x_orig.shape)+\"\\t\\t\"+ str(test_x_norm.shape))\n",
    "print(\"Test Set Labels:\\t\" + str(test_y_orig.shape)+\"\\t\\t\"+ str(test_y_encoded.shape))\n",
    "print(\"=================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Work\n",
    "- [ ] Optimize the code using comprehension techniques\n",
    "- [x] Make sure the code is clean and understandable. Follow a single naming convention and give proper name for all the variables\n",
    "- [x] Write the method description as detailed as possible with proper examples\n",
    "- [x] Generate the Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
